{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fullGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tortoisehare/TSR-GAN/blob/master/fullGAN_notgood.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABnslutLuWVE",
        "colab_type": "code",
        "outputId": "ab481d7e-b91e-4dad-f840-5de52423905d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!mkdir weights\n",
        "!mkdir weights/target_model\n",
        "!mkdir weights/generator\n",
        "!mkdir weights/discriminator\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘weights’: File exists\n",
            "mkdir: cannot create directory ‘weights/target_model’: File exists\n",
            "mkdir: cannot create directory ‘weights/generator’: File exists\n",
            "mkdir: cannot create directory ‘weights/discriminator’: File exists\n",
            "sample_data  test.p  train.p  valid.p  weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-dNiamCtfBr",
        "colab_type": "code",
        "outputId": "37319545-1ae4-443e-ab50-457658563b0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1318
        }
      },
      "source": [
        "#TARGET CLASSIFIER\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(1187) #to help reproduce\n",
        "\n",
        "#from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import csv\n",
        "#import random\n",
        "#import cv2\n",
        "#from skimage.filters import rank\n",
        "#import skimage.morphology as morp\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "#from keras.utils import to_categorical\n",
        "#from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "#from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "\n",
        "class Target:\n",
        "    def __init__(self, lr=0.001, epochs=25, n_input=32, n_classes=43, batch_size=20, restore=0):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.n_input = 32\n",
        "        self.n_classes = 43\n",
        "        self.batch_size = batch_size\n",
        "        self.restore = restore\n",
        "\n",
        "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "    \n",
        "    '''\n",
        "    def next_batch(self, X, Y, i, batch_size):\n",
        "      idx = i*batch_size\n",
        "      idx_n = idx + batch_size\n",
        "      return X[idx:idx_n], Y[idx:idx_n]\n",
        "      '''\n",
        "    \n",
        "    def Model(self, x):\n",
        "        with tf.variable_scope('Model', reuse=tf.AUTO_REUSE):\n",
        "            # Hyperparameters\n",
        "            mu = 0\n",
        "            sigma = 0.1\n",
        "            n_out = self.n_classes\n",
        "            learning_rate = self.lr\n",
        "\n",
        "            # Layer 1 (Convolutional): Input = 32x32x1. Output = 28x28x6.\n",
        "            filter1_width = 5\n",
        "            filter1_height = 5\n",
        "            input1_channels = 1\n",
        "            conv1_output = 6\n",
        "            # Weight and bias\n",
        "            conv1_weight = tf.Variable(tf.truncated_normal(\n",
        "                shape=(filter1_width, filter1_height, input1_channels, conv1_output),\n",
        "                mean = mu, stddev = sigma))\n",
        "            conv1_bias = tf.Variable(tf.zeros(conv1_output))\n",
        "\n",
        "            # Apply Convolution\n",
        "            conv1 = tf.nn.conv2d(x, conv1_weight, strides=[1, 1, 1, 1], padding='VALID') + conv1_bias\n",
        "\n",
        "            # Activation:\n",
        "            conv1 = tf.nn.relu(conv1)\n",
        "\n",
        "            # Pooling: Input = 28x28x6. Output = 14x14x6.\n",
        "            conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "            # Layer 2 (Convolutional): Output = 10x10x16.\n",
        "            filter2_width = 5\n",
        "            filter2_height = 5\n",
        "            input2_channels = 6\n",
        "            conv2_output = 16\n",
        "            # Weight and bias\n",
        "            conv2_weight = tf.Variable(tf.truncated_normal(\n",
        "                shape=(filter2_width, filter2_height, input2_channels, conv2_output),\n",
        "                mean = mu, stddev = sigma))\n",
        "            conv2_bias = tf.Variable(tf.zeros(conv2_output))\n",
        "\n",
        "            # Apply Convolution\n",
        "            conv2 = tf.nn.conv2d(conv1, conv2_weight, strides=[1, 1, 1, 1], padding='VALID') + conv2_bias\n",
        "\n",
        "            # Activation:\n",
        "            conv2 = tf.nn.relu(conv2)\n",
        "\n",
        "            # Pooling: Input = 10x10x16. Output = 5x5x16.\n",
        "            conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "            # Flattening: Input = 5x5x16. Output = 400.\n",
        "            fully_connected0 = Flatten()(conv2)\n",
        "\n",
        "            # Layer 3 (Fully Connected): Input = 400. Output = 120.\n",
        "            connected1_weights = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
        "            connected1_bias = tf.Variable(tf.zeros(120))\n",
        "            fully_connected1 = tf.add((tf.matmul(fully_connected0, connected1_weights)), connected1_bias)\n",
        "\n",
        "            # Activation:\n",
        "            fully_connected1 = tf.nn.relu(fully_connected1)\n",
        "\n",
        "            # Layer 4 (Fully Connected): Input = 120. Output = 84.\n",
        "            connected2_weights = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
        "            connected2_bias = tf.Variable(tf.zeros(84))\n",
        "            fully_connected2 = tf.add((tf.matmul(fully_connected1, connected2_weights)), connected2_bias)\n",
        "\n",
        "            # Activation.\n",
        "            fully_connected2 = tf.nn.relu(fully_connected2)\n",
        "    \n",
        "            # Layer 5 (Fully Connected): Input = 84. Output = 43.\n",
        "            output_weights = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n",
        "            output_bias = tf.Variable(tf.zeros(43))\n",
        "            logits =  tf.add((tf.matmul(fully_connected2, output_weights)), output_bias)\n",
        "\n",
        "            probs = tf.nn.sigmoid(logits)\n",
        "            \n",
        "            return logits, probs\n",
        "    \n",
        "    \n",
        "    \n",
        "    def test(self, X_data, BATCH_SIZE=64):\n",
        "        num_examples = len(X_data)\n",
        "        y_pred = np.zeros(num_examples, dtype=np.int32)\n",
        "        sess = tf.get_default_session()\n",
        "        for offset in range(0, num_examples, BATCH_SIZE):\n",
        "            batch_x = X_data[offset:offset+BATCH_SIZE]\n",
        "            y_pred[offset:offset+BATCH_SIZE] = sess.run(tf.argmax(logits, 1), \n",
        "                               feed_dict={x:batch_x, keep_prob:1, keep_prob_conv:1})\n",
        "        return y_pred\n",
        "    \n",
        "    \n",
        "    def train(self, X_train, Y_train, X_valid, Y_valid):\n",
        "        #placeholders for inputs\n",
        "        x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
        "        y = tf.placeholder(tf.int32, (None))\n",
        "\n",
        "        keep_prob = tf.placeholder(tf.float32)       # For fully-connected layers\n",
        "        keep_prob_conv = tf.placeholder(tf.float32)\n",
        "\n",
        "        #define graph\n",
        "        logits, _ = self.Model(x)\n",
        "        \n",
        "              # Training operation\n",
        "        one_hot_y = tf.one_hot(y, 43)\n",
        "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=one_hot_y)\n",
        "        loss_operation = tf.reduce_mean(cross_entropy)\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate = self.lr)\n",
        "        training_operation = optimizer.minimize(loss_operation)\n",
        "\n",
        "        # Accuracy operation\n",
        "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
        "        accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "\n",
        "          # Saving all variables\n",
        "        saver = tf.train.Saver()\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            num_train = len(Y_train)\n",
        "            num_valid = len(Y_valid)\n",
        "            print(\"Training ...\")\n",
        "            print()\n",
        "            EPOCHS = self.epochs\n",
        "            BATCH_SIZE = self.batch_size\n",
        "            DIR = \"./weights/target_model\"\n",
        "            total_batch = int(X_train.shape[0] / self.batch_size)\n",
        "\n",
        "            for i in range(EPOCHS):\n",
        "                avg_cost = 0.\n",
        "                total_accuracy = 0\n",
        "                validation_accuracy = 0\n",
        "                #Train set\n",
        "                for offset in range(0, num_train, BATCH_SIZE):\n",
        "                    end = offset + BATCH_SIZE\n",
        "                    batch_x, batch_y = X_train[offset:end], Y_train[offset:end]\n",
        "                    _, c = sess.run([training_operation, loss_operation], feed_dict={x: batch_x, y: batch_y, keep_prob : 0.6, keep_prob_conv: 0.8})\n",
        "                    avg_cost += c / total_batch\n",
        "                    \n",
        "                    #Validation Set\n",
        "                for offset in range(0, num_valid, BATCH_SIZE):\n",
        "                    end = offset + BATCH_SIZE\n",
        "                    batch_x, batch_y = X_valid[offset:end], Y_valid[offset:end]\n",
        "                    accuracy = sess.run(accuracy_operation, \n",
        "                                    feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0, keep_prob_conv: 1.0 })\n",
        "                    total_accuracy += (accuracy * len(batch_x))\n",
        "                    validation_accuracy = total_accuracy / num_valid\n",
        "                    #print(\"Validation Accuracy = {:.3f}%\".format(validation_accuracy*100))\n",
        "                    \n",
        "                print(\"Epoch: \", '%04d' % (i+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
        "                print(\"EPOCH {} : Validation Accuracy = {:.3f}%\".format(i+1, (validation_accuracy*100)))\n",
        "            \n",
        "            \n",
        "            #Test set\n",
        "            num_examples = len(X_test)\n",
        "            y_pred = np.zeros(num_examples, dtype=np.int32)\n",
        "            #sess = tf.get_default_session()\n",
        "            for offset in range(0, num_examples, BATCH_SIZE):\n",
        "                batch_x = X_test[offset:offset+BATCH_SIZE]\n",
        "                y_pred[offset:offset+BATCH_SIZE] = sess.run(tf.argmax(logits, 1), \n",
        "                                   feed_dict={x:batch_x, keep_prob:1, keep_prob_conv:1})\n",
        "            test_accuracy = sum(Y_test == y_pred)/len(Y_test)\n",
        "            print(\"Test Accuracy = {:.1f}%\".format(test_accuracy*100))\n",
        "\n",
        "            cm = confusion_matrix(Y_test, y_pred)\n",
        "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "            cm = np.log(.0001 + cm)\n",
        "            plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "            plt.title('Log of normalized Confusion Matrix')\n",
        "            plt.ylabel('True label')\n",
        "            plt.xlabel('Predicted label')\n",
        "            plt.show()\n",
        "            \n",
        "            saver.save(sess, \"./weights/target_model/model\")\n",
        "            print(\"Model saved\")\n",
        "            sess.close()\n",
        "\n",
        "   \n",
        "      \n",
        "      \n",
        "import pickle\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    training_file = 'train.p'\n",
        "    testing_file = 'test.p'\n",
        "    validation_file = 'valid.p'\n",
        "\n",
        "    with open(training_file, mode='rb') as f:\n",
        "        tstrain = pickle.load(f)\n",
        "    with open(testing_file, mode='rb') as f:\n",
        "        tstest = pickle.load(f)\n",
        "    with open(validation_file, mode='rb') as f:\n",
        "        tsvalid = pickle.load(f)\n",
        "\n",
        "    X_train, Y_train = tstrain['features'], tstrain['labels']\n",
        "    X_valid, Y_valid = tsvalid['features'], tsvalid['labels']\n",
        "    X_test, Y_test = tstest['features'], tstest['labels']\n",
        "\n",
        "    #shuffle training set\n",
        "    X_train, Y_train = shuffle(X_train, Y_train)\n",
        "    X_test, Y_test = shuffle(X_test, Y_test)\n",
        "    X_valid, Y_valid = shuffle(X_valid, Y_valid)\n",
        "\n",
        "    #grayscale images\n",
        "    grayscale = [0.299,0.587,0.144]\n",
        "\n",
        "    X_test = np.dot(X_test, grayscale)\n",
        "    X_train = np.dot(X_train, grayscale)\n",
        "    X_valid = np.dot(X_valid, grayscale)\n",
        "\n",
        "\n",
        "    #normalize\n",
        "    X_train = np.array(X_train)/255\n",
        "    X_test = np.array(X_test)/255\n",
        "    X_valid = np.array(X_valid)/255\n",
        "\n",
        "    #expand dimensions to fit 4D input array\n",
        "    X_train = np.expand_dims(X_train,-1)\n",
        "    X_test = np.expand_dims(X_test,-1)\n",
        "    X_valid = np.expand_dims(X_valid,-1)\n",
        "\n",
        "    assert(len(X_train)==len(Y_train))\n",
        "    n_train = len(X_train)\n",
        "    assert(len(X_test)==len(Y_test))\n",
        "    n_test = len(X_test)\n",
        "\n",
        "    cnn = Target()\n",
        "    cnn.train(X_train, Y_train, X_valid, Y_valid)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Training ...\n",
            "\n",
            "Epoch:  0001 cost= 1.221068781\n",
            "EPOCH 1 : Validation Accuracy = 84.626%\n",
            "Epoch:  0002 cost= 0.269277920\n",
            "EPOCH 2 : Validation Accuracy = 89.116%\n",
            "Epoch:  0003 cost= 0.148426772\n",
            "EPOCH 3 : Validation Accuracy = 90.975%\n",
            "Epoch:  0004 cost= 0.100442036\n",
            "EPOCH 4 : Validation Accuracy = 91.043%\n",
            "Epoch:  0005 cost= 0.073347088\n",
            "EPOCH 5 : Validation Accuracy = 91.791%\n",
            "Epoch:  0006 cost= 0.056407771\n",
            "EPOCH 6 : Validation Accuracy = 90.952%\n",
            "Epoch:  0007 cost= 0.053530581\n",
            "EPOCH 7 : Validation Accuracy = 91.837%\n",
            "Epoch:  0008 cost= 0.041833152\n",
            "EPOCH 8 : Validation Accuracy = 91.678%\n",
            "Epoch:  0009 cost= 0.033395743\n",
            "EPOCH 9 : Validation Accuracy = 92.381%\n",
            "Epoch:  0010 cost= 0.034409257\n",
            "EPOCH 10 : Validation Accuracy = 91.995%\n",
            "Epoch:  0011 cost= 0.026168852\n",
            "EPOCH 11 : Validation Accuracy = 90.884%\n",
            "Epoch:  0012 cost= 0.027385393\n",
            "EPOCH 12 : Validation Accuracy = 91.859%\n",
            "Epoch:  0013 cost= 0.028116878\n",
            "EPOCH 13 : Validation Accuracy = 92.109%\n",
            "Epoch:  0014 cost= 0.018162375\n",
            "EPOCH 14 : Validation Accuracy = 90.794%\n",
            "Epoch:  0015 cost= 0.021305048\n",
            "EPOCH 15 : Validation Accuracy = 92.018%\n",
            "Epoch:  0016 cost= 0.017158508\n",
            "EPOCH 16 : Validation Accuracy = 92.245%\n",
            "Epoch:  0017 cost= 0.023072854\n",
            "EPOCH 17 : Validation Accuracy = 90.091%\n",
            "Epoch:  0018 cost= 0.012774075\n",
            "EPOCH 18 : Validation Accuracy = 93.129%\n",
            "Epoch:  0019 cost= 0.022761980\n",
            "EPOCH 19 : Validation Accuracy = 91.859%\n",
            "Epoch:  0020 cost= 0.013090313\n",
            "EPOCH 20 : Validation Accuracy = 91.610%\n",
            "Epoch:  0021 cost= 0.017052188\n",
            "EPOCH 21 : Validation Accuracy = 92.608%\n",
            "Epoch:  0022 cost= 0.016246763\n",
            "EPOCH 22 : Validation Accuracy = 92.766%\n",
            "Epoch:  0023 cost= 0.016510227\n",
            "EPOCH 23 : Validation Accuracy = 92.766%\n",
            "Epoch:  0024 cost= 0.015649815\n",
            "EPOCH 24 : Validation Accuracy = 92.290%\n",
            "Epoch:  0025 cost= 0.018982829\n",
            "EPOCH 25 : Validation Accuracy = 92.857%\n",
            "Test Accuracy = 91.7%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ0AAAEWCAYAAAB8A8JQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nc7l40nDO5fL/AVklMMYAlQMKAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2laXHD7-tmk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generator\n",
        "'''\n",
        "\tGenerator definition for AdvGAN\n",
        "\tref: https://arxiv.org/pdf/1801.02610.pdf\n",
        "'''\n",
        "\n",
        "#import tensorflow as tf\n",
        "#from keras import layers\n",
        "\n",
        "# helper function for convolution -> instance norm -> relu\n",
        "def ConvInstNormRelu(x, filters, kernel_size=3, strides=1):\n",
        "\tConv = tf.layers.conv2d(\n",
        "\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tInstNorm = tf.contrib.layers.instance_norm(Conv)\n",
        "\n",
        "\treturn tf.nn.relu(InstNorm)\n",
        "\n",
        "\n",
        "# helper function for trans convolution -> instance norm -> relu\n",
        "def TransConvInstNormRelu(x, filters, kernel_size=3, strides=2):\n",
        "\tTransConv = tf.layers.conv2d_transpose(\n",
        "\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tInstNorm = tf.contrib.layers.instance_norm(TransConv)\n",
        "\n",
        "\treturn tf.nn.relu(InstNorm)\n",
        "\n",
        "# helper function for residual block of 2 convolutions with same num filters\n",
        "# in the same style as ConvInstNormRelu\n",
        "def ResBlock(x, training, filters=32, kernel_size=3, strides=1):\n",
        "\tconv1 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tconv1_norm = tf.layers.batch_normalization(conv1, training=training)\n",
        "\n",
        "\tconv1_relu = tf.nn.relu(conv1_norm)\n",
        "\n",
        "\tconv2 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\tinputs=conv1_relu,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tconv2_norm = tf.layers.batch_normalization(conv2, training=training)\n",
        "\n",
        "\n",
        "\treturn x + conv2_norm\n",
        "\n",
        "\n",
        "def generator(x, training):\n",
        "\twith tf.variable_scope('g_weights', reuse=tf.AUTO_REUSE): #True\n",
        "\t\t# input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "\t\t# define first three conv + inst + relu layers\n",
        "\t\tc1 = ConvInstNormRelu(x, filters=8, kernel_size=3, strides=1)\n",
        "\t\td1 = ConvInstNormRelu(c1, filters=16, kernel_size=3, strides=2)\n",
        "\t\td2 = ConvInstNormRelu(d1, filters=32, kernel_size=3, strides=2)\n",
        "\n",
        "\t\t# define residual blocks\n",
        "\t\trb1 = ResBlock(d2, training, filters=32)\n",
        "\t\trb2 = ResBlock(rb1, training, filters=32)\n",
        "\t\trb3 = ResBlock(rb2, training, filters=32)\n",
        "\t\trb4 = ResBlock(rb3, training, filters=32)\n",
        "\n",
        "\t\t# upsample using conv transpose\n",
        "\t\tu1 = TransConvInstNormRelu(rb4, filters=16, kernel_size=3, strides=2)\n",
        "\t\tu2 = TransConvInstNormRelu(u1, filters=8, kernel_size=3, strides=2)\n",
        "\n",
        "\t\t# final layer block\n",
        "\t\tout = tf.layers.conv2d_transpose(\n",
        "\t\t\t\t\t\tinputs=u2,\n",
        "\t\t\t\t\t\tfilters=x.get_shape()[-1].value, # or 3 if RGB image\n",
        "\t\t\t\t\t\tkernel_size=3,\n",
        "\t\t\t\t\t\tstrides=1,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\t\t# out = tf.contrib.layers.instance_norm(out)\n",
        "\n",
        "\t\treturn tf.nn.tanh(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKYLa86WtyLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Discriminator\n",
        "'''\n",
        "\tDiscriminator definition for AdvGAN\n",
        "\tref: https://arxiv.org/pdf/1801.02610.pdf\n",
        "'''\n",
        "\n",
        "#import tensorflow as tf\n",
        "\n",
        "def discriminator(x, training):\n",
        "\twith tf.variable_scope('d_weights', reuse=tf.AUTO_REUSE):\n",
        "\t\t# input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "\t\tconv1 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\t\tfilters=8,\n",
        "\t\t\t\t\t\t\tkernel_size=4,\n",
        "\t\t\t\t\t\t\tstrides=2,\n",
        "\t\t\t\t\t\t\tpadding=\"valid\",\n",
        "\t\t\t\t\t\t\tactivation=None)\n",
        "\t\tconv1 = tf.nn.leaky_relu(conv1, alpha=0.2)\n",
        "\n",
        "\t\t\n",
        "\t\tconv2 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\t\tinputs=conv1,\n",
        "\t\t\t\t\t\t\tfilters=16,\n",
        "\t\t\t\t\t\t\tkernel_size=4,\n",
        "\t\t\t\t\t\t\tstrides=2,\n",
        "\t\t\t\t\t\t\tpadding=\"valid\",\n",
        "\t\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\t\tin1 = tf.contrib.layers.instance_norm(conv2)\n",
        "\t\tconv2 = tf.nn.leaky_relu(in1, alpha=0.2)\n",
        "\n",
        "\t\tconv3 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\t\tinputs=conv2,\n",
        "\t\t\t\t\t\t\tfilters=32,\n",
        "\t\t\t\t\t\t\tkernel_size=4,\n",
        "\t\t\t\t\t\t\tstrides=2,\n",
        "\t\t\t\t\t\t\tpadding=\"valid\",\n",
        "\t\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\t\t#in2 = tf.contrib.layers.instance_norm(conv3)\n",
        "\t\tin2 = tf.contrib.layers.instance_norm(conv3)\n",
        "\t\tconv3 = tf.nn.leaky_relu(in2, alpha=0.2)\n",
        "\t\tflat = tf.layers.flatten(conv3)\n",
        "\t\tlogits = tf.layers.dense(flat, 1)\n",
        "\n",
        "\t\tprobs = tf.nn.sigmoid(logits)\n",
        "\n",
        "\t\treturn logits, probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "288LCGWawCVl",
        "colab_type": "code",
        "outputId": "3d3c1deb-5914-494c-9320-8df82cce2310",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4927
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "#from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os, sys\n",
        "import random\n",
        "\n",
        "#make sure GAN_setup.py is in connected folder\n",
        "#from GAN_setup import generator, discriminator\n",
        "\n",
        "#ctargon created class Target and defined/trained his target model in there, then called here\n",
        "#import Target as target_model\n",
        "\n",
        "\n",
        "# get the next batch based on x, y, and the iteration (based on batch_size)\n",
        "def next_batch(X, Y, i, batch_size):\n",
        "    idx = i * batch_size\n",
        "    idx_n = i * batch_size + batch_size\n",
        "    return X[idx:idx_n], Y[idx:idx_n]\n",
        "\n",
        "# loss function to encourage misclassification after perturbation\n",
        "def adv_loss(preds, labels, is_targeted):\n",
        "    real = tf.reduce_sum(labels * preds, 1)\n",
        "    other = tf.reduce_max((1 - labels) * preds - (labels * 10000), 1)\n",
        "    if is_targeted:\n",
        "        return tf.reduce_sum(tf.maximum(0.0, other - real))\n",
        "    return tf.reduce_sum(tf.maximum(0.0, real - other))\n",
        "\n",
        "# loss function to influence the perturbation to be as close to 0 as possible\n",
        "def perturb_loss(preds, thresh=0.3):\n",
        "    zeros = tf.zeros((tf.shape(preds)[0]))\n",
        "    return tf.reduce_mean(tf.maximum(zeros, tf.norm(tf.reshape(preds, (tf.shape(preds)[0], -1)), axis=1) - thresh))\n",
        "\n",
        "\n",
        "# function that defines ops, graphs, and training procedure for AdvGAN framework\n",
        "def AdvGAN(X, y, X_test, y_test, epochs=50, batch_size=128, target=3):\n",
        "    #print(X_train.shape)\n",
        "    #print(y.shape[-1]) is num_images\n",
        "    print(\"y shape\")\n",
        "    print(y.shape)\n",
        "    print(\"y_test shape\")\n",
        "    print(y_test.shape)\n",
        "    \n",
        "    # placeholder definitions\n",
        "    x_pl = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]]) # image placeholder\n",
        "    t = tf.placeholder(tf.float32, [None, y.shape[-1]]) # target placeholder\n",
        "    print(\"t shape)\")\n",
        "    print(t.shape)\n",
        "    is_training = tf.placeholder(tf.bool, [])\n",
        "\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    # MODEL DEFINITIONS\n",
        "    is_targeted = False\n",
        "    if target in range(0, y.shape[-1]):\n",
        "        is_targeted = True\n",
        "\n",
        "    # gather target model\n",
        "    f = Target()\n",
        "    print(\"is targeted boolean\")\n",
        "    print(is_targeted)\n",
        "    \n",
        "    thresh = 0.3\n",
        "\n",
        "    # generate perturbation, add to original input image(s)\n",
        "    perturb = tf.clip_by_value(generator(x_pl, is_training), -thresh, thresh)\n",
        "    x_perturbed = perturb + x_pl\n",
        "    x_perturbed = tf.clip_by_value(x_perturbed, 0, 1)\n",
        "    print(x_perturbed.shape)\n",
        "\n",
        "    # pass real and perturbed image to discriminator and the target model\n",
        "    d_real_logits, d_real_probs = discriminator(x_pl, is_training)\n",
        "    d_fake_logits, d_fake_probs = discriminator(x_perturbed, is_training)\n",
        "    print(d_fake_probs.shape)#1\n",
        "    # pass real and perturbed images to the model we are trying to fool\n",
        "    f_real_logits, f_real_probs = f.Model(x_pl)\n",
        "    f_fake_logits, f_fake_probs = f.Model(x_perturbed)\n",
        "    print(f_fake_probs.shape) #43\n",
        "\n",
        "    # generate labels for discriminator (optionally smooth labels for stability)\n",
        "    smooth = 0.0\n",
        "    d_labels_real = tf.ones_like(d_real_probs) * (1 - smooth)\n",
        "    d_labels_fake = tf.zeros_like(d_fake_probs)\n",
        "\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    # LOSS DEFINITIONS\n",
        "    # discriminator loss\n",
        "    d_loss_real = tf.losses.mean_squared_error(predictions=d_real_probs, labels=d_labels_real)\n",
        "    d_loss_fake = tf.losses.mean_squared_error(predictions=d_fake_probs, labels=d_labels_fake)\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "    # generator loss\n",
        "    g_loss_fake = tf.losses.mean_squared_error(predictions=d_fake_probs, labels=tf.ones_like(d_fake_probs))\n",
        "\n",
        "    # perturbation loss (minimize overall perturbation)\n",
        "    l_perturb = perturb_loss(perturb, thresh)\n",
        "\n",
        "    # adversarial loss (encourage misclassification)\n",
        "    l_adv = adv_loss(f_fake_probs, t, is_targeted)\n",
        "\n",
        "    # weights for generator loss function\n",
        "    alpha = 1.0\n",
        "    beta = 5.0\n",
        "    g_loss = l_adv + alpha*g_loss_fake + beta*l_perturb \n",
        "\n",
        "    # ----------------------------------------------------------------------------------\n",
        "    # gather variables for training/restoring\n",
        "    t_vars = tf.trainable_variables()\n",
        "    f_vars = [var for var in t_vars if 'Model' in var.name]\n",
        "    d_vars = [var for var in t_vars if 'd_' in var.name]\n",
        "    g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='g_weights')\n",
        "\n",
        "    # define optimizers for discriminator and generator\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        d_opt = tf.train.AdamOptimizer().minimize(d_loss, var_list=d_vars)\n",
        "        g_opt = tf.train.AdamOptimizer(learning_rate=0.001).minimize(g_loss, var_list=g_vars)\n",
        "\n",
        "\t# create saver objects for the target model, generator, and discriminator\n",
        "    saver = tf.train.Saver(f_vars)\n",
        "    g_saver = tf.train.Saver(g_vars)\n",
        "    d_saver = tf.train.Saver(d_vars)\n",
        "\n",
        "    init  = tf.global_variables_initializer()\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    sess.run(init)\n",
        "    \n",
        "    # load the pretrained target model\n",
        "    #try:\n",
        "       # saver.restore(sess, \"./weights/target_model/model\")\n",
        "    #except:\n",
        "       # print(\"make sure to train the target model first...\")\n",
        "       # sys.exit(1)\n",
        "\n",
        "    \n",
        "    new_saver = tf.train.import_meta_graph('./weights/target_model/model.meta')\n",
        "    new_saver.restore(sess, tf.train.latest_checkpoint('./weights/target_model'))\n",
        "    #path_to_ckpt_data = './weights/target_model/model.data-00000-of-00001'\n",
        "    #new_saver.restore(sess, path_to_ckpt_data)\n",
        "    \n",
        "    print(\"Pretrained model loaded\")\n",
        "    \n",
        "    total_batches = int(X.shape[0] / batch_size)\n",
        "\n",
        "    for epoch in range(0, epochs):\n",
        "\n",
        "        loss_D_sum = 0.0\n",
        "        loss_G_fake_sum = 0.0\n",
        "        loss_perturb_sum = 0.0\n",
        "        loss_adv_sum = 0.0\n",
        "\n",
        "        for i in range(total_batches):\n",
        "\n",
        "            batch_x, batch_y = next_batch(X, y, i, batch_size)\n",
        "\n",
        "            # if targeted, create one hot vectors of the target\n",
        "            if is_targeted:\n",
        "                targets = np.full((batch_y.shape[0],), target)\n",
        "                batch_y = np.eye(y.shape[-1])[targets]\n",
        "\n",
        "            # train the discriminator first n times\n",
        "            for _ in range(1):\n",
        "                _, loss_D_batch = sess.run([d_opt, d_loss], feed_dict={x_pl: batch_x, \\\n",
        "                                           is_training: True})\n",
        "\n",
        "            #print(\"batch x\")\n",
        "            #print(batch_x.shape)\n",
        "            #print(\"batch y\")\n",
        "            #print(batch_y.shape)\n",
        "            \n",
        "\t\t\t       # train the generator n times\n",
        "            for _ in range(1):\n",
        "                \n",
        "                _, loss_G_fake_batch, loss_adv_batch, loss_perturb_batch = \\\n",
        "                        sess.run([g_opt, g_loss_fake, l_adv, l_perturb], \\\n",
        "                              feed_dict={x_pl: batch_x, \\\n",
        "                                     t: batch_y, \\\n",
        "                                     is_training: True})\n",
        "            loss_D_sum += loss_D_batch\n",
        "            loss_G_fake_sum += loss_G_fake_batch\n",
        "            loss_perturb_sum += loss_perturb_batch\n",
        "            loss_adv_sum += loss_adv_batch\n",
        "\n",
        "        print(\"epoch %d:\\nloss_D: %.3f, loss_G_fake: %.3f, \\\n",
        "            \\nloss_perturb: %.3f, loss_adv: %.3f, \\n\" %\n",
        "            (epoch + 1, loss_D_sum/total_batches, loss_G_fake_sum/total_batches,\n",
        "            loss_perturb_sum/total_batches, loss_adv_sum/total_batches))\n",
        "    #epoch_losses = np.array([loss_D_sum/totalbatches], [loss_G_fake_sum/total_batches], [loss_perturb_sum/totalbatches], [loss_adv_sum/total_batches])\n",
        "\t\t#np.savetxt(\"epoch_{}_losses.txt\".format(epoch), epoch_losses, delimiter=',')\n",
        "    \n",
        "        if epoch % 10 == 0:\n",
        "            g_saver.save(sess, \"weights/generator/gen\")\n",
        "            d_saver.save(sess, \"weights/discriminator/disc\")\n",
        "\n",
        "    # evaluate the test set\n",
        "    correct_prediction = tf.equal(tf.argmax(f_fake_probs, 1), tf.argmax(t, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    accs = []\n",
        "    total_batches_test = int(X_test.shape[0] / batch_size)\n",
        "    for i in range(total_batches_test):\n",
        "        batch_x, batch_y = next_batch(X_test, y_test, i, batch_size)\n",
        "        acc, x_pert = sess.run([accuracy, x_perturbed], feed_dict={x_pl: batch_x, t: batch_y, is_training: False})\n",
        "        accs.append(acc)\n",
        "\n",
        "    print('accuracy of test set: {}'.format(sum(accs) / len(accs)))\n",
        "  #test_acc = np.array(sum(accs)/len(accs))\n",
        "  #np.savetxt(\"test_accuracy_GD.txt\", test_acc, delimiter=',')\n",
        "\n",
        "\t# plot some images and their perturbed counterparts\n",
        "    f, axarr = plt.subplots(2,2)\n",
        "    axarr[0,0].imshow(np.squeeze(batch_x[2]), cmap='Greys_r')\n",
        "    axarr[0,1].imshow(np.squeeze(x_pert[2]), cmap='Greys_r')\n",
        "    axarr[1,0].imshow(np.squeeze(batch_x[5]), cmap='Greys_r')\n",
        "    axarr[1,1].imshow(np.squeeze(x_pert[5]), cmap='Greys_r')\n",
        "    plt.show()\n",
        "\n",
        "    print('finished training, saving weights')\n",
        "    g_saver.save(sess, \"weights/generator/gen\")\n",
        "    d_saver.save(sess, \"weights/discriminator/disc\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def attack(X, y, batch_size=128, thresh=0.3, target=3):\n",
        "    x_pl = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]]) # image placeholder\n",
        "    t = tf.placeholder(tf.float32, [None, y.shape[-1]]) # target placeholder\n",
        "    is_training = tf.placeholder(tf.bool, [])\n",
        "\n",
        "    is_targeted = False\n",
        "    if target in range(0, y.shape[-1]):\n",
        "        is_targeted = True\n",
        "\n",
        "    perturb = tf.clip_by_value(generator(x_pl, is_training), -thresh, thresh)\n",
        "    x_perturbed = perturb + x_pl\n",
        "    x_perturbed = tf.clip_by_value(x_perturbed, 0, 1)\n",
        "\n",
        "    a = Target()\n",
        "    a_real_logits, a_real_probs = a.Model(x_pl)\n",
        "    a_fake_logits, a_fake_probs = a.Model(x_perturbed)\n",
        "\n",
        "    #t_vars = tf.trainable_variables()\n",
        "    #a_vars = [var for var in t_vars if 'Model' in var.name]\n",
        "    g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='g_weights')\n",
        "\n",
        "    init  = tf.global_variables_initializer()\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    sess.run(init)    \n",
        "    \n",
        "    #just using generator and target model\n",
        "    new_saver2 = tf.train.import_meta_graph('./weights/target_model/model.meta')\n",
        "    new_saver2.restore(sess, tf.train.latest_checkpoint('./weights/target_model'))\n",
        "    \n",
        "    #f_saver2 = tf.train.Saver(a_vars)\n",
        "    #g_saver = tf.train.Saver(g_vars)\n",
        "    #f_saver2.restore(sess, \"./weights/target_model/model\")\n",
        "    g_saver = tf.train.import_meta_graph('./weights/generator/gen.meta')\n",
        "    g_saver.restore(sess, tf.train.latest_checkpoint(\"./weights/generator\"))\n",
        "\n",
        "    rawpert, pert, fake_l, real_l = sess.run([perturb, x_perturbed, a_fake_probs, a_real_probs], \\\n",
        "                          feed_dict={x_pl: X[:32], \\\n",
        "                                 is_training: False})\n",
        "    \n",
        "    #changed the way the author named these printed lists - please verify that I interpreted his acronyms properly\n",
        "    print('actual labels: ' + str(np.argmax(y[:32], axis=1)))\n",
        "    print('classifier original prediction: ' + str(np.argmax(real_l, axis=1))) \n",
        "    print('classifier prediction after perturbation: ' + str(np.argmax(fake_l, axis=1)))\n",
        "\n",
        "    correct_prediction = tf.equal(tf.argmax(a_fake_probs, 1), tf.argmax(t, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    accs = []\n",
        "    total_batches_test = int(X.shape[0] / batch_size)\n",
        "    for i in range(total_batches_test):\n",
        "        batch_x, batch_y = next_batch(X, y, i, batch_size)\n",
        "\n",
        "        if is_targeted:\n",
        "            targets = np.full((batch_y.shape[0],), target)\n",
        "            batch_y = np.eye(y.shape[-1])[targets]\n",
        "\n",
        "        acc, fake_l, x_pert = sess.run([accuracy, a_fake_probs, x_perturbed], feed_dict={x_pl: batch_x, t: batch_y, is_training: False})\n",
        "        accs.append(acc)\n",
        "\n",
        "    print('accuracy of test set: {}'.format(sum(accs) / len(accs)))\n",
        "\n",
        "    f, axarr = plt.subplots(2,2)\n",
        "    axarr[0,0].imshow(np.squeeze(X[3]), cmap='Greys_r')\n",
        "    axarr[0,1].imshow(np.squeeze(pert[3]), cmap='Greys_r')\n",
        "    axarr[1,0].imshow(np.squeeze(X[4]), cmap='Greys_r')\n",
        "    axarr[1,1].imshow(np.squeeze(pert[4]), cmap='Greys_r')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#main code\n",
        "Y_train = to_categorical(Y_train, num_classes=43)\n",
        "Y_test = to_categorical(Y_test, num_classes=43)\n",
        "Y_valid = to_categorical(Y_valid, num_classes=43)\n",
        "\n",
        "AdvGAN(X_train, Y_train, X_test, Y_test, batch_size=128, epochs=50, target=3)\n",
        "\n",
        "attack(X_valid, Y_valid, target=3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y shape\n",
            "(34799, 43)\n",
            "y_test shape\n",
            "(12630, 43)\n",
            "t shape)\n",
            "(?, 43)\n",
            "is targeted boolean\n",
            "True\n",
            "WARNING:tensorflow:From <ipython-input-3-5caa55e74d60>:17: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-3-5caa55e74d60>:49: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n",
            "WARNING:tensorflow:From <ipython-input-3-5caa55e74d60>:32: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d_transpose instead.\n",
            "(?, 32, 32, 1)\n",
            "WARNING:tensorflow:From <ipython-input-4-85d97f4f431c>:44: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-4-85d97f4f431c>:45: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "(?, 1)\n",
            "(?, 43)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./weights/target_model/model\n",
            "Pretrained model loaded\n",
            "epoch 1:\n",
            "loss_D: 0.080, loss_G_fake: 0.874,             \n",
            "loss_perturb: 1.512, loss_adv: 6.310, \n",
            "\n",
            "epoch 2:\n",
            "loss_D: 0.028, loss_G_fake: 0.951,             \n",
            "loss_perturb: 0.308, loss_adv: 5.966, \n",
            "\n",
            "epoch 3:\n",
            "loss_D: 0.020, loss_G_fake: 0.968,             \n",
            "loss_perturb: 0.226, loss_adv: 5.958, \n",
            "\n",
            "epoch 4:\n",
            "loss_D: 0.013, loss_G_fake: 0.979,             \n",
            "loss_perturb: 0.143, loss_adv: 5.944, \n",
            "\n",
            "epoch 5:\n",
            "loss_D: 0.010, loss_G_fake: 0.989,             \n",
            "loss_perturb: 0.088, loss_adv: 5.930, \n",
            "\n",
            "epoch 6:\n",
            "loss_D: 0.006, loss_G_fake: 0.995,             \n",
            "loss_perturb: 0.027, loss_adv: 5.882, \n",
            "\n",
            "epoch 7:\n",
            "loss_D: 0.003, loss_G_fake: 0.997,             \n",
            "loss_perturb: 0.007, loss_adv: 5.786, \n",
            "\n",
            "epoch 8:\n",
            "loss_D: 0.003, loss_G_fake: 0.998,             \n",
            "loss_perturb: 0.006, loss_adv: 5.723, \n",
            "\n",
            "epoch 9:\n",
            "loss_D: 0.002, loss_G_fake: 0.997,             \n",
            "loss_perturb: 0.002, loss_adv: 5.655, \n",
            "\n",
            "epoch 10:\n",
            "loss_D: 0.002, loss_G_fake: 0.998,             \n",
            "loss_perturb: 0.003, loss_adv: 5.623, \n",
            "\n",
            "epoch 11:\n",
            "loss_D: 0.002, loss_G_fake: 0.999,             \n",
            "loss_perturb: 0.004, loss_adv: 5.647, \n",
            "\n",
            "epoch 12:\n",
            "loss_D: 0.018, loss_G_fake: 0.998,             \n",
            "loss_perturb: 0.031, loss_adv: 5.845, \n",
            "\n",
            "epoch 13:\n",
            "loss_D: 0.064, loss_G_fake: 0.956,             \n",
            "loss_perturb: 0.009, loss_adv: 5.916, \n",
            "\n",
            "epoch 14:\n",
            "loss_D: 0.554, loss_G_fake: 0.262,             \n",
            "loss_perturb: 0.001, loss_adv: 5.889, \n",
            "\n",
            "epoch 15:\n",
            "loss_D: 0.501, loss_G_fake: 0.252,             \n",
            "loss_perturb: 0.002, loss_adv: 5.884, \n",
            "\n",
            "epoch 16:\n",
            "loss_D: 0.500, loss_G_fake: 0.252,             \n",
            "loss_perturb: 0.003, loss_adv: 5.883, \n",
            "\n",
            "epoch 17:\n",
            "loss_D: 0.500, loss_G_fake: 0.252,             \n",
            "loss_perturb: 0.003, loss_adv: 5.881, \n",
            "\n",
            "epoch 18:\n",
            "loss_D: 0.499, loss_G_fake: 0.252,             \n",
            "loss_perturb: 0.007, loss_adv: 5.880, \n",
            "\n",
            "epoch 19:\n",
            "loss_D: 0.447, loss_G_fake: 0.321,             \n",
            "loss_perturb: 1.501, loss_adv: 5.414, \n",
            "\n",
            "epoch 20:\n",
            "loss_D: 0.227, loss_G_fake: 0.652,             \n",
            "loss_perturb: 4.407, loss_adv: 4.534, \n",
            "\n",
            "epoch 21:\n",
            "loss_D: 0.202, loss_G_fake: 0.696,             \n",
            "loss_perturb: 3.940, loss_adv: 4.662, \n",
            "\n",
            "epoch 22:\n",
            "loss_D: 0.196, loss_G_fake: 0.708,             \n",
            "loss_perturb: 3.619, loss_adv: 4.752, \n",
            "\n",
            "epoch 23:\n",
            "loss_D: 0.185, loss_G_fake: 0.721,             \n",
            "loss_perturb: 3.518, loss_adv: 4.781, \n",
            "\n",
            "epoch 24:\n",
            "loss_D: 0.178, loss_G_fake: 0.732,             \n",
            "loss_perturb: 3.429, loss_adv: 4.806, \n",
            "\n",
            "epoch 25:\n",
            "loss_D: 0.171, loss_G_fake: 0.742,             \n",
            "loss_perturb: 3.378, loss_adv: 4.821, \n",
            "\n",
            "epoch 26:\n",
            "loss_D: 0.166, loss_G_fake: 0.749,             \n",
            "loss_perturb: 3.312, loss_adv: 4.840, \n",
            "\n",
            "epoch 27:\n",
            "loss_D: 0.172, loss_G_fake: 0.743,             \n",
            "loss_perturb: 3.098, loss_adv: 4.902, \n",
            "\n",
            "epoch 28:\n",
            "loss_D: 0.176, loss_G_fake: 0.738,             \n",
            "loss_perturb: 2.938, loss_adv: 4.949, \n",
            "\n",
            "epoch 29:\n",
            "loss_D: 0.171, loss_G_fake: 0.744,             \n",
            "loss_perturb: 2.921, loss_adv: 4.954, \n",
            "\n",
            "epoch 30:\n",
            "loss_D: 0.170, loss_G_fake: 0.747,             \n",
            "loss_perturb: 2.882, loss_adv: 4.965, \n",
            "\n",
            "epoch 31:\n",
            "loss_D: 0.173, loss_G_fake: 0.744,             \n",
            "loss_perturb: 2.831, loss_adv: 4.980, \n",
            "\n",
            "epoch 32:\n",
            "loss_D: 0.166, loss_G_fake: 0.751,             \n",
            "loss_perturb: 2.793, loss_adv: 4.991, \n",
            "\n",
            "epoch 33:\n",
            "loss_D: 0.169, loss_G_fake: 0.749,             \n",
            "loss_perturb: 2.708, loss_adv: 5.017, \n",
            "\n",
            "epoch 34:\n",
            "loss_D: 0.166, loss_G_fake: 0.753,             \n",
            "loss_perturb: 2.686, loss_adv: 5.023, \n",
            "\n",
            "epoch 35:\n",
            "loss_D: 0.167, loss_G_fake: 0.755,             \n",
            "loss_perturb: 2.668, loss_adv: 5.028, \n",
            "\n",
            "epoch 36:\n",
            "loss_D: 0.166, loss_G_fake: 0.757,             \n",
            "loss_perturb: 2.686, loss_adv: 5.023, \n",
            "\n",
            "epoch 37:\n",
            "loss_D: 0.236, loss_G_fake: 0.679,             \n",
            "loss_perturb: 1.914, loss_adv: 5.260, \n",
            "\n",
            "epoch 38:\n",
            "loss_D: 0.268, loss_G_fake: 0.637,             \n",
            "loss_perturb: 1.582, loss_adv: 5.364, \n",
            "\n",
            "epoch 39:\n",
            "loss_D: 0.270, loss_G_fake: 0.627,             \n",
            "loss_perturb: 1.508, loss_adv: 5.388, \n",
            "\n",
            "epoch 40:\n",
            "loss_D: 0.272, loss_G_fake: 0.621,             \n",
            "loss_perturb: 1.446, loss_adv: 5.408, \n",
            "\n",
            "epoch 41:\n",
            "loss_D: 0.271, loss_G_fake: 0.618,             \n",
            "loss_perturb: 1.411, loss_adv: 5.419, \n",
            "\n",
            "epoch 42:\n",
            "loss_D: 0.272, loss_G_fake: 0.616,             \n",
            "loss_perturb: 1.378, loss_adv: 5.430, \n",
            "\n",
            "epoch 43:\n",
            "loss_D: 0.277, loss_G_fake: 0.609,             \n",
            "loss_perturb: 1.325, loss_adv: 5.447, \n",
            "\n",
            "epoch 44:\n",
            "loss_D: 0.277, loss_G_fake: 0.607,             \n",
            "loss_perturb: 1.302, loss_adv: 5.455, \n",
            "\n",
            "epoch 45:\n",
            "loss_D: 0.279, loss_G_fake: 0.605,             \n",
            "loss_perturb: 1.274, loss_adv: 5.464, \n",
            "\n",
            "epoch 46:\n",
            "loss_D: 0.278, loss_G_fake: 0.605,             \n",
            "loss_perturb: 1.257, loss_adv: 5.469, \n",
            "\n",
            "epoch 47:\n",
            "loss_D: 0.280, loss_G_fake: 0.603,             \n",
            "loss_perturb: 1.240, loss_adv: 5.475, \n",
            "\n",
            "epoch 48:\n",
            "loss_D: 0.282, loss_G_fake: 0.602,             \n",
            "loss_perturb: 1.215, loss_adv: 5.483, \n",
            "\n",
            "epoch 49:\n",
            "loss_D: 0.281, loss_G_fake: 0.601,             \n",
            "loss_perturb: 1.203, loss_adv: 5.487, \n",
            "\n",
            "epoch 50:\n",
            "loss_D: 0.282, loss_G_fake: 0.600,             \n",
            "loss_perturb: 1.181, loss_adv: 5.495, \n",
            "\n",
            "accuracy of test set: 0.020169005102040817\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAD8CAYAAADub8g7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "finished training, saving weights\n",
            "INFO:tensorflow:Restoring parameters from ./weights/target_model/model\n",
            "INFO:tensorflow:Restoring parameters from ./weights/generator/gen\n",
            "actual labels: [33  5 18 31 11 28 25 12 39  8  9  1 12 38 10 28 12 40 17 38 21 25 16  8\n",
            "  5  6 12  7 38 14 31 14]\n",
            "classifier original prediction: [23  0 23 23 23 23 23  5 23 23  0  0 23 23 23  0  0 23 23  5  0  5 23  0\n",
            " 23 23 23  0  0  5 23 23]\n",
            "classifier prediction after perturbation: [31 18 31 31 31 31 31 31 31 18 18 31 31 31 31 31 31 17 31 31 31 31 31 31\n",
            " 31 31 31 15 31 31 31 31]\n",
            "accuracy of test set: 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAD8CAYAAADub8g7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnVuMXdWRhv9q08YG39r39gW3Dc3F\nOMEGY0wgEUpAIbyAlFEURoocCcl5mJESaR5AeZqRZiTmJTNvI1kCxYqiAaREwhqhEOS0xYzkGF/A\n+O72BV/bd2y3bWxjXPPQ56z+z85Zp3ffdvf2+j8JUWf33muvfVy9umpXrSpzdwghREo0jfQEhBCi\naLTwCSGSQwufECI5tPAJIZJDC58QIjm08AkhkkMLnxAiOQa18JnZi2a2z8wOmNkbQzUpIUYa6fad\njQ00gdnMxgDYD+AFAMcBbAbwqrvvHrrpCVE80u07n7sGce0KAAfc/RAAmNk7AF4GEFWOlpYWnzNn\nDgBg3Lhx4biZ1T3/m2++CfK1a9dqfnb58uUgf/XVV0G+evVq3bF4gb99+3bd47F5MNlzxowZU3dc\nnjtfk0cezPyyxP6wufs5d5/R7wHToF+6Lb0un14PZuGbC+AYfT4O4KlGF8yZMwfvvPMOAKC9vT0c\nb25uDjJ/CawEW7durRmro6MjyDt37gzypk2bgsxf9I0bN4LMCsXnxP7Bmpp63wiwQgDAlClTgszK\neeXKlbpj3X333UG+667er3/s2LF1r+X58TzqzaXeNTGlvXnz5pG6Fwugn7otvS6fXg97cMPMVpvZ\nFjPb8uWXXw737YQoBOl1uRmMxXcCwHz6PK9yrAZ3XwNgDQAsWrTIP//8cwC1fw3b2tqCzK7ChAkT\ngvzYY49lx607qZs3bwa5ei/gb12KKjEzO/ZXJ3vfW7du1f1Z7K8pn/P1118Hmf9qxdyXgbgETPYv\nq4jSp25Lr8ut14P5TdgMoN3MFprZWAA/BbBuEOMJMVqQbt/hDNjic/dbZvaPAD4EMAbA2+6+a8hm\nJsQIId2+8xmMqwt3/wDAB3nPN7Ng8u/du7fuOQsWLAjy+PHjgzxp0qSa85YuXRpkNq3Z9OfjO3bs\nCDKb8WyKM3wOm/SNzHI+j1/qxq7n+fH92K3ha7MmPb9EZnjcmDsiGtMf3ZZel0+v9dJHCJEcWviE\nEMkxKFe3vzQ1NYV8H8452r9/f5A52hNzDwBg4sSJQV6xYkWQ2cRn0/j06dNBvn79epA5D4qvzZts\nyTlOMZcgFnXie7NLwDS6d558J34mRXWHB+l1LWXQa/0mCCGSQwufECI5CnV13T2YvhzhmTGjd2vd\nvffeG+RGkZ/Yz+bOnVtXnj17dpDPnj0bZJ5HbDsPwwmq2Wti17OJzm5KLKGTr+UIFyfBAnGXILaf\nUq7u8CC9Lp9e6zdBCJEcWviEEMlRqKt769YtXLhwAQAwf37vVkiOck2bNi3IbA5nEzI5ofPSpUtB\n5ijXxYsXg8xRqlikiWHTn03vbBSOP3NUjV2NWEJnLMk05u5kEzXzlAKKnS+GDul1+fRaFp8QIjm0\n8AkhkkMLnxAiOQp9x3f33Xdj0aJFAGrff0yfPj3IHNpm35+rtwJAV1dXkA8ePBjkTz75JMiHDx8O\nMr8j4XcT/H7gnnvuCTK/k6mWFQeA8+fP18yDN5nzPXi+/F6Ew/tMrM4Zvw+Kbd7O3mOwZb1F/5Be\nl0+vZfEJIZJDC58QIjkKdXXHjx+PJUuWAKjNZOcQO5vMbFYfPXq0Ziwuv719+/Ygs3vQ2dkZZE4N\niNUk4zlxdvzMmTPrzjU7X3ZneDM3m+uxLHNu1sImPc+VN39nz+MUAp5jrOGNGDqk1+XTa1l8Qojk\n0MInhEiOQl3dMWPGhH6dbBqzyVzNgAeAL774IsifffZZzVj8ed++fUE+cqS3rSb3L41Fh9hkbmlp\nCTJvMOe5Zl0Cbi3IJjdnvvO9Y7XRYlGxWOluIB4l43nwObHN32JwSK/Lp9ey+IQQyaGFTwiRHIW6\numYWzGuO5Jw6dSrIHL3aunVrkLmbFADs2tXb7e/YsWNB7u7uDnKsdhibz7GI19SpU4PMkbNz587V\nzINLjXPEi10CPoc3lfOc2PTnebOcjV7xc/B57HbwBnN+VjF0SK/Lp9d9Wnxm9raZnTGznXRsqpl9\nZGadlf+3NBpDiNGIdDtd8ri6vwXwYubYGwDWu3s7gPWVz0KUjd9Cup0kfbq67v6xmbVlDr8M4LmK\nvBbABgCv9zXW7du3g4l64sSJcJzN+23btgV59+7ddWWgdk8jm+yxxMZY7THer8hlvDlCxhGurFnN\nZn22plkVdgNYZmL7EBvVI4u5ASzHImRi6HRbel0+vR5ocGOWu1f/hU4BmDXAcYQYbUi3E2DQUV3v\nWdI99nMzW21mW8xsS/YFqhCjmUa6Lb0uNwP1fU6bWau7d5lZK4AzsRPdfQ2ANQCwePFirzZZZjeA\nS+7s3BneM+PQoUNBzioXl+iOlbhhc533DPIexXnz5tU9n10ObvL8/PPP18xjy5YtQeZ9lxzNiu0x\n5DJCPO9Yie5safFYAinDY3EUTkTJpdvS63Lr9UAtvnUAVlXkVQDeH+A4Qow2pNsJkCed5b8BbATw\nkJkdN7PXALwJ4AUz6wTwfOWzEKVCup0ueaK6r0Z+9IP+3uzq1avYuHEjAODTTz8Nxznidfz48SDH\n9iQCtZEgNuU5isRm9uTJk4PMbgCfz3souWrt008/HeTvfOc7NfPghFVOMuWIWazED9+bj8eaMGdd\nAnYDYpVu+XvK04UrJYZKt6XX5dNrbVkTQiSHFj4hRHIUmtHa3d2Njz/+GEBt+R2ObHGEJrbPD6g1\ne9k0ZhOYzWzeo8gJmZx4yW7Dd7/73SA/+eSTQc4mS7a1tQWZE055HlxSiKNf/HyxRM+Yu5OdC18T\ncw9iJYLE4JBel0+vZfEJIZJDC58QIjkKd3U3bNgAoNYU5xIzMbM1W2U1tm+P4f2Hs2b17jxiU5qj\natwHlZNBea6c9AnURtK4xBBH7risDyecctMZPp/nxy5E1iWIRfpiiaKxhjBicEivy6fX+k0QQiSH\nFj4hRHIU6urevn07mNexHqB5e2TGzuPj7e3tQWbTnc1sdgnY3L948WKQDx8+HOSXXnqp5n7sOqxc\nuTLInLDKkb6YexCLfnEiadZdipn4PBZfz3soxdAhvS6fXsviE0IkhxY+IURyjJirG6ugysez0Z4Y\nbOIvXbo0yIsXLw4yJ2FyE5j777+/7rXcB5X3J/JeR6A2aZTv98QTT9S9nksSxUx0Ph5LYgXiiZ6x\na2JRPzE4pNfl02tZfEKI5NDCJ4RIDi18QojkKLztVjVUnWcjMpOtucXXc8PkhQsXBvn8+fNB5vLZ\nDz30UJB/+MMfBpmz4Ds6OoLM7zL27dtXM4/HH388yJyxPm3atCDfd999QeZ0An6Pwu8mONs9lsWe\nhc9jmVMFYmW8xeCRXpdLr2XxCSGSQwufECI5CnV1zSyY/7Hy0rFMbw7tA8CECROCPH/+/Lr3YzeA\nmypzJjqb67FuTZwl/rvf/a7mZwcPHqw7LrspU6ZMCXLMRJ8xY0aQ2fTndIVshnts43vseN40CtE/\npNfl02tZfEKI5NDCJ4RIjsKjulXYHGZTlTc7s6uQdQm4nDZvqObscC6fzREv3uTN43LUicfh+Z04\ncaJmHmvXrg0yZ69zByveSM7y1q1bg8zRvdh3k/0O2GXi+bJLwOfI1R1+pNfl0Os8fXXnm1mHme02\ns11m9svK8alm9pGZdVb+39Lvuwsxgki30yWPq3sLwD+5+2IAKwH8g5ktBvAGgPXu3g5gfeWzEGVC\nup0oeRqKdwHoqsjdZrYHwFwALwN4rnLaWgAbALzeaCwzC2Zto05TdO8gc7QLAB544IEgc6epY8eO\nBZkjUJzEOWnSpCCzC8JuxjPPPBPkRx55JMjXrl2rmcf27duDzLXKuLE0z4PHYtOfmz5zMiiXB89G\ntXjje6xMOSNXt5ah0m3pdfn0ul/BDTNrA7AMwCYAsyqKAwCnAMyKXCbEqEe6nRa5gxtmNgHAHwD8\nyt0vZ14uupnVXXbNbDWA1RV5cLMVYhgYiG5Lr8tNroXPzJrRoxi/d/c/Vg6fNrNWd+8ys1YAZ+pd\n6+5rAKwBgDFjxgQF4mRGNnVZ5gTJBx98sGZcrjfGiZjc2aq1tTXIS5YsCXJLS++7anZHuEQ3d7Li\nZNDs3kqe18aNG4N89uzZIHMD6FhEj2V2XzjRk90DIB4lizWlFn/LQHVbel1uvc4T1TUAbwHY4+6/\noR+tA7CqIq8C8P6gZiJEwUi30yWPxfcMgJ8B2GFm1e4ivwbwJoD3zOw1AEcA/GR4pijEsCHdTpQ8\nUd3/AxB7ifGD/tzM3YNJHXMJuEMTm8bZfYtc7mbv3r1B5oRH3sfIUSsux8NRNTbX8zYv5r2I3/ve\n94LMESyeEyeKsgvCrgyXAWd3gl0f4G/dkyrsBsTcLTF0ui29Lp9ea8uaECI5tPAJIZKj0L267h7M\n0pg5yy4BJ2FmEyw5KsQVaZktW7YEmfcPcmSLI2x8PzbF2X3JdqPiqBXPnV0KnjtXs+W9mDwu763k\nMbOVamMdpWJVa5XAPDxIr8un17L4hBDJoYVPCJEcI1aWKgabrWw+nzx5suY8No8ffvjhutfw/kaO\nCLEpncdkjjUvBmpNfzbfY8mdDM+J3QCeH++/zFbS5WiYEpVHN9Lr0aXXsviEEMmhhU8IkRyFu7pV\nsztWTZUTHjlZMmuusyk+ffr0ILMJXe++QL4EyVjl2GyZHP7M94jts+RIGLsK7E5wX1KOrvHx7Fix\nhjKiGKTX5dJrWXxCiOTQwieESI5REdVll4DNb957yNEhoDZZk6NF3PSE90TGEjI5SsWJnjwnlvna\n7DUMm/vd3d1B5gRVhsv9cOIqfwdZtyg2R4av4Tlp3+7wI70evXoti08IkRxa+IQQyaGFTwiRHIW/\n46v64Oy7s48eq7mV9fX53QZ3o+LS3Zz5zvXJOEs9m7Fe796cAsA1yBr9LFYvjJ/v3LlzQeZuVPwu\nhFMGWM7eL9ZgOZadH9sILgaG9Lpcei2LTwiRHFr4hBDJMWLpLLFN1Gw+N9pozedx5je7B3wNH582\nbVqQY+4Bm9Js9mdNaTbT2UTnzPejR48GmcP4XKK7q6sryKdOnUI9YpvCG8HPwSkKcnWHB+l1OfRa\nFp8QIjm08AkhkmNU7NyIZVvHojjZa7gh8Zkzvb2fudYZZ8hzZys2y2OZ72zGZzPU2bTmzdW8qZyv\n52z32GbsWFZ69juIRbz4u+HrY9n4YniQXo9evc7TUHycmX1iZtvNbJeZ/Uvl+EIz22RmB8zsXTPT\nb5UoFdLtdMnj6t4A8H13fwzAUgAvmtlKAP8O4D/c/QEAXwJ4bfimKcSwIN1OlDwNxR1Ate51c+U/\nB/B9AH9fOb4WwD8D+K++xqsXwYmZto2iPRyp4qRP7i7FESg2kzs7O4PM7gSfM3ny5CCzeZ9NDOUu\nUuxexCJ3fH1sYzfTaMN2nqghX6PCBLUMpW5Lr8ul17mCG2Y2xsw+A3AGwEcADgK46O7VePhxAHP7\nfXchRhjpdprkWvjc/Rt3XwpgHoAVAB7u45KAma02sy1mtqXvs4UoloHqtvS63PQrquvuF82sA8DT\nAKaY2V2Vv4zzAJyIXLMGwBoAaGpq8qqZHtu7GOsIlS2NHStvzS4BR6b4fD6HE0A5oZNlTry8cOFC\nzTzYdeBxY8mkbKJzgmpsj2ejZNc83bP4e4vVNhP9123pdbn1Ok9Ud4aZTanI4wG8AGAPgA4Af1c5\nbRWA9/t9dyFGEOl2uuSx+FoBrDWzMehZKN9z9/8xs90A3jGzfwXwKYC3hnGeQgwH0u1EsZhZOSw3\nMzsL4CqAc32dewcyHaPruRe4+4yRnsSdQEWvj2D0/RsXxWh67lx6XejCBwBmtsXdlxd601FAqs+d\nEqn+G5fxubVXVwiRHFr4hBDJMRIL35oRuOdoINXnTolU/41L99yFv+MTQoiRRq6uECI5tPAJIZKj\n0IXPzF40s32VOmdvFHnvIjGz+WbWYWa7K3Xeflk5PtXMPjKzzsr/W0Z6rmLwSK/Lp9eFveOrZMfv\nR8+2oOMANgN41d13FzKBAjGzVgCt7r7NzCYC2ArgFQA/B3DB3d+s/IK0uPvrIzhVMUik1+XU6yIt\nvhUADrj7IXe/CeAdAC8XeP/CcPcud99WkbvRs/9zLnqed23ltLXoURpRbqTXJdTrIhe+uQCO0eck\n6pyZWRuAZQA2AZjl7tV+e6cAzBqhaYmhQ3pdQr1WcGMYMbMJAP4A4Ffufpl/Vqn+q1wiUTruBL0u\ncuE7AWA+fY7W8LsTMLNm9CjH7939j5XDpyvvSarvS87ErhelQXpdQr0ucuHbDKC90sFqLICfAlhX\n4P0Lw3oqI74FYI+7/4Z+tA499d0A1Xm7U5Bel1Cviy5L9RKA/wQwBsDb7v5vhd28QMzsWQD/C2AH\ngGoZ3l+j533IewDuQ08Zo5+4+4W6g4jSIL0un15ry5oQIjkU3BBCJMegFr5UMtZFeki372wG7Oqm\nlLEu0kK6fefTr/aSGULGOgCYWTVjPaoczc3NPm7cOFTOD8dv3LgRZG7P1whuL3fvvfcGmVv6Xb9+\nve5xbovH43DbPj6fu8pzd/vsfPl+PFasAzy322MG8sdo1qzenFFuB1j9vrP33rp16zn13IjSL92W\nXpdPrwez8NXLWH+q0QXjxo3DsmXLglzlwIEDQb506VLda7Nf2sSJE4O8YsWKIH/99ddB3rlzZ5Cv\nXbsW5JkzZwaZ+4ceO9b7ON3d3UFevry3nQBfCwBXrlwJ8p49e+qOxcoyduzYIPN3wPCzNurNysr9\ni1/8Isg//vGPg9ze3l733mZ2pO7NBdBP3ZZel0+vB7Pw5cLMVgNYDdQ2SBaizEivy81gFr5cGevc\ncX7SpEleNcf5LxWbxmyuN/qrwKY5/4Xh1b+lpbc6DivnvHnzgtzW1lb33vv27Qsym9KNuraze8Dz\n5XF53izztdztnsm6EPxM/FeSZb63Updy06duS6/LrdeDieomk7EukkO6fYczYIvP3W+Z2T8C+BC9\nGeu7hmxmQowQ0u07n0G943P3DwB8kPf8pqam8PKWX55OmDAhyBxBipnYADB+/Pggs9nL13DUis1k\n5quvvgoyv0BmN4NN96xLEHNb+Bq+Nx/nefO47BLwC+RGLgHfm89r5MKIOP3Rbel1+fRaOzeEEMmh\nhU8IkRzDns7CuHtI6mTTOBbFYXM7C5u6V69eDXLMjeDjbGZfvtxbR5HdFIbdAzbjgVrzOxaRi7kU\nPD9+7li+UzYJll0YlvkalmNukRgc0uvy6bUsPiFEcmjhE0IkR+GubtUcZzcgFvlhEzgbxWET+OzZ\ns0GORZ3YdOcIG8+Dj0+aNCnIvF9w2rRp0XnwWPwcsf2U/Ewx053lWAIoEN9Dyd9hbA+lGBzS6/Lp\ntX4ThBDJoYVPCJEchbq6TMzsZXOWTeCsOcsmNEeaZszorUjDZvLFixfrHudruQwQ73vkhNEFCxbU\nzIP3UHZ2dkbnWyVWyofh74Pnxy5LdqxG+z/rnSOGB+l1OfRaFp8QIjm08AkhkkMLnxAiOQpPZ6n6\n6fw+gt8JxDZmZ/14fk/C7w5iG8M5k53H5Xck/B6F5XPnzgX59OnTNfN44IEHgnz//fcHmVMRODUg\nVgY89o6E0xiy30EsvJ9nXDF0SK/Lp9ey+IQQyaGFTwiRHIW6urdv364pzV2FzdlYie5sdncsE57d\nA65txiH9WDoBZ7WzKX3y5MkgZ83qhx56qK7MTVnYpeCmM7EuXHnKbWfnznIsHUA7N4YH6XX59Fq/\nCUKI5NDCJ4RIjsKjutVIEJutbBrHanlla3axWc+RLa5hxj1KeUN2rBkxZ6vHaphduHCh5vOnn34a\n5FgkjCNseTpbsRnfqFFzrK1h3u5ZYmiQXpdPr2XxCSGSQwufECI5Co/qVk32PEmKsXLWQK1LwNEl\nNvfZDYjVJ+Nz+H7nz58P8pw5c4LM7gsAHD58OMizZ88O8qOPPhrkEyd6e1F/+eWXdZ8pJjeKWMUi\ngCpGUCzS6/LpdZ8Wn5m9bWZnzGwnHZtqZh+ZWWfl/y2NxhBiNCLdTpc8ru5vAbyYOfYGgPXu3g5g\nfeWzEGXjt5BuJ0mfrq67f2xmbZnDLwN4riKvBbABwOt9jXX79u0QVcqz147P4X2BWdhM7+rqCjJH\nqjhJkhNAW1p6/6Bz5IxrlT377LN1xwSAbdu2BZlN/8cffzzI7e3tQf7iiy+CzE2fY12nGtUj42fi\nvZL8fbD7owTmWoZKt6XX5dPrgb7jm+Xu1X+JUwBmxU40s9UAVgNqbyhKQS7dll6Xm0GbAN6zXNcv\njdrz8zXuvtzdl8viEGWikW5Lr8vNQC2+02bW6u5dZtYK4Eyei8wsJFPGTF02/WPRHaDWvGUTmMfl\nZE12O9ra2urej8fh/YnLli2rOyZQ6wawuX/fffcF+dvf/naQt2/fHmSO2uWJXmW/Az6P587uwalT\np4LMiawiSr91W3pdPr0e6J+qdQBWVeRVAN4f4DhCjDak2wmQJ53lvwFsBPCQmR03s9cAvAngBTPr\nBPB85bMQpUK6nS55orqvRn70g/7erKmpKSRixhoFx0r2ZCNksX17sSgSV3zlKrQcgeJk0G9961tB\nnjJlSpB5nyQALF++PMh/+tOfgrx///4gP/HEE0HmvY58Ds+DiSWAArXPyq4NP3cjtyp1hkq3pdfl\n02u9lRVCJIcWPiFEchS+V7capYntV8ybGsBJjrHqtjwuN2thmaNZc+fODTJHv2L7LLPn7dwZdj5F\nK9UuWbIkyJwkykmmsehX9rthN4d/xtdzjhknr4qhQ3pdPr2WxSeESA4tfEKI5Cjc1e3u7gZQa8Ky\nSR9zFRpVXOXIT6yXaWtra808qvA8Fi9eHGSOhLHpnm0Owy4BJ3QePXo0yJzcuXLlyiA/88wzQea9\nklzip1FTldj3xq7C9OnTgzx58mSIoUd6XT69lsUnhEgOLXxCiOQo1NVtamoKfUA56sQJiLF9d40S\nPWPRMzaBZ86cGWSONPHxWBLmu+++W3dOAPDKK68EmZuycMmePXv2BJn3QD788MNB3rFjR5CrbhNQ\nmxCbTdTkz7F9new2aK/u8CC9Lp9ey+ITQiSHFj4hRHIU6uqOHTsWCxcuBAAcOXIkHGcTnU19TlLM\nJj+yqRvrWcoldDiaxWY2713kBNAPPvggyH/961+DnK2Yy5VuH3zwwSBzpVp2Aw4dOhRkTizlvZEc\nOTt79myQs5G3WHVaUSzS6/LptSw+IURyaOETQiSHFj4hRHIU+o7PzMK7Dq4Fxs2PL126FOSLFy9G\nx+J3IRzq58bLfA/uysTHOTzPYfQzZ3orjnOKQvadDL/n4GbNsXSCzZs3B5lTCzhlgEuCd3R0BDn7\nLoTnwu9F+H1NttaZGHqk1+XTa1l8Qojk0MInhEiOQl3dGzduhI5NbJZzuPzAgQNBbuQSMBzSnzdv\nXpC5Theb9dXUA6DWteBMdO7ixKZ41sQ+fvx4kNl85zA+m/ic7sDuxOzZs4PMLsSuXbuCfPLkyZp7\n81y4NlqjVAEx9Eivy6fXsviEEMmhhU8IkRyFurrffPNNiG5xRjZvur527VqQObqTjTpxtIdNf67T\nxREvPoc3UXPE6rPPPgvy6dOng8xZ81mXgLtIceSOM/U5Evboo48GecOGDUFmV4Hn9+STTwb5z3/+\nc/Te/BxcHpzPiZX+FoNDel0+vc7TV3e+mXWY2W4z22Vmv6wcn2pmH5lZZ+X/LX2NJcRoQrqdLnlc\n3VsA/sndFwNYCeAfzGwxgDcArHf3dgDrK5+FKBPS7UTJ01C8C0BXRe42sz0A5gJ4GcBzldPWAtgA\n4PU+xgqmPLsEe/fuDXK1rhlQa1Y36lLF5v748eODzCb6okWLgszRNoYTJDmqxW4KR8uA2g3jzz33\nXJDZrJ80aVKQuQkzPzdH27gx9GOPPRZkrm0G1G4M50jhwYMHg8wRM06oFUOn29Lr8ul1v97xmVkb\ngGUANgGYVVEcADgFoO7dzWw1gNUVud8TFKII+qvb0utykzuqa2YTAPwBwK/c/TL/zHvejNbdQ+Lu\na9x9ubsvl4KI0chAdFt6XW5yWXxm1owexfi9u/+xcvi0mbW6e5eZtQI4Ex+hh6ampmC+c1SGkxE5\nQhNrLAzUmv6c3MmuBpvvHHWaOnVq3XHZ/N64cWOQ58yZE+QFCxbUzIM7R3FnKr4H/2KwWf7UU08F\n+cMPPwwyR694TtztCqiN0HHtt88//zzIvIeSvyfRw1DotvS6fHqdJ6prAN4CsMfdf0M/WgdgVUVe\nBeD9ft9diBFEup0ueSy+ZwD8DMAOM6smBP0awJsA3jOz1wAcAfCT4ZmiEMOGdDtR8kR1/w9A7CXG\nD/pzs+bm5mASsznLbkAs4pV1CXhPJJvf3Mkptk+Q90DyuBxF40bNS5YsCTK7FkDt/kGOhLHLwlE1\nThrlsTgSxnsreXwuJ569hmWOfrFrwyXBxdDptvS6fHqtLWtCiOTQwieESI5C9+qOGzcumMEcpbp8\n+XLd83n/YHY/Hu+DZDji9cgjjwR52rRpQY41bWZzfenSpUHmqBa7GUCt6R/rkMUyR/r4Gdg96Ozs\nDPK+ffvqzin7mUsB8ffJnbSySapiaJBel0+vZfEJIZJDC58QIjkK9X2amppC0xRuWMyleGKmdNac\n5b2PnDTKJvr8+fPrjsVJkQzPI5YUyYmdQO2+yVhEL9YwhcsLsRnP1164cCHI3CgGANra2oLMkS3e\n38iuwl/+8pfs44ghQHpdPr2WxSeESA4tfEKI5CjU1b158yaOHj0KoNYs52gUyxyNypro3GeUx+J9\nkLw3kCu5slnOMkfkuBcpy3xO9nNM5ogXuwc8LrsHbNLzONXvrsrEiRODzCV/+Dvke2ddCjE0SK/L\np9ey+IQQyaGFTwiRHIW6utevXw9JjLyXMLaPkY9ztAuoNXV5HyO7B+vXr687FkfC2Czn47Hzswmn\nA7mmHuya8Dhjx46tew4A7N+dqm1MAAAE70lEQVS/P8gceWN4LH5WMXRIr+OMVr2WxSeESA4tfEKI\n5NDCJ4RIjkLf8XE3Ks5Y51A9w+9FuNYYEC/lzXDInFMIYu9bYpuuea7ZTHsel8m+t6jC7yO4yxWH\n/fnZ+Pxz587VjMXXc0g/9l6FyTaQFgNHel0+vZbFJ4RIDi18QojkGLECbTGTlE1sNoe7urpqzuPs\nbt7MzdewqxALf8eyz9md4NA7y9lxeSxOP2ATna+Pnc/HY526svPle8Q2xIvhR3pdDr2WxSeESA4t\nfEKI5CjU1W1ubg4lrmNRJzZt2RzmrkxAbT0vNqc5ihQzh2PRJT7OMpf35nprAHDlypW692aXgp+J\n3ZRY1C622ZxdgLxwpC9Ppr3oP9Lr8ul1nobi48zsEzPbbma7zOxfKscXmtkmMztgZu+a2di+xhJi\nNCHdTpc8ru4NAN9398cALAXwopmtBPDvAP7D3R8A8CWA14ZvmkIMC9LtRMnTUNwBVO3e5sp/DuD7\nAP6+cnwtgH8G8F+Nxpo8eTJ+9KMfAagtbc2m7qFDh4J85MiRIGcTJ9nk5ugXuwexDdV5NmDzcTbd\nJ0yYUDOPWH0zdg9iEaxYQ2aGE0uzSZsxd0aR3HwMlW5Lr8un17mCG2Y2xsw+A3AGwEcADgK46O7V\nJz8OoG47czNbbWZbzGwLZ2QLMRoYqG5Lr8tNroXP3b9x96UA5gFYAeDhvDdw9zXuvtzdl99zzz0D\nnKYQw8NAdVt6XW76FdV194tm1gHgaQBTzOyuyl/GeQBONL66xyV46aWXAACHDx8Ox7nuGJv33Ekp\n6xLEzF42odnVyGMyx+qnsbmeVXJ2Tfgvf6wUd54IVOzeWZeAz2OZid1PEd5aBqPb0uvy6XWeqO4M\nM5tSkccDeAHAHgAdAP6uctoqAO/nuqMQowTpdrrksfhaAaw1szHoWSjfc/f/MbPdAN4xs38F8CmA\nt4ZxnkIMB9LtRLEiI4BmdhbAVQDn+jr3DmQ6RtdzL3D3GX2fJvqiotdHMPr+jYtiND13Lr0udOED\nADPb4u7LC73pKCDV506JVP+Ny/jc2qsrhEgOLXxCiOQYiYVvzQjcczSQ6nOnRKr/xqV77sLf8Qkh\nxEgjV1cIkRyFLnxm9qKZ7auU+3mjyHsXiZnNN7MOM9tdKXf0y8rxqWb2kZl1Vv7f0tdYYvQjvS6f\nXhfm6laSRPejJzv+OIDNAF51992FTKBAzKwVQKu7bzOziQC2AngFwM8BXHD3Nyu/IC3u/voITlUM\nEul1OfW6SItvBYAD7n7I3W8CeAfAywXevzDcvcvdt1XkbvRsg5qLnuddWzltLXqURpQb6XUJ9brI\nhW8ugGP0OVrK6k7CzNoALAOwCcAsd6+21ToFYNYITUsMHdLrEuq1ghvDiJlNAPAHAL9y98v8s0oR\nTIXURem4E/S6yIXvBID59DlXKauyYmbN6FGO37v7HyuHT1fek1Tfl5wZqfmJIUN6XUK9LnLh2wyg\nvdLIZSyAnwJYV+D9C8N6Coy9BWCPu/+GfrQOPWWOAJU7ulOQXpdQr4uuzvISgP8EMAbA2+7+b4Xd\nvEDM7FkA/wtgB4BqZcRfo+d9yHsA7kNPNY+fuPuFuoOI0iC9Lp9ea+eGECI5FNwQQiSHFj4hRHJo\n4RNCJIcWPiFEcmjhE0IkhxY+IURyaOETQiSHFj4hRHL8P8EczleP96m+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fullGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tortoisehare/TSR-GAN/blob/master/fullGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABnslutLuWVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir weights\n",
        "!mkdir weights/target_model\n",
        "!mkdir weights/generator\n",
        "!mkdir weights/discriminator\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-dNiamCtfBr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "outputId": "d83c181e-54cc-46b1-bfca-a7a9fa73e4b4"
      },
      "source": [
        "#TARGET CLASSIFIER\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(1187) #to help reproduce\n",
        "\n",
        "#from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import csv\n",
        "#import random\n",
        "#import cv2\n",
        "#from skimage.filters import rank\n",
        "#import skimage.morphology as morp\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "#from keras.utils import to_categorical\n",
        "#from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "#from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "\n",
        "class Target:\n",
        "    def __init__(self, lr=0.001, epochs=15, n_input=32, n_classes=43, batch_size=20, restore=0):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.n_input = 32\n",
        "        self.n_classes = 43\n",
        "        self.batch_size = batch_size\n",
        "        self.restore = restore\n",
        "\n",
        "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "    \n",
        "    '''\n",
        "    def next_batch(self, X, Y, i, batch_size):\n",
        "      idx = i*batch_size\n",
        "      idx_n = idx + batch_size\n",
        "      return X[idx:idx_n], Y[idx:idx_n]\n",
        "      '''\n",
        "    \n",
        "    def Model(self, x):\n",
        "        with tf.variable_scope('Model', reuse=tf.AUTO_REUSE):\n",
        "            # Hyperparameters\n",
        "            mu = 0\n",
        "            sigma = 0.1\n",
        "            n_out = self.n_classes\n",
        "            learning_rate = self.lr\n",
        "\n",
        "            # Layer 1 (Convolutional): Input = 32x32x1. Output = 28x28x6.\n",
        "            filter1_width = 5\n",
        "            filter1_height = 5\n",
        "            input1_channels = 1\n",
        "            conv1_output = 6\n",
        "            # Weight and bias\n",
        "            conv1_weight = tf.Variable(tf.truncated_normal(\n",
        "                shape=(filter1_width, filter1_height, input1_channels, conv1_output),\n",
        "                mean = mu, stddev = sigma))\n",
        "            conv1_bias = tf.Variable(tf.zeros(conv1_output))\n",
        "\n",
        "            # Apply Convolution\n",
        "            conv1 = tf.nn.conv2d(x, conv1_weight, strides=[1, 1, 1, 1], padding='VALID') + conv1_bias\n",
        "\n",
        "            # Activation:\n",
        "            conv1 = tf.nn.relu(conv1)\n",
        "\n",
        "            # Pooling: Input = 28x28x6. Output = 14x14x6.\n",
        "            conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "            # Layer 2 (Convolutional): Output = 10x10x16.\n",
        "            filter2_width = 5\n",
        "            filter2_height = 5\n",
        "            input2_channels = 6\n",
        "            conv2_output = 16\n",
        "            # Weight and bias\n",
        "            conv2_weight = tf.Variable(tf.truncated_normal(\n",
        "                shape=(filter2_width, filter2_height, input2_channels, conv2_output),\n",
        "                mean = mu, stddev = sigma))\n",
        "            conv2_bias = tf.Variable(tf.zeros(conv2_output))\n",
        "\n",
        "            # Apply Convolution\n",
        "            conv2 = tf.nn.conv2d(conv1, conv2_weight, strides=[1, 1, 1, 1], padding='VALID') + conv2_bias\n",
        "\n",
        "            # Activation:\n",
        "            conv2 = tf.nn.relu(conv2)\n",
        "\n",
        "            # Pooling: Input = 10x10x16. Output = 5x5x16.\n",
        "            conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "            # Flattening: Input = 5x5x16. Output = 400.\n",
        "            fully_connected0 = Flatten()(conv2)\n",
        "\n",
        "            # Layer 3 (Fully Connected): Input = 400. Output = 120.\n",
        "            connected1_weights = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
        "            connected1_bias = tf.Variable(tf.zeros(120))\n",
        "            fully_connected1 = tf.add((tf.matmul(fully_connected0, connected1_weights)), connected1_bias)\n",
        "\n",
        "            # Activation:\n",
        "            fully_connected1 = tf.nn.relu(fully_connected1)\n",
        "\n",
        "            # Layer 4 (Fully Connected): Input = 120. Output = 84.\n",
        "            connected2_weights = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
        "            connected2_bias = tf.Variable(tf.zeros(84))\n",
        "            fully_connected2 = tf.add((tf.matmul(fully_connected1, connected2_weights)), connected2_bias)\n",
        "\n",
        "            # Activation.\n",
        "            fully_connected2 = tf.nn.relu(fully_connected2)\n",
        "    \n",
        "            # Layer 5 (Fully Connected): Input = 84. Output = 43.\n",
        "            output_weights = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n",
        "            output_bias = tf.Variable(tf.zeros(43))\n",
        "            logits =  tf.add((tf.matmul(fully_connected2, output_weights)), output_bias)\n",
        "\n",
        "            probs = tf.nn.sigmoid(logits)\n",
        "            \n",
        "            return logits, probs\n",
        "    \n",
        "    \n",
        "    \n",
        "    def test(self, X_data, BATCH_SIZE=64):\n",
        "        num_examples = len(X_data)\n",
        "        y_pred = np.zeros(num_examples, dtype=np.int32)\n",
        "        sess = tf.get_default_session()\n",
        "        for offset in range(0, num_examples, BATCH_SIZE):\n",
        "            batch_x = X_data[offset:offset+BATCH_SIZE]\n",
        "            y_pred[offset:offset+BATCH_SIZE] = sess.run(tf.argmax(logits, 1), \n",
        "                               feed_dict={x:batch_x, keep_prob:1, keep_prob_conv:1})\n",
        "        return y_pred\n",
        "    \n",
        "    \n",
        "    def train(self, X_train, Y_train, X_valid, Y_valid):\n",
        "        #placeholders for inputs\n",
        "        x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
        "        y = tf.placeholder(tf.int32, (None))\n",
        "\n",
        "        keep_prob = tf.placeholder(tf.float32)       # For fully-connected layers\n",
        "        keep_prob_conv = tf.placeholder(tf.float32)\n",
        "\n",
        "        #define graph\n",
        "        logits, _ = self.Model(x)\n",
        "        \n",
        "              # Training operation\n",
        "        one_hot_y = tf.one_hot(y, 43)\n",
        "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=one_hot_y)\n",
        "        loss_operation = tf.reduce_mean(cross_entropy)\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate = self.lr)\n",
        "        training_operation = optimizer.minimize(loss_operation)\n",
        "\n",
        "        # Accuracy operation\n",
        "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
        "        accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "\n",
        "          # Saving all variables\n",
        "        saver = tf.train.Saver()\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            num_train = len(Y_train)\n",
        "            num_valid = len(Y_valid)\n",
        "            print(\"Training ...\")\n",
        "            print()\n",
        "            EPOCHS = self.epochs\n",
        "            BATCH_SIZE = self.batch_size\n",
        "            DIR = \"./weights/target_model\"\n",
        "            total_batch = int(X_train.shape[0] / self.batch_size)\n",
        "\n",
        "            for i in range(EPOCHS):\n",
        "                avg_cost = 0.\n",
        "                total_accuracy = 0\n",
        "                validation_accuracy = 0\n",
        "                #Train set\n",
        "                for offset in range(0, num_train, BATCH_SIZE):\n",
        "                    end = offset + BATCH_SIZE\n",
        "                    batch_x, batch_y = X_train[offset:end], Y_train[offset:end]\n",
        "                    _, c = sess.run([training_operation, loss_operation], feed_dict={x: batch_x, y: batch_y, keep_prob : 0.6, keep_prob_conv: 0.8})\n",
        "                    avg_cost += c / total_batch\n",
        "                    \n",
        "                    #Validation Set\n",
        "                for offset in range(0, num_valid, BATCH_SIZE):\n",
        "                    end = offset + BATCH_SIZE\n",
        "                    batch_x, batch_y = X_valid[offset:end], Y_valid[offset:end]\n",
        "                    accuracy = sess.run(accuracy_operation, \n",
        "                                    feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0, keep_prob_conv: 1.0 })\n",
        "                    total_accuracy += (accuracy * len(batch_x))\n",
        "                    validation_accuracy = total_accuracy / num_valid\n",
        "                    #print(\"Validation Accuracy = {:.3f}%\".format(validation_accuracy*100))\n",
        "                    \n",
        "                print(\"Epoch: \", '%04d' % (i+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
        "                print(\"EPOCH {} : Validation Accuracy = {:.3f}%\".format(i+1, (validation_accuracy*100)))\n",
        "            \n",
        "            \n",
        "            #Test set\n",
        "            num_examples = len(X_test)\n",
        "            y_pred = np.zeros(num_examples, dtype=np.int32)\n",
        "            #sess = tf.get_default_session()\n",
        "            for offset in range(0, num_examples, BATCH_SIZE):\n",
        "                batch_x = X_test[offset:offset+BATCH_SIZE]\n",
        "                y_pred[offset:offset+BATCH_SIZE] = sess.run(tf.argmax(logits, 1), \n",
        "                                   feed_dict={x:batch_x, keep_prob:1, keep_prob_conv:1})\n",
        "            test_accuracy = sum(Y_test == y_pred)/len(Y_test)\n",
        "            print(\"Test Accuracy = {:.1f}%\".format(test_accuracy*100))\n",
        "\n",
        "            cm = confusion_matrix(Y_test, y_pred)\n",
        "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "            cm = np.log(.0001 + cm)\n",
        "            plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "            plt.title('Log of normalized Confusion Matrix')\n",
        "            plt.ylabel('True label')\n",
        "            plt.xlabel('Predicted label')\n",
        "            plt.show()\n",
        "            \n",
        "            saver.save(sess, \"./weights/target_model/model.ckpt\")\n",
        "            print(\"Model saved\")\n",
        "            sess.close()\n",
        "\n",
        "   \n",
        "      \n",
        "      \n",
        "import pickle\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    training_file = 'train.p'\n",
        "    testing_file = 'test.p'\n",
        "    validation_file = 'valid.p'\n",
        "\n",
        "    with open(training_file, mode='rb') as f:\n",
        "        tstrain = pickle.load(f)\n",
        "    with open(testing_file, mode='rb') as f:\n",
        "        tstest = pickle.load(f)\n",
        "    with open(validation_file, mode='rb') as f:\n",
        "        tsvalid = pickle.load(f)\n",
        "\n",
        "    X_train, Y_train = tstrain['features'], tstrain['labels']\n",
        "    X_valid, Y_valid = tsvalid['features'], tsvalid['labels']\n",
        "    X_test, Y_test = tstest['features'], tstest['labels']\n",
        "\n",
        "    #shuffle training set\n",
        "    X_train, Y_train = shuffle(X_train, Y_train)\n",
        "\n",
        "    #grayscale images\n",
        "    grayscale = [0.299,0.587,0.144]\n",
        "\n",
        "    X_test = np.dot(X_test, grayscale)\n",
        "    X_train = np.dot(X_train, grayscale)\n",
        "    X_valid = np.dot(X_valid, grayscale)\n",
        "\n",
        "\n",
        "    #normalize\n",
        "    X_train = np.array(X_train)/255\n",
        "    X_test = np.array(X_test)/255\n",
        "    X_valid = np.array(X_valid)/255\n",
        "\n",
        "    #expand dimensions to fit 4D input array\n",
        "    X_train = np.expand_dims(X_train,-1)\n",
        "    X_test = np.expand_dims(X_test,-1)\n",
        "    X_valid = np.expand_dims(X_valid,-1)\n",
        "\n",
        "    assert(len(X_train)==len(Y_train))\n",
        "    n_train = len(X_train)\n",
        "    assert(len(X_test)==len(Y_test))\n",
        "    n_test = len(X_test)\n",
        "\n",
        "    cnn = Target()\n",
        "    cnn.train(X_train, Y_train, X_valid, Y_valid)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training ...\n",
            "\n",
            "Epoch:  0001 cost= 0.942889310\n",
            "EPOCH 1 : Validation Accuracy = 89.705%\n",
            "Epoch:  0002 cost= 0.201903688\n",
            "EPOCH 2 : Validation Accuracy = 90.522%\n",
            "Epoch:  0003 cost= 0.112019129\n",
            "EPOCH 3 : Validation Accuracy = 90.884%\n",
            "Epoch:  0004 cost= 0.078378597\n",
            "EPOCH 4 : Validation Accuracy = 90.703%\n",
            "Epoch:  0005 cost= 0.058381033\n",
            "EPOCH 5 : Validation Accuracy = 90.816%\n",
            "Epoch:  0006 cost= 0.040495406\n",
            "EPOCH 6 : Validation Accuracy = 92.268%\n",
            "Epoch:  0007 cost= 0.039245306\n",
            "EPOCH 7 : Validation Accuracy = 91.837%\n",
            "Epoch:  0008 cost= 0.028446352\n",
            "EPOCH 8 : Validation Accuracy = 92.744%\n",
            "Epoch:  0009 cost= 0.028969797\n",
            "EPOCH 9 : Validation Accuracy = 92.449%\n",
            "Epoch:  0010 cost= 0.020000446\n",
            "EPOCH 10 : Validation Accuracy = 89.841%\n",
            "Epoch:  0011 cost= 0.020542971\n",
            "EPOCH 11 : Validation Accuracy = 91.814%\n",
            "Epoch:  0012 cost= 0.023177462\n",
            "EPOCH 12 : Validation Accuracy = 91.814%\n",
            "Epoch:  0013 cost= 0.017620927\n",
            "EPOCH 13 : Validation Accuracy = 92.109%\n",
            "Epoch:  0014 cost= 0.020050219\n",
            "EPOCH 14 : Validation Accuracy = 92.245%\n",
            "Epoch:  0015 cost= 0.018923408\n",
            "EPOCH 15 : Validation Accuracy = 92.925%\n",
            "Test Accuracy = 92.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ0AAAEWCAYAAAB8A8JQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nScM5l8v/AzmP09/7rCcFAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2laXHD7-tmk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generator\n",
        "'''\n",
        "\tGenerator definition for AdvGAN\n",
        "\tref: https://arxiv.org/pdf/1801.02610.pdf\n",
        "'''\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# helper function for convolution -> instance norm -> relu\n",
        "def ConvInstNormRelu(x, filters, kernel_size=3, strides=1):\n",
        "\tConv = tf.layers.conv2d(\n",
        "\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tInstNorm = tf.contrib.layers.instance_norm(Conv)\n",
        "\n",
        "\treturn tf.nn.relu(InstNorm)\n",
        "\n",
        "\n",
        "# helper function for trans convolution -> instance norm -> relu\n",
        "def TransConvInstNormRelu(x, filters, kernel_size=3, strides=2):\n",
        "\tTransConv = tf.layers.conv2d_transpose(\n",
        "\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tInstNorm = tf.contrib.layers.instance_norm(TransConv)\n",
        "\n",
        "\treturn tf.nn.relu(InstNorm)\n",
        "\n",
        "# helper function for residual block of 2 convolutions with same num filters\n",
        "# in the same style as ConvInstNormRelu\n",
        "def ResBlock(x, training, filters=32, kernel_size=3, strides=1):\n",
        "\tconv1 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tconv1_norm = tf.layers.batch_normalization(conv1, training=training)\n",
        "\n",
        "\tconv1_relu = tf.nn.relu(conv1_norm)\n",
        "\n",
        "\tconv2 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\tinputs=conv1_relu,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tconv2_norm = tf.layers.batch_normalization(conv2, training=training)\n",
        "\n",
        "\n",
        "\treturn x + conv2_norm\n",
        "\n",
        "\n",
        "def generator(x, training):\n",
        "\twith tf.variable_scope('g_weights', reuse=tf.AUTO_REUSE):\n",
        "\t\t# input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "\t\t# define first three conv + inst + relu layers\n",
        "\t\tc1 = ConvInstNormRelu(x, filters=8, kernel_size=3, strides=1)\n",
        "\t\td1 = ConvInstNormRelu(c1, filters=16, kernel_size=3, strides=2)\n",
        "\t\td2 = ConvInstNormRelu(d1, filters=32, kernel_size=3, strides=2)\n",
        "\n",
        "\t\t# define residual blocks\n",
        "\t\trb1 = ResBlock(d2, training, filters=32)\n",
        "\t\trb2 = ResBlock(rb1, training, filters=32)\n",
        "\t\trb3 = ResBlock(rb2, training, filters=32)\n",
        "\t\trb4 = ResBlock(rb3, training, filters=32)\n",
        "\n",
        "\t\t# upsample using conv transpose\n",
        "\t\tu1 = TransConvInstNormRelu(rb4, filters=16, kernel_size=3, strides=2)\n",
        "\t\tu2 = TransConvInstNormRelu(u1, filters=8, kernel_size=3, strides=2)\n",
        "\n",
        "\t\t# final layer block\n",
        "\t\tout = tf.layers.conv2d_transpose(\n",
        "\t\t\t\t\t\tinputs=u2,\n",
        "\t\t\t\t\t\tfilters=x.get_shape()[-1].value, # or 3 if RGB image\n",
        "\t\t\t\t\t\tkernel_size=3,\n",
        "\t\t\t\t\t\tstrides=1,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\t\t# out = tf.contrib.layers.instance_norm(out)\n",
        "\n",
        "\t\treturn tf.nn.tanh(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKYLa86WtyLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Discriminator\n",
        "'''\n",
        "\tDiscriminator definition for AdvGAN\n",
        "\tref: https://arxiv.org/pdf/1801.02610.pdf\n",
        "'''\n",
        "\n",
        "#import tensorflow as tf\n",
        "\n",
        "def discriminator(x, training):\n",
        "\twith tf.variable_scope('d_weights', reuse=tf.AUTO_REUSE):\n",
        "\t\t# input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "\t\tconv1 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\t\tfilters=8,\n",
        "\t\t\t\t\t\t\tkernel_size=4,\n",
        "\t\t\t\t\t\t\tstrides=2,\n",
        "\t\t\t\t\t\t\tpadding=\"valid\",\n",
        "\t\t\t\t\t\t\tactivation=None)\n",
        "\t\tconv1 = tf.nn.leaky_relu(conv1, alpha=0.2)\n",
        "\n",
        "\t\t\n",
        "\t\tconv2 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\t\tinputs=conv1,\n",
        "\t\t\t\t\t\t\tfilters=16,\n",
        "\t\t\t\t\t\t\tkernel_size=4,\n",
        "\t\t\t\t\t\t\tstrides=2,\n",
        "\t\t\t\t\t\t\tpadding=\"valid\",\n",
        "\t\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\t\tin1 = tf.contrib.layers.instance_norm(conv2)\n",
        "\t\tconv2 = tf.nn.leaky_relu(in1, alpha=0.2)\n",
        "\n",
        "\t\tconv3 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\t\tinputs=conv2,\n",
        "\t\t\t\t\t\t\tfilters=32,\n",
        "\t\t\t\t\t\t\tkernel_size=4,\n",
        "\t\t\t\t\t\t\tstrides=2,\n",
        "\t\t\t\t\t\t\tpadding=\"valid\",\n",
        "\t\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\t\t#in2 = tf.contrib.layers.instance_norm(conv3)\n",
        "\t\tin2 = tf.contrib.layers.instance_norm(conv3)\n",
        "\t\tconv3 = tf.nn.leaky_relu(in2, alpha=0.2)\n",
        "\t\tflat = tf.layers.flatten(conv3)\n",
        "\t\tlogits = tf.layers.dense(flat, 1)\n",
        "\n",
        "\t\tprobs = tf.nn.sigmoid(logits)\n",
        "\n",
        "\t\treturn logits, probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "288LCGWawCVl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "outputId": "dfb9c17a-25fe-4919-d6ab-be245bc85aa9"
      },
      "source": [
        "import tensorflow as tf\n",
        "#from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os, sys\n",
        "import random\n",
        "\n",
        "#make sure GAN_setup.py is in connected folder **as of 5/21, it's an ipynb file not py\n",
        "#from GAN_setup import generator, discriminator\n",
        "\n",
        "#ctargon created class Target and defined/trained his target model in there, then called here\n",
        "#import Target as target_model\n",
        "\n",
        "\n",
        "# get the next batch based on x, y, and the iteration (based on batch_size)\n",
        "def next_batch(X, Y, i, batch_size):\n",
        "    idx = i * batch_size\n",
        "    idx_n = i * batch_size + batch_size\n",
        "    return X[idx:idx_n], Y[idx:idx_n]\n",
        "\n",
        "# loss function to encourage misclassification after perturbation\n",
        "def adv_loss(preds, labels, is_targeted):\n",
        "    real = tf.reduce_sum(labels * preds, 1)\n",
        "    other = tf.reduce_max((1 - labels) * preds - (labels * 10000), 1)\n",
        "    if is_targeted:\n",
        "        return tf.reduce_sum(tf.maximum(0.0, other - real))\n",
        "    return tf.reduce_sum(tf.maximum(0.0, real - other))\n",
        "\n",
        "# loss function to influence the perturbation to be as close to 0 as possible\n",
        "def perturb_loss(preds, thresh=0.3):\n",
        "    zeros = tf.zeros((tf.shape(preds)[0]))\n",
        "    return tf.reduce_mean(tf.maximum(zeros, tf.norm(tf.reshape(preds, (tf.shape(preds)[0], -1)), axis=1) - thresh))\n",
        "\n",
        "\n",
        "# function that defines ops, graphs, and training procedure for AdvGAN framework\n",
        "def AdvGAN(X, y, X_test, y_test, epochs=50, batch_size=128, target=-1):\n",
        "    #print(X_train.shape)\n",
        "    #print(y.shape[-1]) is num_images\n",
        "    \n",
        "    # placeholder definitions\n",
        "    x_pl = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]]) # image placeholder\n",
        "    t = tf.placeholder(tf.float32, [None, y.shape[-1]]) # target placeholder\n",
        "    is_training = tf.placeholder(tf.bool, [])\n",
        "\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    # MODEL DEFINITIONS\n",
        "    is_targeted = False\n",
        "    if target in range(0, y.shape[-1]):\n",
        "        is_targeted = True\n",
        "\n",
        "    # gather target model\n",
        "    f = Target()\n",
        "\n",
        "    thresh = 0.3\n",
        "\n",
        "    # generate perturbation, add to original input image(s)\n",
        "    perturb = tf.clip_by_value(generator(x_pl, is_training), -thresh, thresh)\n",
        "    x_perturbed = perturb + x_pl\n",
        "    x_perturbed = tf.clip_by_value(x_perturbed, 0, 1)\n",
        "    print(x_perturbed.shape)\n",
        "\n",
        "    # pass real and perturbed image to discriminator and the target model\n",
        "    d_real_logits, d_real_probs = discriminator(x_pl, is_training)\n",
        "    d_fake_logits, d_fake_probs = discriminator(x_perturbed, is_training)\n",
        "    print(d_fake_probs.shape)#1\n",
        "    # pass real and perturbed images to the model we are trying to fool\n",
        "    f_real_logits, f_real_probs = f.Model(x_pl)\n",
        "    f_fake_logits, f_fake_probs = f.Model(x_perturbed)\n",
        "    print(f_fake_probs.shape) #43\n",
        "\n",
        "    # generate labels for discriminator (optionally smooth labels for stability)\n",
        "    smooth = 0.0\n",
        "    d_labels_real = tf.ones_like(d_real_probs) * (1 - smooth)\n",
        "    d_labels_fake = tf.zeros_like(d_fake_probs)\n",
        "\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    # LOSS DEFINITIONS\n",
        "    # discriminator loss\n",
        "    d_loss_real = tf.losses.mean_squared_error(predictions=d_real_probs, labels=d_labels_real)\n",
        "    d_loss_fake = tf.losses.mean_squared_error(predictions=d_fake_probs, labels=d_labels_fake)\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "    # generator loss\n",
        "    g_loss_fake = tf.losses.mean_squared_error(predictions=d_fake_probs, labels=tf.ones_like(d_fake_probs))\n",
        "\n",
        "    # perturbation loss (minimize overall perturbation)\n",
        "    l_perturb = perturb_loss(perturb, thresh)\n",
        "\n",
        "    # adversarial loss (encourage misclassification)\n",
        "    l_adv = adv_loss(f_fake_probs, t, is_targeted)\n",
        "\n",
        "    # weights for generator loss function\n",
        "    alpha = 1.0\n",
        "    beta = 5.0\n",
        "    g_loss = l_adv + alpha*g_loss_fake + beta*l_perturb \n",
        "\n",
        "    # ----------------------------------------------------------------------------------\n",
        "    # gather variables for training/restoring\n",
        "    t_vars = tf.trainable_variables()\n",
        "    f_vars = [var for var in t_vars if 'Model' in var.name]\n",
        "    d_vars = [var for var in t_vars if 'd_' in var.name]\n",
        "    g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='g_weights')\n",
        "\n",
        "    # define optimizers for discriminator and generator\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        d_opt = tf.train.AdamOptimizer().minimize(d_loss, var_list=d_vars)\n",
        "        g_opt = tf.train.AdamOptimizer(learning_rate=0.001).minimize(g_loss, var_list=g_vars)\n",
        "\n",
        "\t# create saver objects for the target model, generator, and discriminator\n",
        "    saver = tf.train.Saver(f_vars)\n",
        "    g_saver = tf.train.Saver(g_vars)\n",
        "    d_saver = tf.train.Saver(d_vars)\n",
        "\n",
        "    init  = tf.global_variables_initializer()\n",
        "\n",
        "    sess  = tf.Session()\n",
        "    sess.run(init)\n",
        "\n",
        "    # load the pretrained target model\n",
        "    try:\n",
        "        saver.restore(sess, \"./weights/target_model/model.ckpt\")\n",
        "    except:\n",
        "        print(\"make sure to train the target model first...\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    total_batches = int(X.shape[0] / batch_size)\n",
        "\n",
        "    for epoch in range(0, epochs):\n",
        "\n",
        "        loss_D_sum = 0.0\n",
        "        loss_G_fake_sum = 0.0\n",
        "        loss_perturb_sum = 0.0\n",
        "        loss_adv_sum = 0.0\n",
        "\n",
        "        for i in range(total_batches):\n",
        "\n",
        "            batch_x, batch_y = next_batch(X, y, i, batch_size)\n",
        "\n",
        "            # if targeted, create one hot vectors of the target\n",
        "            if is_targeted:\n",
        "                targets = np.full((batch_y.shape[0],), target)\n",
        "                batch_y = np.eye(y.shape[-1])[targets]\n",
        "\n",
        "            # train the discriminator first n times\n",
        "            for _ in range(1):\n",
        "                _, loss_D_batch = sess.run([d_opt, d_loss], feed_dict={x_pl: batch_x, \\\n",
        "                                           is_training: True})\n",
        "\n",
        "\t\t\t       # train the generator n times\n",
        "            for _ in range(1):\n",
        "                _, loss_G_fake_batch, loss_adv_batch, loss_perturb_batch = \\\n",
        "                        sess.run([g_opt, g_loss_fake, l_adv, l_perturb], \\\n",
        "                              feed_dict={x_pl: batch_x, \\\n",
        "                                     t: batch_y, \\\n",
        "                                     is_training: True})\n",
        "            loss_D_sum += loss_D_batch\n",
        "            loss_G_fake_sum += loss_G_fake_batch\n",
        "            loss_perturb_sum += loss_perturb_batch\n",
        "            loss_adv_sum += loss_adv_batch\n",
        "\n",
        "        print(\"epoch %d:\\nloss_D: %.3f, loss_G_fake: %.3f, \\\n",
        "            \\nloss_perturb: %.3f, loss_adv: %.3f, \\n\" %\n",
        "            (epoch + 1, loss_D_sum/total_batches, loss_G_fake_sum/total_batches,\n",
        "            loss_perturb_sum/total_batches, loss_adv_sum/total_batches))\n",
        "    #epoch_losses = np.array([loss_D_sum/totalbatches], [loss_G_fake_sum/total_batches], [loss_perturb_sum/totalbatches], [loss_adv_sum/total_batches])\n",
        "\t\t#np.savetxt(\"epoch_{}_losses.txt\".format(epoch), epoch_losses, delimiter=',')\n",
        "    \n",
        "        if epoch % 10 == 0:\n",
        "            g_saver.save(sess, \"weights/generator/gen.ckpt\")\n",
        "            d_saver.save(sess, \"weights/discriminator/disc.ckpt\")\n",
        "\n",
        "    # evaluate the test set\n",
        "    correct_prediction = tf.equal(tf.argmax(f_fake_probs, 1), tf.argmax(t, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    accs = []\n",
        "    total_batches_test = int(X_test.shape[0] / batch_size)\n",
        "    for i in range(total_batches_test):\n",
        "        batch_x, batch_y = next_batch(X_test, y_test, i, batch_size)\n",
        "        acc, x_pert = sess.run([accuracy, x_perturbed], feed_dict={x_pl: batch_x, t: batch_y, is_training: False})\n",
        "        accs.append(acc)\n",
        "\n",
        "    print('accuracy of test set: {}'.format(sum(accs) / len(accs)))\n",
        "  #test_acc = np.array(sum(accs)/len(accs))\n",
        "  #np.savetxt(\"test_accuracy_GD.txt\", test_acc, delimiter=',')\n",
        "\n",
        "\t# plot some images and their perturbed counterparts\n",
        "    f, axarr = plt.subplots(2,2)\n",
        "    axarr[0,0].imshow(np.squeeze(batch_x[2]), cmap='Greys_r')\n",
        "    axarr[0,1].imshow(np.squeeze(x_pert[2]), cmap='Greys_r')\n",
        "    axarr[1,0].imshow(np.squeeze(batch_x[5]), cmap='Greys_r')\n",
        "    axarr[1,1].imshow(np.squeeze(x_pert[5]), cmap='Greys_r')\n",
        "    plt.show()\n",
        "\n",
        "    print('finished training, saving weights')\n",
        "    g_saver.save(sess, \"weights/generator/gen.ckpt\")\n",
        "    d_saver.save(sess, \"weights/discriminator/disc.ckpt\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def attack(X, y, batch_size=128, thresh=0.3, target=-1):\n",
        "    x_pl = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]]) # image placeholder\n",
        "    t = tf.placeholder(tf.float32, [None, 43]) # target placeholder\n",
        "    is_training = tf.placeholder(tf.bool, [])\n",
        "\n",
        "    is_targeted = False\n",
        "    if target in range(0, y.shape[-1]):\n",
        "        is_targeted = True\n",
        "\n",
        "    perturb = tf.clip_by_value(generator(x_pl, is_training), -thresh, thresh)\n",
        "    x_perturbed = perturb + x_pl\n",
        "    x_perturbed = tf.clip_by_value(x_perturbed, 0, 1)\n",
        "\n",
        "    f = Target()\n",
        "    f_real_logits, f_real_probs = f.Model(x_pl)\n",
        "    f_fake_logits, f_fake_probs = f.Model(x_perturbed)\n",
        "\n",
        "    t_vars = tf.trainable_variables()\n",
        "    f_vars = [var for var in t_vars if 'Model' in var.name]\n",
        "    g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='g_weights')\n",
        "\n",
        "    sess = tf.Session()\n",
        "\n",
        "    f_saver = tf.train.Saver(f_vars)\n",
        "    g_saver = tf.train.Saver(g_vars)\n",
        "    f_saver.restore(sess, \"./weights/target_model/model.ckpt\")\n",
        "    g_saver.restore(sess, tf.train.latest_checkpoint(\"./weights/generator/\"))\n",
        "\n",
        "    rawpert, pert, fake_l, real_l = sess.run([perturb, x_perturbed, f_fake_probs, f_real_probs], \\\n",
        "                          feed_dict={x_pl: X[:32], \\\n",
        "                                 is_training: False})\n",
        "    print('LA: ' + str(np.argmax(y[:32], axis=1)))\n",
        "    print('OG: ' + str(np.argmax(real_l, axis=1)))\n",
        "    print('PB: ' + str(np.argmax(fake_l, axis=1)))\n",
        "\n",
        "    correct_prediction = tf.equal(tf.argmax(f_fake_probs, 1), tf.argmax(t, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    accs = []\n",
        "    total_batches_test = int(X.shape[0] / batch_size)\n",
        "    for i in range(total_batches_test):\n",
        "        batch_x, batch_y = next_batch(X, y, i, batch_size)\n",
        "\n",
        "        if is_targeted:\n",
        "            targets = np.full((batch_y.shape[0],), target)\n",
        "            batch_y = np.eye(y.shape[-1])[targets]\n",
        "\n",
        "        acc, fake_l, x_pert = sess.run([accuracy, f_fake_probs, x_perturbed], feed_dict={x_pl: batch_x, t: batch_y, is_training: False})\n",
        "        accs.append(acc)\n",
        "\n",
        "    print('accuracy of test set: {}'.format(sum(accs) / len(accs)))\n",
        "\n",
        "    f, axarr = plt.subplots(2,2)\n",
        "    axarr[0,0].imshow(np.squeeze(X[3]), cmap='Greys_r')\n",
        "    axarr[0,1].imshow(np.squeeze(pert[3]), cmap='Greys_r')\n",
        "    axarr[1,0].imshow(np.squeeze(X[4]), cmap='Greys_r')\n",
        "    axarr[1,1].imshow(np.squeeze(pert[4]), cmap='Greys_r')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "'''\n",
        "import pickle\n",
        "\n",
        "training_file = 'train.p'\n",
        "testing_file = 'test.p'\n",
        "validation_file = 'valid.p'\n",
        "\n",
        "with open(training_file, mode='rb') as f:\n",
        "    tstrain = pickle.load(f)\n",
        "with open(testing_file, mode='rb') as f:\n",
        "    tstest = pickle.load(f)\n",
        "with open(validation_file, mode='rb') as f:\n",
        "    tsvalid = pickle.load(f)\n",
        "\n",
        "X_train, Y_train = tstrain['features'], tstrain['labels']\n",
        "X_valid, Y_valid = tsvalid['features'], tsvalid['labels']\n",
        "X_test, Y_test = tstest['features'], tstest['labels']\n",
        "\n",
        "#shuffle training set\n",
        "X_train, Y_train = shuffle(X_train, Y_train)\n",
        "\n",
        "#grayscale images\n",
        "grayscale = [0.299,0.587,0.144]\n",
        "\n",
        "X_test = np.dot(X_test, grayscale)\n",
        "X_train = np.dot(X_train, grayscale)\n",
        "X_valid = np.dot(X_valid, grayscale)\n",
        "\n",
        "\n",
        "#normalize\n",
        "X_train = np.array(X_train)/255\n",
        "X_test = np.array(X_test)/255\n",
        "X_valid = np.array(X_valid)/255\n",
        "\n",
        "#expand dimensions to fit 4D input array\n",
        "X_train = np.expand_dims(X_train,-1)\n",
        "X_test = np.expand_dims(X_test,-1)\n",
        "X_valid = np.expand_dims(X_valid,-1)\n",
        "\n",
        "assert(len(X_train)==len(Y_train))\n",
        "n_train = len(X_train)\n",
        "assert(len(X_test)==len(Y_test))\n",
        "n_test = len(X_test)\n",
        "'''\n",
        "AdvGAN(X_train, Y_train, X_test, Y_test, batch_size=128, epochs=50, target=-1)\n",
        "attack(X_test, Y_test, target=-1)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "34799\n",
            "(?, 32, 32, 1)\n",
            "(?, 1)\n",
            "(?, 43)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1658\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 34799 and 43 for 'mul_7' (op: 'Mul') with input shapes: [?,34799], [?,43].",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-0be9353e293b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0mn_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m '''\n\u001b[0;32m--> 301\u001b[0;31m \u001b[0mAdvGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-0be9353e293b>\u001b[0m in \u001b[0;36mAdvGAN\u001b[0;34m(X, y, X_test, y_test, epochs, batch_size, target)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# adversarial loss (encourage misclassification)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0ml_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madv_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_fake_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_targeted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# weights for generator loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-0be9353e293b>\u001b[0m in \u001b[0;36madv_loss\u001b[0;34m(preds, labels, is_targeted)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# loss function to encourage misclassification after perturbation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madv_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_targeted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_targeted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1076\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Case: Dense * Sparse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   5858\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5859\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 5860\u001b[0;31m         \"Mul\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m   5861\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5862\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    789\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3298\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3299\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3300\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3301\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1821\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1822\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1823\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1662\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 34799 and 43 for 'mul_7' (op: 'Mul') with input shapes: [?,34799], [?,43]."
          ]
        }
      ]
    }
  ]
}
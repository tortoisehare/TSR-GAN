{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fullGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tortoisehare/TSR-GAN/blob/master/fullGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABnslutLuWVE",
        "colab_type": "code",
        "outputId": "2eacd202-1554-4a3a-80d5-83b697255e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!mkdir weights\n",
        "!mkdir weights/target_model\n",
        "!mkdir weights/generator\n",
        "!mkdir weights/discriminator\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘weights’: File exists\n",
            "mkdir: cannot create directory ‘weights/target_model’: File exists\n",
            "mkdir: cannot create directory ‘weights/generator’: File exists\n",
            "mkdir: cannot create directory ‘weights/discriminator’: File exists\n",
            "sample_data  test.p  train.p  valid.p  weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-dNiamCtfBr",
        "colab_type": "code",
        "outputId": "e8a51e46-820b-4f3c-8c40-65a294a65c79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1318
        }
      },
      "source": [
        "#TARGET CLASSIFIER\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(1187) #to help reproduce\n",
        "\n",
        "#from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import csv\n",
        "#import random\n",
        "#import cv2\n",
        "#from skimage.filters import rank\n",
        "#import skimage.morphology as morp\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "#from keras.utils import to_categorical\n",
        "#from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "#from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "\n",
        "class Target:\n",
        "    def __init__(self, lr=0.001, epochs=25, n_input=32, n_classes=43, batch_size=20, restore=0):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.n_input = 32\n",
        "        self.n_classes = 43\n",
        "        self.batch_size = batch_size\n",
        "        self.restore = restore\n",
        "\n",
        "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "    \n",
        "    '''\n",
        "    def next_batch(self, X, Y, i, batch_size):\n",
        "      idx = i*batch_size\n",
        "      idx_n = idx + batch_size\n",
        "      return X[idx:idx_n], Y[idx:idx_n]\n",
        "      '''\n",
        "    \n",
        "    def Model(self, x):\n",
        "        with tf.variable_scope('Model', reuse=tf.AUTO_REUSE):\n",
        "            # Hyperparameters\n",
        "            mu = 0\n",
        "            sigma = 0.1\n",
        "            n_out = self.n_classes\n",
        "            learning_rate = self.lr\n",
        "\n",
        "            # Layer 1 (Convolutional): Input = 32x32x1. Output = 28x28x6.\n",
        "            filter1_width = 5\n",
        "            filter1_height = 5\n",
        "            input1_channels = 1\n",
        "            conv1_output = 6\n",
        "            # Weight and bias\n",
        "            conv1_weight = tf.Variable(tf.truncated_normal(\n",
        "                shape=(filter1_width, filter1_height, input1_channels, conv1_output),\n",
        "                mean = mu, stddev = sigma))\n",
        "            conv1_bias = tf.Variable(tf.zeros(conv1_output))\n",
        "\n",
        "            # Apply Convolution\n",
        "            conv1 = tf.nn.conv2d(x, conv1_weight, strides=[1, 1, 1, 1], padding='VALID') + conv1_bias\n",
        "\n",
        "            # Activation:\n",
        "            conv1 = tf.nn.relu(conv1)\n",
        "\n",
        "            # Pooling: Input = 28x28x6. Output = 14x14x6.\n",
        "            conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "            # Layer 2 (Convolutional): Output = 10x10x16.\n",
        "            filter2_width = 5\n",
        "            filter2_height = 5\n",
        "            input2_channels = 6\n",
        "            conv2_output = 16\n",
        "            # Weight and bias\n",
        "            conv2_weight = tf.Variable(tf.truncated_normal(\n",
        "                shape=(filter2_width, filter2_height, input2_channels, conv2_output),\n",
        "                mean = mu, stddev = sigma))\n",
        "            conv2_bias = tf.Variable(tf.zeros(conv2_output))\n",
        "\n",
        "            # Apply Convolution\n",
        "            conv2 = tf.nn.conv2d(conv1, conv2_weight, strides=[1, 1, 1, 1], padding='VALID') + conv2_bias\n",
        "\n",
        "            # Activation:\n",
        "            conv2 = tf.nn.relu(conv2)\n",
        "\n",
        "            # Pooling: Input = 10x10x16. Output = 5x5x16.\n",
        "            conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "            # Flattening: Input = 5x5x16. Output = 400.\n",
        "            fully_connected0 = Flatten()(conv2)\n",
        "\n",
        "            # Layer 3 (Fully Connected): Input = 400. Output = 120.\n",
        "            connected1_weights = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
        "            connected1_bias = tf.Variable(tf.zeros(120))\n",
        "            fully_connected1 = tf.add((tf.matmul(fully_connected0, connected1_weights)), connected1_bias)\n",
        "\n",
        "            # Activation:\n",
        "            fully_connected1 = tf.nn.relu(fully_connected1)\n",
        "\n",
        "            # Layer 4 (Fully Connected): Input = 120. Output = 84.\n",
        "            connected2_weights = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
        "            connected2_bias = tf.Variable(tf.zeros(84))\n",
        "            fully_connected2 = tf.add((tf.matmul(fully_connected1, connected2_weights)), connected2_bias)\n",
        "\n",
        "            # Activation.\n",
        "            fully_connected2 = tf.nn.relu(fully_connected2)\n",
        "    \n",
        "            # Layer 5 (Fully Connected): Input = 84. Output = 43.\n",
        "            output_weights = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n",
        "            output_bias = tf.Variable(tf.zeros(43))\n",
        "            logits =  tf.add((tf.matmul(fully_connected2, output_weights)), output_bias)\n",
        "\n",
        "            probs = tf.nn.sigmoid(logits)\n",
        "            \n",
        "            return logits, probs\n",
        "    \n",
        "    \n",
        "    \n",
        "    def test(self, X_data, BATCH_SIZE=64):\n",
        "        num_examples = len(X_data)\n",
        "        y_pred = np.zeros(num_examples, dtype=np.int32)\n",
        "        sess = tf.get_default_session()\n",
        "        for offset in range(0, num_examples, BATCH_SIZE):\n",
        "            batch_x = X_data[offset:offset+BATCH_SIZE]\n",
        "            y_pred[offset:offset+BATCH_SIZE] = sess.run(tf.argmax(logits, 1), \n",
        "                               feed_dict={x:batch_x, keep_prob:1, keep_prob_conv:1})\n",
        "        return y_pred\n",
        "    \n",
        "    \n",
        "    def train(self, X_train, Y_train, X_valid, Y_valid):\n",
        "        #placeholders for inputs\n",
        "        x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
        "        y = tf.placeholder(tf.int32, (None))\n",
        "\n",
        "        keep_prob = tf.placeholder(tf.float32)       # For fully-connected layers\n",
        "        keep_prob_conv = tf.placeholder(tf.float32)\n",
        "\n",
        "        #define graph\n",
        "        logits, _ = self.Model(x)\n",
        "        \n",
        "              # Training operation\n",
        "        one_hot_y = tf.one_hot(y, 43)\n",
        "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=one_hot_y)\n",
        "        loss_operation = tf.reduce_mean(cross_entropy)\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate = self.lr)\n",
        "        training_operation = optimizer.minimize(loss_operation)\n",
        "\n",
        "        # Accuracy operation\n",
        "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
        "        accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "\n",
        "          # Saving all variables\n",
        "        saver = tf.train.Saver()\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            num_train = len(Y_train)\n",
        "            num_valid = len(Y_valid)\n",
        "            print(\"Training ...\")\n",
        "            print()\n",
        "            EPOCHS = self.epochs\n",
        "            BATCH_SIZE = self.batch_size\n",
        "            DIR = \"./weights/target_model\"\n",
        "            total_batch = int(X_train.shape[0] / self.batch_size)\n",
        "\n",
        "            for i in range(EPOCHS):\n",
        "                avg_cost = 0.\n",
        "                total_accuracy = 0\n",
        "                validation_accuracy = 0\n",
        "                #Train set\n",
        "                for offset in range(0, num_train, BATCH_SIZE):\n",
        "                    end = offset + BATCH_SIZE\n",
        "                    batch_x, batch_y = X_train[offset:end], Y_train[offset:end]\n",
        "                    _, c = sess.run([training_operation, loss_operation], feed_dict={x: batch_x, y: batch_y, keep_prob : 0.6, keep_prob_conv: 0.8})\n",
        "                    avg_cost += c / total_batch\n",
        "                    \n",
        "                    #Validation Set\n",
        "                for offset in range(0, num_valid, BATCH_SIZE):\n",
        "                    end = offset + BATCH_SIZE\n",
        "                    batch_x, batch_y = X_valid[offset:end], Y_valid[offset:end]\n",
        "                    accuracy = sess.run(accuracy_operation, \n",
        "                                    feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0, keep_prob_conv: 1.0 })\n",
        "                    total_accuracy += (accuracy * len(batch_x))\n",
        "                    validation_accuracy = total_accuracy / num_valid\n",
        "                    #print(\"Validation Accuracy = {:.3f}%\".format(validation_accuracy*100))\n",
        "                    \n",
        "                print(\"Epoch: \", '%04d' % (i+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
        "                print(\"EPOCH {} : Validation Accuracy = {:.3f}%\".format(i+1, (validation_accuracy*100)))\n",
        "            \n",
        "            \n",
        "            #Test set\n",
        "            num_examples = len(X_test)\n",
        "            y_pred = np.zeros(num_examples, dtype=np.int32)\n",
        "            #sess = tf.get_default_session()\n",
        "            for offset in range(0, num_examples, BATCH_SIZE):\n",
        "                batch_x = X_test[offset:offset+BATCH_SIZE]\n",
        "                y_pred[offset:offset+BATCH_SIZE] = sess.run(tf.argmax(logits, 1), \n",
        "                                   feed_dict={x:batch_x, keep_prob:1, keep_prob_conv:1})\n",
        "            test_accuracy = sum(Y_test == y_pred)/len(Y_test)\n",
        "            print(\"Test Accuracy = {:.1f}%\".format(test_accuracy*100))\n",
        "\n",
        "            cm = confusion_matrix(Y_test, y_pred)\n",
        "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "            cm = np.log(.0001 + cm)\n",
        "            plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "            plt.title('Log of normalized Confusion Matrix')\n",
        "            plt.ylabel('True label')\n",
        "            plt.xlabel('Predicted label')\n",
        "            plt.show()\n",
        "            \n",
        "            saver.save(sess, \"./weights/target_model/model\")\n",
        "            print(\"Model saved\")\n",
        "            sess.close()\n",
        "\n",
        "   \n",
        "      \n",
        "      \n",
        "import pickle\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    training_file = 'train.p'\n",
        "    testing_file = 'test.p'\n",
        "    validation_file = 'valid.p'\n",
        "\n",
        "    with open(training_file, mode='rb') as f:\n",
        "        tstrain = pickle.load(f)\n",
        "    with open(testing_file, mode='rb') as f:\n",
        "        tstest = pickle.load(f)\n",
        "    with open(validation_file, mode='rb') as f:\n",
        "        tsvalid = pickle.load(f)\n",
        "\n",
        "    X_train, Y_train = tstrain['features'], tstrain['labels']\n",
        "    X_valid, Y_valid = tsvalid['features'], tsvalid['labels']\n",
        "    X_test, Y_test = tstest['features'], tstest['labels']\n",
        "\n",
        "    #shuffle training set\n",
        "    X_train, Y_train = shuffle(X_train, Y_train)\n",
        "\n",
        "    #grayscale images\n",
        "    grayscale = [0.299,0.587,0.144]\n",
        "\n",
        "    X_test = np.dot(X_test, grayscale)\n",
        "    X_train = np.dot(X_train, grayscale)\n",
        "    X_valid = np.dot(X_valid, grayscale)\n",
        "\n",
        "\n",
        "    #normalize\n",
        "    X_train = np.array(X_train)/255\n",
        "    X_test = np.array(X_test)/255\n",
        "    X_valid = np.array(X_valid)/255\n",
        "\n",
        "    #expand dimensions to fit 4D input array\n",
        "    X_train = np.expand_dims(X_train,-1)\n",
        "    X_test = np.expand_dims(X_test,-1)\n",
        "    X_valid = np.expand_dims(X_valid,-1)\n",
        "\n",
        "    assert(len(X_train)==len(Y_train))\n",
        "    n_train = len(X_train)\n",
        "    assert(len(X_test)==len(Y_test))\n",
        "    n_test = len(X_test)\n",
        "\n",
        "    cnn = Target()\n",
        "    cnn.train(X_train, Y_train, X_valid, Y_valid)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Training ...\n",
            "\n",
            "Epoch:  0001 cost= 1.143467917\n",
            "EPOCH 1 : Validation Accuracy = 85.306%\n",
            "Epoch:  0002 cost= 0.259472334\n",
            "EPOCH 2 : Validation Accuracy = 89.433%\n",
            "Epoch:  0003 cost= 0.146756447\n",
            "EPOCH 3 : Validation Accuracy = 91.383%\n",
            "Epoch:  0004 cost= 0.096168410\n",
            "EPOCH 4 : Validation Accuracy = 90.000%\n",
            "Epoch:  0005 cost= 0.072873450\n",
            "EPOCH 5 : Validation Accuracy = 92.222%\n",
            "Epoch:  0006 cost= 0.057228964\n",
            "EPOCH 6 : Validation Accuracy = 91.769%\n",
            "Epoch:  0007 cost= 0.049171635\n",
            "EPOCH 7 : Validation Accuracy = 92.948%\n",
            "Epoch:  0008 cost= 0.036925727\n",
            "EPOCH 8 : Validation Accuracy = 91.814%\n",
            "Epoch:  0009 cost= 0.033298687\n",
            "EPOCH 9 : Validation Accuracy = 92.290%\n",
            "Epoch:  0010 cost= 0.029385127\n",
            "EPOCH 10 : Validation Accuracy = 90.454%\n",
            "Epoch:  0011 cost= 0.026538879\n",
            "EPOCH 11 : Validation Accuracy = 91.814%\n",
            "Epoch:  0012 cost= 0.026469827\n",
            "EPOCH 12 : Validation Accuracy = 90.680%\n",
            "Epoch:  0013 cost= 0.023299567\n",
            "EPOCH 13 : Validation Accuracy = 92.245%\n",
            "Epoch:  0014 cost= 0.020261788\n",
            "EPOCH 14 : Validation Accuracy = 91.406%\n",
            "Epoch:  0015 cost= 0.024069453\n",
            "EPOCH 15 : Validation Accuracy = 92.925%\n",
            "Epoch:  0016 cost= 0.016160478\n",
            "EPOCH 16 : Validation Accuracy = 93.152%\n",
            "Epoch:  0017 cost= 0.020125889\n",
            "EPOCH 17 : Validation Accuracy = 93.537%\n",
            "Epoch:  0018 cost= 0.018506469\n",
            "EPOCH 18 : Validation Accuracy = 94.739%\n",
            "Epoch:  0019 cost= 0.010660831\n",
            "EPOCH 19 : Validation Accuracy = 93.673%\n",
            "Epoch:  0020 cost= 0.015504620\n",
            "EPOCH 20 : Validation Accuracy = 92.132%\n",
            "Epoch:  0021 cost= 0.017784293\n",
            "EPOCH 21 : Validation Accuracy = 94.399%\n",
            "Epoch:  0022 cost= 0.013004571\n",
            "EPOCH 22 : Validation Accuracy = 94.331%\n",
            "Epoch:  0023 cost= 0.014552169\n",
            "EPOCH 23 : Validation Accuracy = 93.946%\n",
            "Epoch:  0024 cost= 0.017063509\n",
            "EPOCH 24 : Validation Accuracy = 91.746%\n",
            "Epoch:  0025 cost= 0.009959780\n",
            "EPOCH 25 : Validation Accuracy = 94.082%\n",
            "Test Accuracy = 92.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ0AAAEWCAYAAAB8A8JQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXDxpOOdy8aThnMvFk4ZzLpf/D0flxWm2LhpyAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2laXHD7-tmk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generator\n",
        "'''\n",
        "\tGenerator definition for AdvGAN\n",
        "\tref: https://arxiv.org/pdf/1801.02610.pdf\n",
        "'''\n",
        "\n",
        "#import tensorflow as tf\n",
        "#from keras import layers\n",
        "\n",
        "# helper function for convolution -> instance norm -> relu\n",
        "def ConvInstNormRelu(x, filters, kernel_size=3, strides=1):\n",
        "\tConv = tf.layers.conv2d(\n",
        "\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tInstNorm = tf.contrib.layers.instance_norm(Conv)\n",
        "\n",
        "\treturn tf.nn.relu(InstNorm)\n",
        "\n",
        "\n",
        "# helper function for trans convolution -> instance norm -> relu\n",
        "def TransConvInstNormRelu(x, filters, kernel_size=3, strides=2):\n",
        "\tTransConv = tf.layers.conv2d_transpose(\n",
        "\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tInstNorm = tf.contrib.layers.instance_norm(TransConv)\n",
        "\n",
        "\treturn tf.nn.relu(InstNorm)\n",
        "\n",
        "# helper function for residual block of 2 convolutions with same num filters\n",
        "# in the same style as ConvInstNormRelu\n",
        "def ResBlock(x, training, filters=32, kernel_size=3, strides=1):\n",
        "\tconv1 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tconv1_norm = tf.layers.batch_normalization(conv1, training=training)\n",
        "\n",
        "\tconv1_relu = tf.nn.relu(conv1_norm)\n",
        "\n",
        "\tconv2 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\tinputs=conv1_relu,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tconv2_norm = tf.layers.batch_normalization(conv2, training=training)\n",
        "\n",
        "\n",
        "\treturn x + conv2_norm\n",
        "\n",
        "\n",
        "def generator(x, training):\n",
        "\twith tf.variable_scope('g_weights', reuse=tf.AUTO_REUSE): #True\n",
        "\t\t# input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "\t\t# define first three conv + inst + relu layers\n",
        "\t\tc1 = ConvInstNormRelu(x, filters=8, kernel_size=3, strides=1)\n",
        "\t\td1 = ConvInstNormRelu(c1, filters=16, kernel_size=3, strides=2)\n",
        "\t\td2 = ConvInstNormRelu(d1, filters=32, kernel_size=3, strides=2)\n",
        "\n",
        "\t\t# define residual blocks\n",
        "\t\trb1 = ResBlock(d2, training, filters=32)\n",
        "\t\trb2 = ResBlock(rb1, training, filters=32)\n",
        "\t\trb3 = ResBlock(rb2, training, filters=32)\n",
        "\t\trb4 = ResBlock(rb3, training, filters=32)\n",
        "\n",
        "\t\t# upsample using conv transpose\n",
        "\t\tu1 = TransConvInstNormRelu(rb4, filters=16, kernel_size=3, strides=2)\n",
        "\t\tu2 = TransConvInstNormRelu(u1, filters=8, kernel_size=3, strides=2)\n",
        "\n",
        "\t\t# final layer block\n",
        "\t\tout = tf.layers.conv2d_transpose(\n",
        "\t\t\t\t\t\tinputs=u2,\n",
        "\t\t\t\t\t\tfilters=x.get_shape()[-1].value, # or 3 if RGB image\n",
        "\t\t\t\t\t\tkernel_size=3,\n",
        "\t\t\t\t\t\tstrides=1,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\t\t# out = tf.contrib.layers.instance_norm(out)\n",
        "\n",
        "\t\treturn tf.nn.tanh(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKYLa86WtyLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Discriminator\n",
        "'''\n",
        "\tDiscriminator definition for AdvGAN\n",
        "\tref: https://arxiv.org/pdf/1801.02610.pdf\n",
        "'''\n",
        "\n",
        "#import tensorflow as tf\n",
        "\n",
        "def discriminator(x, training):\n",
        "\twith tf.variable_scope('d_weights', reuse=tf.AUTO_REUSE):\n",
        "\t\t# input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "\t\tconv1 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\t\tfilters=8,\n",
        "\t\t\t\t\t\t\tkernel_size=4,\n",
        "\t\t\t\t\t\t\tstrides=2,\n",
        "\t\t\t\t\t\t\tpadding=\"valid\",\n",
        "\t\t\t\t\t\t\tactivation=None)\n",
        "\t\tconv1 = tf.nn.leaky_relu(conv1, alpha=0.2)\n",
        "\n",
        "\t\t\n",
        "\t\tconv2 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\t\tinputs=conv1,\n",
        "\t\t\t\t\t\t\tfilters=16,\n",
        "\t\t\t\t\t\t\tkernel_size=4,\n",
        "\t\t\t\t\t\t\tstrides=2,\n",
        "\t\t\t\t\t\t\tpadding=\"valid\",\n",
        "\t\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\t\tin1 = tf.contrib.layers.instance_norm(conv2)\n",
        "\t\tconv2 = tf.nn.leaky_relu(in1, alpha=0.2)\n",
        "\n",
        "\t\tconv3 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\t\tinputs=conv2,\n",
        "\t\t\t\t\t\t\tfilters=32,\n",
        "\t\t\t\t\t\t\tkernel_size=4,\n",
        "\t\t\t\t\t\t\tstrides=2,\n",
        "\t\t\t\t\t\t\tpadding=\"valid\",\n",
        "\t\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\t\t#in2 = tf.contrib.layers.instance_norm(conv3)\n",
        "\t\tin2 = tf.contrib.layers.instance_norm(conv3)\n",
        "\t\tconv3 = tf.nn.leaky_relu(in2, alpha=0.2)\n",
        "\t\tflat = tf.layers.flatten(conv3)\n",
        "\t\tlogits = tf.layers.dense(flat, 1)\n",
        "\n",
        "\t\tprobs = tf.nn.sigmoid(logits)\n",
        "\n",
        "\t\treturn logits, probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "288LCGWawCVl",
        "colab_type": "code",
        "outputId": "c94fcd2e-ab2e-4310-9a2b-7b559310267c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4846
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "#from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os, sys\n",
        "import random\n",
        "\n",
        "#make sure GAN_setup.py is in connected folder\n",
        "#from GAN_setup import generator, discriminator\n",
        "\n",
        "#ctargon created class Target and defined/trained his target model in there, then called here\n",
        "#import Target as target_model\n",
        "\n",
        "\n",
        "# get the next batch based on x, y, and the iteration (based on batch_size)\n",
        "def next_batch(X, Y, i, batch_size):\n",
        "    idx = i * batch_size\n",
        "    idx_n = i * batch_size + batch_size\n",
        "    return X[idx:idx_n], Y[idx:idx_n]\n",
        "\n",
        "# loss function to encourage misclassification after perturbation\n",
        "def adv_loss(preds, labels, is_targeted):\n",
        "    real = tf.reduce_sum(labels * preds, 1)\n",
        "    other = tf.reduce_max((1 - labels) * preds - (labels * 10000), 1)\n",
        "    if is_targeted:\n",
        "        return tf.reduce_sum(tf.maximum(0.0, other - real))\n",
        "    return tf.reduce_sum(tf.maximum(0.0, real - other))\n",
        "\n",
        "# loss function to influence the perturbation to be as close to 0 as possible\n",
        "def perturb_loss(preds, thresh=0.3):\n",
        "    zeros = tf.zeros((tf.shape(preds)[0]))\n",
        "    return tf.reduce_mean(tf.maximum(zeros, tf.norm(tf.reshape(preds, (tf.shape(preds)[0], -1)), axis=1) - thresh))\n",
        "\n",
        "\n",
        "# function that defines ops, graphs, and training procedure for AdvGAN framework\n",
        "def AdvGAN(X, y, X_test, y_test, epochs=50, batch_size=128, target=3):\n",
        "    #print(X_train.shape)\n",
        "    #print(y.shape[-1]) is num_images\n",
        "    print(\"y shape\")\n",
        "    print(y.shape)\n",
        "    print(\"y_test shape\")\n",
        "    print(y_test.shape)\n",
        "    \n",
        "    # placeholder definitions\n",
        "    x_pl = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]]) # image placeholder\n",
        "    t = tf.placeholder(tf.float32, [None, y.shape[-1]]) # target placeholder\n",
        "    print(\"t shape)\")\n",
        "    print(t.shape)\n",
        "    is_training = tf.placeholder(tf.bool, [])\n",
        "\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    # MODEL DEFINITIONS\n",
        "    is_targeted = False\n",
        "    if target in range(0, y.shape[-1]):\n",
        "        is_targeted = True\n",
        "\n",
        "    # gather target model\n",
        "    f = Target()\n",
        "    print(\"is targeted boolean\")\n",
        "    print(is_targeted)\n",
        "    \n",
        "    thresh = 0.3\n",
        "\n",
        "    # generate perturbation, add to original input image(s)\n",
        "    perturb = tf.clip_by_value(generator(x_pl, is_training), -thresh, thresh)\n",
        "    x_perturbed = perturb + x_pl\n",
        "    x_perturbed = tf.clip_by_value(x_perturbed, 0, 1)\n",
        "    print(x_perturbed.shape)\n",
        "\n",
        "    # pass real and perturbed image to discriminator and the target model\n",
        "    d_real_logits, d_real_probs = discriminator(x_pl, is_training)\n",
        "    d_fake_logits, d_fake_probs = discriminator(x_perturbed, is_training)\n",
        "    print(d_fake_probs.shape)#1\n",
        "    # pass real and perturbed images to the model we are trying to fool\n",
        "    f_real_logits, f_real_probs = f.Model(x_pl)\n",
        "    f_fake_logits, f_fake_probs = f.Model(x_perturbed)\n",
        "    print(f_fake_probs.shape) #43\n",
        "\n",
        "    # generate labels for discriminator (optionally smooth labels for stability)\n",
        "    smooth = 0.0\n",
        "    d_labels_real = tf.ones_like(d_real_probs) * (1 - smooth)\n",
        "    d_labels_fake = tf.zeros_like(d_fake_probs)\n",
        "\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    # LOSS DEFINITIONS\n",
        "    # discriminator loss\n",
        "    d_loss_real = tf.losses.mean_squared_error(predictions=d_real_probs, labels=d_labels_real)\n",
        "    d_loss_fake = tf.losses.mean_squared_error(predictions=d_fake_probs, labels=d_labels_fake)\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "    # generator loss\n",
        "    g_loss_fake = tf.losses.mean_squared_error(predictions=d_fake_probs, labels=tf.ones_like(d_fake_probs))\n",
        "\n",
        "    # perturbation loss (minimize overall perturbation)\n",
        "    l_perturb = perturb_loss(perturb, thresh)\n",
        "\n",
        "    # adversarial loss (encourage misclassification)\n",
        "    l_adv = adv_loss(f_fake_probs, t, is_targeted)\n",
        "\n",
        "    # weights for generator loss function\n",
        "    alpha = 1.0\n",
        "    beta = 5.0\n",
        "    g_loss = l_adv + alpha*g_loss_fake + beta*l_perturb \n",
        "\n",
        "    # ----------------------------------------------------------------------------------\n",
        "    # gather variables for training/restoring\n",
        "    t_vars = tf.trainable_variables()\n",
        "    f_vars = [var for var in t_vars if 'Model' in var.name]\n",
        "    d_vars = [var for var in t_vars if 'd_' in var.name]\n",
        "    g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='g_weights')\n",
        "\n",
        "    # define optimizers for discriminator and generator\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        d_opt = tf.train.AdamOptimizer().minimize(d_loss, var_list=d_vars)\n",
        "        g_opt = tf.train.AdamOptimizer(learning_rate=0.001).minimize(g_loss, var_list=g_vars)\n",
        "\n",
        "\t# create saver objects for the target model, generator, and discriminator\n",
        "    saver = tf.train.Saver(f_vars)\n",
        "    g_saver = tf.train.Saver(g_vars)\n",
        "    d_saver = tf.train.Saver(d_vars)\n",
        "\n",
        "    init  = tf.global_variables_initializer()\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    sess.run(init)\n",
        "    \n",
        "    # load the pretrained target model\n",
        "    #try:\n",
        "       # saver.restore(sess, \"./weights/target_model/model\")\n",
        "    #except:\n",
        "       # print(\"make sure to train the target model first...\")\n",
        "       # sys.exit(1)\n",
        "\n",
        "    \n",
        "    new_saver = tf.train.import_meta_graph('./weights/target_model/model.meta')\n",
        "    new_saver.restore(sess, tf.train.latest_checkpoint('./weights/target_model'))\n",
        "    #path_to_ckpt_data = './weights/target_model/model.data-00000-of-00001'\n",
        "    #new_saver.restore(sess, path_to_ckpt_data)\n",
        "    \n",
        "    print(\"Pretrained model loaded\")\n",
        "    \n",
        "    total_batches = int(X.shape[0] / batch_size)\n",
        "\n",
        "    for epoch in range(0, epochs):\n",
        "\n",
        "        loss_D_sum = 0.0\n",
        "        loss_G_fake_sum = 0.0\n",
        "        loss_perturb_sum = 0.0\n",
        "        loss_adv_sum = 0.0\n",
        "\n",
        "        for i in range(total_batches):\n",
        "\n",
        "            batch_x, batch_y = next_batch(X, y, i, batch_size)\n",
        "\n",
        "            # if targeted, create one hot vectors of the target\n",
        "            if is_targeted:\n",
        "                targets = np.full((batch_y.shape[0],), target)\n",
        "                batch_y = np.eye(y.shape[-1])[targets]\n",
        "\n",
        "            # train the discriminator first n times\n",
        "            for _ in range(1):\n",
        "                _, loss_D_batch = sess.run([d_opt, d_loss], feed_dict={x_pl: batch_x, \\\n",
        "                                           is_training: True})\n",
        "\n",
        "            #print(\"batch x\")\n",
        "            #print(batch_x.shape)\n",
        "            #print(\"batch y\")\n",
        "            #print(batch_y.shape)\n",
        "            \n",
        "\t\t\t       # train the generator n times\n",
        "            for _ in range(1):\n",
        "                \n",
        "                _, loss_G_fake_batch, loss_adv_batch, loss_perturb_batch = \\\n",
        "                        sess.run([g_opt, g_loss_fake, l_adv, l_perturb], \\\n",
        "                              feed_dict={x_pl: batch_x, \\\n",
        "                                     t: batch_y, \\\n",
        "                                     is_training: True})\n",
        "            loss_D_sum += loss_D_batch\n",
        "            loss_G_fake_sum += loss_G_fake_batch\n",
        "            loss_perturb_sum += loss_perturb_batch\n",
        "            loss_adv_sum += loss_adv_batch\n",
        "\n",
        "        print(\"epoch %d:\\nloss_D: %.3f, loss_G_fake: %.3f, \\\n",
        "            \\nloss_perturb: %.3f, loss_adv: %.3f, \\n\" %\n",
        "            (epoch + 1, loss_D_sum/total_batches, loss_G_fake_sum/total_batches,\n",
        "            loss_perturb_sum/total_batches, loss_adv_sum/total_batches))\n",
        "    #epoch_losses = np.array([loss_D_sum/totalbatches], [loss_G_fake_sum/total_batches], [loss_perturb_sum/totalbatches], [loss_adv_sum/total_batches])\n",
        "\t\t#np.savetxt(\"epoch_{}_losses.txt\".format(epoch), epoch_losses, delimiter=',')\n",
        "    \n",
        "        if epoch % 10 == 0:\n",
        "            g_saver.save(sess, \"weights/generator/gen\")\n",
        "            d_saver.save(sess, \"weights/discriminator/disc\")\n",
        "\n",
        "    # evaluate the test set\n",
        "    correct_prediction = tf.equal(tf.argmax(f_fake_probs, 1), tf.argmax(t, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    accs = []\n",
        "    total_batches_test = int(X_test.shape[0] / batch_size)\n",
        "    for i in range(total_batches_test):\n",
        "        batch_x, batch_y = next_batch(X_test, y_test, i, batch_size)\n",
        "        acc, x_pert = sess.run([accuracy, x_perturbed], feed_dict={x_pl: batch_x, t: batch_y, is_training: False})\n",
        "        accs.append(acc)\n",
        "\n",
        "    print('accuracy of test set: {}'.format(sum(accs) / len(accs)))\n",
        "  #test_acc = np.array(sum(accs)/len(accs))\n",
        "  #np.savetxt(\"test_accuracy_GD.txt\", test_acc, delimiter=',')\n",
        "\n",
        "\t# plot some images and their perturbed counterparts\n",
        "    f, axarr = plt.subplots(2,2)\n",
        "    axarr[0,0].imshow(np.squeeze(batch_x[2]), cmap='Greys_r')\n",
        "    axarr[0,1].imshow(np.squeeze(x_pert[2]), cmap='Greys_r')\n",
        "    axarr[1,0].imshow(np.squeeze(batch_x[5]), cmap='Greys_r')\n",
        "    axarr[1,1].imshow(np.squeeze(x_pert[5]), cmap='Greys_r')\n",
        "    plt.show()\n",
        "\n",
        "    print('finished training, saving weights')\n",
        "    g_saver.save(sess, \"weights/generator/gen\")\n",
        "    d_saver.save(sess, \"weights/discriminator/disc\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def attack(X, y, batch_size=128, thresh=0.3, target=3):\n",
        "    x_pl = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]]) # image placeholder\n",
        "    t = tf.placeholder(tf.float32, [None, y.shape[-1]]) # target placeholder\n",
        "    is_training = tf.placeholder(tf.bool, [])\n",
        "\n",
        "    is_targeted = False\n",
        "    if target in range(0, y.shape[-1]):\n",
        "        is_targeted = True\n",
        "\n",
        "    perturb = tf.clip_by_value(generator(x_pl, is_training), -thresh, thresh)\n",
        "    x_perturbed = perturb + x_pl\n",
        "    x_perturbed = tf.clip_by_value(x_perturbed, 0, 1)\n",
        "\n",
        "    f = Target()\n",
        "    f_real_logits, f_real_probs = f.Model(x_pl)\n",
        "    f_fake_logits, f_fake_probs = f.Model(x_perturbed)\n",
        "\n",
        "    t_vars = tf.trainable_variables()\n",
        "    f_vars = [var for var in t_vars if 'Model' in var.name]\n",
        "    g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='g_weights')\n",
        "\n",
        "    sess = tf.Session()\n",
        "\n",
        "    f_saver2 = tf.train.Saver(f_vars)\n",
        "    g_saver2 = tf.train.Saver(g_vars)\n",
        "    f_saver2.restore(sess, \"./weights/target_model/model\")\n",
        "    g_saver2.restore(sess, tf.train.latest_checkpoint(\"./weights/generator/\"))\n",
        "\n",
        "    rawpert, pert, fake_l, real_l = sess.run([perturb, x_perturbed, f_fake_probs, f_real_probs], \\\n",
        "                          feed_dict={x_pl: X[:32], \\\n",
        "                                 is_training: False})\n",
        "    print('LA: ' + str(np.argmax(y[:32], axis=1)))\n",
        "    print('OG: ' + str(np.argmax(real_l, axis=1)))\n",
        "    print('PB: ' + str(np.argmax(fake_l, axis=1)))\n",
        "\n",
        "    correct_prediction = tf.equal(tf.argmax(f_fake_probs, 1), tf.argmax(t, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    accs = []\n",
        "    total_batches_test = int(X.shape[0] / batch_size)\n",
        "    for i in range(total_batches_test):\n",
        "        batch_x, batch_y = next_batch(X, y, i, batch_size)\n",
        "\n",
        "        if is_targeted:\n",
        "            targets = np.full((batch_y.shape[0],), target)\n",
        "            batch_y = np.eye(y.shape[-1])[targets]\n",
        "\n",
        "        acc, fake_l, x_pert = sess.run([accuracy, f_fake_probs, x_perturbed], feed_dict={x_pl: batch_x, t: batch_y, is_training: False})\n",
        "        accs.append(acc)\n",
        "\n",
        "    print('accuracy of test set: {}'.format(sum(accs) / len(accs)))\n",
        "\n",
        "    f, axarr = plt.subplots(2,2)\n",
        "    axarr[0,0].imshow(np.squeeze(X[3]), cmap='Greys_r')\n",
        "    axarr[0,1].imshow(np.squeeze(pert[3]), cmap='Greys_r')\n",
        "    axarr[1,0].imshow(np.squeeze(X[4]), cmap='Greys_r')\n",
        "    axarr[1,1].imshow(np.squeeze(pert[4]), cmap='Greys_r')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "'''\n",
        "import pickle\n",
        "\n",
        "training_file = 'train.p'\n",
        "testing_file = 'test.p'\n",
        "validation_file = 'valid.p'\n",
        "\n",
        "with open(training_file, mode='rb') as f:\n",
        "    tstrain = pickle.load(f)\n",
        "with open(testing_file, mode='rb') as f:\n",
        "    tstest = pickle.load(f)\n",
        "with open(validation_file, mode='rb') as f:\n",
        "    tsvalid = pickle.load(f)\n",
        "\n",
        "X_train, Y_train = tstrain['features'], tstrain['labels']\n",
        "X_valid, Y_valid = tsvalid['features'], tsvalid['labels']\n",
        "X_test, Y_test = tstest['features'], tstest['labels']\n",
        "\n",
        "#shuffle training set\n",
        "X_train, Y_train = shuffle(X_train, Y_train)\n",
        "\n",
        "#grayscale images\n",
        "grayscale = [0.299,0.587,0.144]\n",
        "\n",
        "X_test = np.dot(X_test, grayscale)\n",
        "X_train = np.dot(X_train, grayscale)\n",
        "X_valid = np.dot(X_valid, grayscale)\n",
        "\n",
        "\n",
        "#normalize\n",
        "X_train = np.array(X_train)/255\n",
        "X_test = np.array(X_test)/255\n",
        "X_valid = np.array(X_valid)/255\n",
        "\n",
        "#expand dimensions to fit 4D input array\n",
        "X_train = np.expand_dims(X_train,-1)\n",
        "X_test = np.expand_dims(X_test,-1)\n",
        "X_valid = np.expand_dims(X_valid,-1)\n",
        "\n",
        "assert(len(X_train)==len(Y_train))\n",
        "n_train = len(X_train)\n",
        "assert(len(X_test)==len(Y_test))\n",
        "n_test = len(X_test)\n",
        "'''\n",
        "Y_train = to_categorical(Y_train, num_classes=43)\n",
        "Y_test = to_categorical(Y_test, num_classes=43)\n",
        "AdvGAN(X_train, Y_train, X_test, Y_test, batch_size=128, epochs=50, target=3)\n",
        "attack(X_test, Y_test, target=3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y shape\n",
            "(34799, 43)\n",
            "y_test shape\n",
            "(12630, 43)\n",
            "t shape)\n",
            "(?, 43)\n",
            "is targeted boolean\n",
            "True\n",
            "WARNING:tensorflow:From <ipython-input-3-5caa55e74d60>:17: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-3-5caa55e74d60>:49: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n",
            "WARNING:tensorflow:From <ipython-input-3-5caa55e74d60>:32: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d_transpose instead.\n",
            "(?, 32, 32, 1)\n",
            "WARNING:tensorflow:From <ipython-input-4-85d97f4f431c>:44: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-4-85d97f4f431c>:45: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "(?, 1)\n",
            "(?, 43)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./weights/target_model/model\n",
            "Pretrained model loaded\n",
            "epoch 1:\n",
            "loss_D: 0.052, loss_G_fake: 0.918,             \n",
            "loss_perturb: 1.636, loss_adv: 4.909, \n",
            "\n",
            "epoch 2:\n",
            "loss_D: 0.021, loss_G_fake: 0.974,             \n",
            "loss_perturb: 0.363, loss_adv: 4.953, \n",
            "\n",
            "epoch 3:\n",
            "loss_D: 0.011, loss_G_fake: 0.982,             \n",
            "loss_perturb: 0.201, loss_adv: 4.927, \n",
            "\n",
            "epoch 4:\n",
            "loss_D: 0.005, loss_G_fake: 0.992,             \n",
            "loss_perturb: 0.022, loss_adv: 4.883, \n",
            "\n",
            "epoch 5:\n",
            "loss_D: 0.003, loss_G_fake: 0.994,             \n",
            "loss_perturb: 0.019, loss_adv: 4.841, \n",
            "\n",
            "epoch 6:\n",
            "loss_D: 0.002, loss_G_fake: 0.995,             \n",
            "loss_perturb: 0.027, loss_adv: 4.827, \n",
            "\n",
            "epoch 7:\n",
            "loss_D: 0.002, loss_G_fake: 0.996,             \n",
            "loss_perturb: 0.028, loss_adv: 4.774, \n",
            "\n",
            "epoch 8:\n",
            "loss_D: 0.002, loss_G_fake: 0.997,             \n",
            "loss_perturb: 0.036, loss_adv: 4.792, \n",
            "\n",
            "epoch 9:\n",
            "loss_D: 0.001, loss_G_fake: 0.998,             \n",
            "loss_perturb: 0.034, loss_adv: 4.778, \n",
            "\n",
            "epoch 10:\n",
            "loss_D: 0.001, loss_G_fake: 0.999,             \n",
            "loss_perturb: 0.047, loss_adv: 4.790, \n",
            "\n",
            "epoch 11:\n",
            "loss_D: 0.001, loss_G_fake: 0.999,             \n",
            "loss_perturb: 0.030, loss_adv: 4.774, \n",
            "\n",
            "epoch 12:\n",
            "loss_D: 0.001, loss_G_fake: 0.999,             \n",
            "loss_perturb: 0.038, loss_adv: 4.787, \n",
            "\n",
            "epoch 13:\n",
            "loss_D: 0.001, loss_G_fake: 0.999,             \n",
            "loss_perturb: 0.038, loss_adv: 4.804, \n",
            "\n",
            "epoch 14:\n",
            "loss_D: 0.001, loss_G_fake: 0.999,             \n",
            "loss_perturb: 0.056, loss_adv: 4.806, \n",
            "\n",
            "epoch 15:\n",
            "loss_D: 0.001, loss_G_fake: 0.999,             \n",
            "loss_perturb: 0.071, loss_adv: 4.839, \n",
            "\n",
            "epoch 16:\n",
            "loss_D: 0.001, loss_G_fake: 0.999,             \n",
            "loss_perturb: 0.053, loss_adv: 4.825, \n",
            "\n",
            "epoch 17:\n",
            "loss_D: 0.001, loss_G_fake: 1.000,             \n",
            "loss_perturb: 0.060, loss_adv: 4.819, \n",
            "\n",
            "epoch 18:\n",
            "loss_D: 0.001, loss_G_fake: 1.000,             \n",
            "loss_perturb: 0.066, loss_adv: 4.855, \n",
            "\n",
            "epoch 19:\n",
            "loss_D: 0.001, loss_G_fake: 1.000,             \n",
            "loss_perturb: 0.042, loss_adv: 4.858, \n",
            "\n",
            "epoch 20:\n",
            "loss_D: 0.001, loss_G_fake: 1.000,             \n",
            "loss_perturb: 0.070, loss_adv: 4.865, \n",
            "\n",
            "epoch 21:\n",
            "loss_D: 0.001, loss_G_fake: 1.000,             \n",
            "loss_perturb: 0.045, loss_adv: 4.889, \n",
            "\n",
            "epoch 22:\n",
            "loss_D: 0.001, loss_G_fake: 1.000,             \n",
            "loss_perturb: 0.045, loss_adv: 4.893, \n",
            "\n",
            "epoch 23:\n",
            "loss_D: 0.003, loss_G_fake: 1.000,             \n",
            "loss_perturb: 0.076, loss_adv: 4.973, \n",
            "\n",
            "epoch 24:\n",
            "loss_D: 0.017, loss_G_fake: 0.999,             \n",
            "loss_perturb: 0.096, loss_adv: 5.021, \n",
            "\n",
            "epoch 25:\n",
            "loss_D: 0.060, loss_G_fake: 0.987,             \n",
            "loss_perturb: 0.094, loss_adv: 5.043, \n",
            "\n",
            "epoch 26:\n",
            "loss_D: 0.549, loss_G_fake: 0.313,             \n",
            "loss_perturb: 0.001, loss_adv: 4.999, \n",
            "\n",
            "epoch 27:\n",
            "loss_D: 0.503, loss_G_fake: 0.252,             \n",
            "loss_perturb: 0.000, loss_adv: 4.997, \n",
            "\n",
            "epoch 28:\n",
            "loss_D: 0.501, loss_G_fake: 0.251,             \n",
            "loss_perturb: 0.001, loss_adv: 4.991, \n",
            "\n",
            "epoch 29:\n",
            "loss_D: 0.501, loss_G_fake: 0.251,             \n",
            "loss_perturb: 0.001, loss_adv: 4.990, \n",
            "\n",
            "epoch 30:\n",
            "loss_D: 0.500, loss_G_fake: 0.251,             \n",
            "loss_perturb: 0.002, loss_adv: 4.989, \n",
            "\n",
            "epoch 31:\n",
            "loss_D: 0.500, loss_G_fake: 0.251,             \n",
            "loss_perturb: 0.003, loss_adv: 4.987, \n",
            "\n",
            "epoch 32:\n",
            "loss_D: 0.500, loss_G_fake: 0.251,             \n",
            "loss_perturb: 0.006, loss_adv: 4.986, \n",
            "\n",
            "epoch 33:\n",
            "loss_D: 0.500, loss_G_fake: 0.251,             \n",
            "loss_perturb: 0.010, loss_adv: 4.984, \n",
            "\n",
            "epoch 34:\n",
            "loss_D: 0.479, loss_G_fake: 0.279,             \n",
            "loss_perturb: 1.119, loss_adv: 4.559, \n",
            "\n",
            "epoch 35:\n",
            "loss_D: 0.247, loss_G_fake: 0.599,             \n",
            "loss_perturb: 5.784, loss_adv: 3.030, \n",
            "\n",
            "epoch 36:\n",
            "loss_D: 0.145, loss_G_fake: 0.757,             \n",
            "loss_perturb: 6.411, loss_adv: 2.869, \n",
            "\n",
            "epoch 37:\n",
            "loss_D: 0.130, loss_G_fake: 0.790,             \n",
            "loss_perturb: 6.106, loss_adv: 2.944, \n",
            "\n",
            "epoch 38:\n",
            "loss_D: 0.175, loss_G_fake: 0.747,             \n",
            "loss_perturb: 4.628, loss_adv: 3.343, \n",
            "\n",
            "epoch 39:\n",
            "loss_D: 0.197, loss_G_fake: 0.716,             \n",
            "loss_perturb: 3.943, loss_adv: 3.545, \n",
            "\n",
            "epoch 40:\n",
            "loss_D: 0.197, loss_G_fake: 0.710,             \n",
            "loss_perturb: 3.771, loss_adv: 3.598, \n",
            "\n",
            "epoch 41:\n",
            "loss_D: 0.197, loss_G_fake: 0.709,             \n",
            "loss_perturb: 3.651, loss_adv: 3.636, \n",
            "\n",
            "epoch 42:\n",
            "loss_D: 0.199, loss_G_fake: 0.705,             \n",
            "loss_perturb: 3.496, loss_adv: 3.685, \n",
            "\n",
            "epoch 43:\n",
            "loss_D: 0.204, loss_G_fake: 0.699,             \n",
            "loss_perturb: 3.328, loss_adv: 3.740, \n",
            "\n",
            "epoch 44:\n",
            "loss_D: 0.208, loss_G_fake: 0.694,             \n",
            "loss_perturb: 3.188, loss_adv: 3.786, \n",
            "\n",
            "epoch 45:\n",
            "loss_D: 0.211, loss_G_fake: 0.691,             \n",
            "loss_perturb: 3.082, loss_adv: 3.821, \n",
            "\n",
            "epoch 46:\n",
            "loss_D: 0.211, loss_G_fake: 0.692,             \n",
            "loss_perturb: 3.045, loss_adv: 3.833, \n",
            "\n",
            "epoch 47:\n",
            "loss_D: 0.208, loss_G_fake: 0.695,             \n",
            "loss_perturb: 3.017, loss_adv: 3.843, \n",
            "\n",
            "epoch 48:\n",
            "loss_D: 0.209, loss_G_fake: 0.695,             \n",
            "loss_perturb: 2.977, loss_adv: 3.856, \n",
            "\n",
            "epoch 49:\n",
            "loss_D: 0.209, loss_G_fake: 0.696,             \n",
            "loss_perturb: 2.953, loss_adv: 3.864, \n",
            "\n",
            "epoch 50:\n",
            "loss_D: 0.210, loss_G_fake: 0.695,             \n",
            "loss_perturb: 2.891, loss_adv: 3.886, \n",
            "\n",
            "accuracy of test set: 0.031568877551020405\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAD8CAYAAADub8g7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJHYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "finished training, saving weights\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8012cbe63755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m43\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0mAdvGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m \u001b[0mattack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-8012cbe63755>\u001b[0m in \u001b[0;36mattack\u001b[0;34m(X, y, batch_size, thresh, target)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0mf_saver2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0mg_saver2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mf_saver2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./weights/target_model/model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[1;32m    830\u001b[0m           time.time() + self._keep_checkpoint_every_n_hours * 3600)\n\u001b[1;32m    831\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0mrestore_sequentially\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_sequentially\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m           \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m           build_save=build_save, build_restore=build_restore)\n\u001b[0m\u001b[1;32m    882\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m       \u001b[0;31m# Since self._name is used as a name_scope by builder(), we are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build_internal\u001b[0;34m(self, names_to_saveables, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, filename, build_save, build_restore)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     saveables = saveable_object_util.validate_and_slice_inputs(\n\u001b[0;32m--> 487\u001b[0;31m         names_to_saveables)\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_to_keep\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0mmax_to_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\u001b[0m in \u001b[0;36mvalidate_and_slice_inputs\u001b[0;34m(names_to_saveables)\u001b[0m\n\u001b[1;32m    329\u001b[0m   \"\"\"\n\u001b[1;32m    330\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0mnames_to_saveables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_list_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m   \u001b[0msaveables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\u001b[0m in \u001b[0;36mop_list_to_dict\u001b[0;34m(op_list, convert_variable_to_tensor)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m           raise ValueError(\"At least two variables have the same name: %s\" %\n\u001b[0;32m--> 288\u001b[0;31m                            name)\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: At least two variables have the same name: Model/Variable"
          ]
        }
      ]
    }
  ]
}
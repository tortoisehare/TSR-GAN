{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fullGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tortoisehare/TSR-GAN/blob/master/fullGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABnslutLuWVE",
        "colab_type": "code",
        "outputId": "63f80e7d-f50d-41ad-aae8-1d4fc2164f6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!mkdir weights\n",
        "!mkdir weights/target_model\n",
        "!mkdir weights/generator\n",
        "!mkdir weights/discriminator\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘weights’: File exists\n",
            "mkdir: cannot create directory ‘weights/target_model’: File exists\n",
            "mkdir: cannot create directory ‘weights/generator’: File exists\n",
            "mkdir: cannot create directory ‘weights/discriminator’: File exists\n",
            "sample_data  test.p  train.p  valid.p  weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-dNiamCtfBr",
        "colab_type": "code",
        "outputId": "f4f94813-e9f2-402f-e44f-a1c39390ffc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1318
        }
      },
      "source": [
        "#TARGET CLASSIFIER\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(1187) #to help reproduce\n",
        "\n",
        "#from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import csv\n",
        "#import random\n",
        "#import cv2\n",
        "#from skimage.filters import rank\n",
        "#import skimage.morphology as morp\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "#from keras.utils import to_categorical\n",
        "#from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "#from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "\n",
        "class Target:\n",
        "    def __init__(self, lr=0.001, epochs=25, n_input=32, n_classes=43, batch_size=20, restore=0):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.n_input = 32\n",
        "        self.n_classes = 43\n",
        "        self.batch_size = batch_size\n",
        "        self.restore = restore\n",
        "\n",
        "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "    \n",
        "    '''\n",
        "    def next_batch(self, X, Y, i, batch_size):\n",
        "      idx = i*batch_size\n",
        "      idx_n = idx + batch_size\n",
        "      return X[idx:idx_n], Y[idx:idx_n]\n",
        "      '''\n",
        "    \n",
        "    def Model(self, x):\n",
        "        with tf.variable_scope('Model', reuse=tf.AUTO_REUSE):\n",
        "            # Hyperparameters\n",
        "            mu = 0\n",
        "            sigma = 0.1\n",
        "            n_out = self.n_classes\n",
        "            learning_rate = self.lr\n",
        "\n",
        "            # Layer 1 (Convolutional): Input = 32x32x1. Output = 28x28x6.\n",
        "            filter1_width = 5\n",
        "            filter1_height = 5\n",
        "            input1_channels = 1\n",
        "            conv1_output = 6\n",
        "            # Weight and bias\n",
        "            conv1_weight = tf.Variable(tf.truncated_normal(\n",
        "                shape=(filter1_width, filter1_height, input1_channels, conv1_output),\n",
        "                mean = mu, stddev = sigma))\n",
        "            conv1_bias = tf.Variable(tf.zeros(conv1_output))\n",
        "\n",
        "            # Apply Convolution\n",
        "            conv1 = tf.nn.conv2d(x, conv1_weight, strides=[1, 1, 1, 1], padding='VALID') + conv1_bias\n",
        "\n",
        "            # Activation:\n",
        "            conv1 = tf.nn.relu(conv1)\n",
        "\n",
        "            # Pooling: Input = 28x28x6. Output = 14x14x6.\n",
        "            conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "            # Layer 2 (Convolutional): Output = 10x10x16.\n",
        "            filter2_width = 5\n",
        "            filter2_height = 5\n",
        "            input2_channels = 6\n",
        "            conv2_output = 16\n",
        "            # Weight and bias\n",
        "            conv2_weight = tf.Variable(tf.truncated_normal(\n",
        "                shape=(filter2_width, filter2_height, input2_channels, conv2_output),\n",
        "                mean = mu, stddev = sigma))\n",
        "            conv2_bias = tf.Variable(tf.zeros(conv2_output))\n",
        "\n",
        "            # Apply Convolution\n",
        "            conv2 = tf.nn.conv2d(conv1, conv2_weight, strides=[1, 1, 1, 1], padding='VALID') + conv2_bias\n",
        "\n",
        "            # Activation:\n",
        "            conv2 = tf.nn.relu(conv2)\n",
        "\n",
        "            # Pooling: Input = 10x10x16. Output = 5x5x16.\n",
        "            conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "            # Flattening: Input = 5x5x16. Output = 400.\n",
        "            fully_connected0 = Flatten()(conv2)\n",
        "\n",
        "            # Layer 3 (Fully Connected): Input = 400. Output = 120.\n",
        "            connected1_weights = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
        "            connected1_bias = tf.Variable(tf.zeros(120))\n",
        "            fully_connected1 = tf.add((tf.matmul(fully_connected0, connected1_weights)), connected1_bias)\n",
        "\n",
        "            # Activation:\n",
        "            fully_connected1 = tf.nn.relu(fully_connected1)\n",
        "\n",
        "            # Layer 4 (Fully Connected): Input = 120. Output = 84.\n",
        "            connected2_weights = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
        "            connected2_bias = tf.Variable(tf.zeros(84))\n",
        "            fully_connected2 = tf.add((tf.matmul(fully_connected1, connected2_weights)), connected2_bias)\n",
        "\n",
        "            # Activation.\n",
        "            fully_connected2 = tf.nn.relu(fully_connected2)\n",
        "    \n",
        "            # Layer 5 (Fully Connected): Input = 84. Output = 43.\n",
        "            output_weights = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n",
        "            output_bias = tf.Variable(tf.zeros(43))\n",
        "            logits =  tf.add((tf.matmul(fully_connected2, output_weights)), output_bias)\n",
        "\n",
        "            probs = tf.nn.sigmoid(logits)\n",
        "            \n",
        "            return logits, probs\n",
        "    \n",
        "    \n",
        "    \n",
        "    def test(self, X_data, BATCH_SIZE=64):\n",
        "        num_examples = len(X_data)\n",
        "        y_pred = np.zeros(num_examples, dtype=np.int32)\n",
        "        sess = tf.get_default_session()\n",
        "        for offset in range(0, num_examples, BATCH_SIZE):\n",
        "            batch_x = X_data[offset:offset+BATCH_SIZE]\n",
        "            y_pred[offset:offset+BATCH_SIZE] = sess.run(tf.argmax(logits, 1), \n",
        "                               feed_dict={x:batch_x, keep_prob:1, keep_prob_conv:1})\n",
        "        return y_pred\n",
        "    \n",
        "    \n",
        "    def train(self, X_train, Y_train, X_valid, Y_valid):\n",
        "        #placeholders for inputs\n",
        "        x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
        "        y = tf.placeholder(tf.int32, (None))\n",
        "\n",
        "        keep_prob = tf.placeholder(tf.float32)       # For fully-connected layers\n",
        "        keep_prob_conv = tf.placeholder(tf.float32)\n",
        "\n",
        "        #define graph\n",
        "        logits, _ = self.Model(x)\n",
        "        \n",
        "              # Training operation\n",
        "        one_hot_y = tf.one_hot(y, 43)\n",
        "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=one_hot_y)\n",
        "        loss_operation = tf.reduce_mean(cross_entropy)\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate = self.lr)\n",
        "        training_operation = optimizer.minimize(loss_operation)\n",
        "\n",
        "        # Accuracy operation\n",
        "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
        "        accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "\n",
        "          # Saving all variables\n",
        "        saver = tf.train.Saver()\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            num_train = len(Y_train)\n",
        "            num_valid = len(Y_valid)\n",
        "            print(\"Training ...\")\n",
        "            print()\n",
        "            EPOCHS = self.epochs\n",
        "            BATCH_SIZE = self.batch_size\n",
        "            DIR = \"./weights/target_model\"\n",
        "            total_batch = int(X_train.shape[0] / self.batch_size)\n",
        "\n",
        "            for i in range(EPOCHS):\n",
        "                avg_cost = 0.\n",
        "                total_accuracy = 0\n",
        "                validation_accuracy = 0\n",
        "                #Train set\n",
        "                for offset in range(0, num_train, BATCH_SIZE):\n",
        "                    end = offset + BATCH_SIZE\n",
        "                    batch_x, batch_y = X_train[offset:end], Y_train[offset:end]\n",
        "                    _, c = sess.run([training_operation, loss_operation], feed_dict={x: batch_x, y: batch_y, keep_prob : 0.6, keep_prob_conv: 0.8})\n",
        "                    avg_cost += c / total_batch\n",
        "                    \n",
        "                    #Validation Set\n",
        "                for offset in range(0, num_valid, BATCH_SIZE):\n",
        "                    end = offset + BATCH_SIZE\n",
        "                    batch_x, batch_y = X_valid[offset:end], Y_valid[offset:end]\n",
        "                    accuracy = sess.run(accuracy_operation, \n",
        "                                    feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0, keep_prob_conv: 1.0 })\n",
        "                    total_accuracy += (accuracy * len(batch_x))\n",
        "                    validation_accuracy = total_accuracy / num_valid\n",
        "                    #print(\"Validation Accuracy = {:.3f}%\".format(validation_accuracy*100))\n",
        "                    \n",
        "                print(\"Epoch: \", '%04d' % (i+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
        "                print(\"EPOCH {} : Validation Accuracy = {:.3f}%\".format(i+1, (validation_accuracy*100)))\n",
        "            \n",
        "            \n",
        "            #Test set\n",
        "            num_examples = len(X_test)\n",
        "            y_pred = np.zeros(num_examples, dtype=np.int32)\n",
        "            #sess = tf.get_default_session()\n",
        "            for offset in range(0, num_examples, BATCH_SIZE):\n",
        "                batch_x = X_test[offset:offset+BATCH_SIZE]\n",
        "                y_pred[offset:offset+BATCH_SIZE] = sess.run(tf.argmax(logits, 1), \n",
        "                                   feed_dict={x:batch_x, keep_prob:1, keep_prob_conv:1})\n",
        "            test_accuracy = sum(Y_test == y_pred)/len(Y_test)\n",
        "            print(\"Test Accuracy = {:.1f}%\".format(test_accuracy*100))\n",
        "\n",
        "            cm = confusion_matrix(Y_test, y_pred)\n",
        "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "            cm = np.log(.0001 + cm)\n",
        "            plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "            plt.title('Log of normalized Confusion Matrix')\n",
        "            plt.ylabel('True label')\n",
        "            plt.xlabel('Predicted label')\n",
        "            plt.show()\n",
        "            \n",
        "            saver.save(sess, \"./weights/target_model/model\")\n",
        "            print(\"Model saved\")\n",
        "            sess.close()\n",
        "\n",
        "   \n",
        "      \n",
        "      \n",
        "import pickle\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    training_file = 'train.p'\n",
        "    testing_file = 'test.p'\n",
        "    validation_file = 'valid.p'\n",
        "\n",
        "    with open(training_file, mode='rb') as f:\n",
        "        tstrain = pickle.load(f)\n",
        "    with open(testing_file, mode='rb') as f:\n",
        "        tstest = pickle.load(f)\n",
        "    with open(validation_file, mode='rb') as f:\n",
        "        tsvalid = pickle.load(f)\n",
        "\n",
        "    X_train, Y_train = tstrain['features'], tstrain['labels']\n",
        "    X_valid, Y_valid = tsvalid['features'], tsvalid['labels']\n",
        "    X_test, Y_test = tstest['features'], tstest['labels']\n",
        "\n",
        "    #shuffle training set\n",
        "    X_train, Y_train = shuffle(X_train, Y_train)\n",
        "\n",
        "    #grayscale images\n",
        "    grayscale = [0.299,0.587,0.144]\n",
        "\n",
        "    X_test = np.dot(X_test, grayscale)\n",
        "    X_train = np.dot(X_train, grayscale)\n",
        "    X_valid = np.dot(X_valid, grayscale)\n",
        "\n",
        "\n",
        "    #normalize\n",
        "    X_train = np.array(X_train)/255\n",
        "    X_test = np.array(X_test)/255\n",
        "    X_valid = np.array(X_valid)/255\n",
        "\n",
        "    #expand dimensions to fit 4D input array\n",
        "    X_train = np.expand_dims(X_train,-1)\n",
        "    X_test = np.expand_dims(X_test,-1)\n",
        "    X_valid = np.expand_dims(X_valid,-1)\n",
        "\n",
        "    assert(len(X_train)==len(Y_train))\n",
        "    n_train = len(X_train)\n",
        "    assert(len(X_test)==len(Y_test))\n",
        "    n_test = len(X_test)\n",
        "\n",
        "    cnn = Target()\n",
        "    cnn.train(X_train, Y_train, X_valid, Y_valid)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Training ...\n",
            "\n",
            "Epoch:  0001 cost= 1.267719335\n",
            "EPOCH 1 : Validation Accuracy = 82.268%\n",
            "Epoch:  0002 cost= 0.304000083\n",
            "EPOCH 2 : Validation Accuracy = 88.118%\n",
            "Epoch:  0003 cost= 0.168383416\n",
            "EPOCH 3 : Validation Accuracy = 89.615%\n",
            "Epoch:  0004 cost= 0.109131617\n",
            "EPOCH 4 : Validation Accuracy = 90.227%\n",
            "Epoch:  0005 cost= 0.082652130\n",
            "EPOCH 5 : Validation Accuracy = 91.202%\n",
            "Epoch:  0006 cost= 0.061802101\n",
            "EPOCH 6 : Validation Accuracy = 91.383%\n",
            "Epoch:  0007 cost= 0.053338525\n",
            "EPOCH 7 : Validation Accuracy = 90.635%\n",
            "Epoch:  0008 cost= 0.051431685\n",
            "EPOCH 8 : Validation Accuracy = 91.338%\n",
            "Epoch:  0009 cost= 0.039200420\n",
            "EPOCH 9 : Validation Accuracy = 91.973%\n",
            "Epoch:  0010 cost= 0.034143563\n",
            "EPOCH 10 : Validation Accuracy = 92.971%\n",
            "Epoch:  0011 cost= 0.032655930\n",
            "EPOCH 11 : Validation Accuracy = 91.406%\n",
            "Epoch:  0012 cost= 0.031156589\n",
            "EPOCH 12 : Validation Accuracy = 92.109%\n",
            "Epoch:  0013 cost= 0.022223867\n",
            "EPOCH 13 : Validation Accuracy = 92.698%\n",
            "Epoch:  0014 cost= 0.023301881\n",
            "EPOCH 14 : Validation Accuracy = 92.834%\n",
            "Epoch:  0015 cost= 0.024402578\n",
            "EPOCH 15 : Validation Accuracy = 90.862%\n",
            "Epoch:  0016 cost= 0.020941608\n",
            "EPOCH 16 : Validation Accuracy = 90.907%\n",
            "Epoch:  0017 cost= 0.019687291\n",
            "EPOCH 17 : Validation Accuracy = 91.361%\n",
            "Epoch:  0018 cost= 0.023893170\n",
            "EPOCH 18 : Validation Accuracy = 90.998%\n",
            "Epoch:  0019 cost= 0.019918802\n",
            "EPOCH 19 : Validation Accuracy = 91.383%\n",
            "Epoch:  0020 cost= 0.017176873\n",
            "EPOCH 20 : Validation Accuracy = 90.703%\n",
            "Epoch:  0021 cost= 0.020248161\n",
            "EPOCH 21 : Validation Accuracy = 92.630%\n",
            "Epoch:  0022 cost= 0.016965450\n",
            "EPOCH 22 : Validation Accuracy = 92.630%\n",
            "Epoch:  0023 cost= 0.014343798\n",
            "EPOCH 23 : Validation Accuracy = 93.084%\n",
            "Epoch:  0024 cost= 0.018179027\n",
            "EPOCH 24 : Validation Accuracy = 91.882%\n",
            "Epoch:  0025 cost= 0.015645056\n",
            "EPOCH 25 : Validation Accuracy = 92.018%\n",
            "Test Accuracy = 91.1%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ0AAAEWCAYAAAB8A8JQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2laXHD7-tmk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generator\n",
        "'''\n",
        "\tGenerator definition for AdvGAN\n",
        "\tref: https://arxiv.org/pdf/1801.02610.pdf\n",
        "'''\n",
        "\n",
        "#import tensorflow as tf\n",
        "#from keras import layers\n",
        "\n",
        "# helper function for convolution -> instance norm -> relu\n",
        "def ConvInstNormRelu(x, filters, kernel_size=3, strides=1):\n",
        "\tConv = tf.layers.conv2d(\n",
        "\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tInstNorm = tf.contrib.layers.instance_norm(Conv)\n",
        "\n",
        "\treturn tf.nn.relu(InstNorm)\n",
        "\n",
        "\n",
        "# helper function for trans convolution -> instance norm -> relu\n",
        "def TransConvInstNormRelu(x, filters, kernel_size=3, strides=2):\n",
        "\tTransConv = tf.layers.conv2d_transpose(\n",
        "\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tInstNorm = tf.contrib.layers.instance_norm(TransConv)\n",
        "\n",
        "\treturn tf.nn.relu(InstNorm)\n",
        "\n",
        "# helper function for residual block of 2 convolutions with same num filters\n",
        "# in the same style as ConvInstNormRelu\n",
        "def ResBlock(x, training, filters=32, kernel_size=3, strides=1):\n",
        "\tconv1 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tconv1_norm = tf.layers.batch_normalization(conv1, training=training)\n",
        "\n",
        "\tconv1_relu = tf.nn.relu(conv1_norm)\n",
        "\n",
        "\tconv2 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\tinputs=conv1_relu,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tconv2_norm = tf.layers.batch_normalization(conv2, training=training)\n",
        "\n",
        "\n",
        "\treturn x + conv2_norm\n",
        "\n",
        "\n",
        "def generator(x, training):\n",
        "\twith tf.variable_scope('g_weights', reuse=tf.AUTO_REUSE): #True\n",
        "\t\t# input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "\t\t# define first three conv + inst + relu layers\n",
        "\t\tc1 = ConvInstNormRelu(x, filters=8, kernel_size=3, strides=1)\n",
        "\t\td1 = ConvInstNormRelu(c1, filters=16, kernel_size=3, strides=2)\n",
        "\t\td2 = ConvInstNormRelu(d1, filters=32, kernel_size=3, strides=2)\n",
        "\n",
        "\t\t# define residual blocks\n",
        "\t\trb1 = ResBlock(d2, training, filters=32)\n",
        "\t\trb2 = ResBlock(rb1, training, filters=32)\n",
        "\t\trb3 = ResBlock(rb2, training, filters=32)\n",
        "\t\trb4 = ResBlock(rb3, training, filters=32)\n",
        "\n",
        "\t\t# upsample using conv transpose\n",
        "\t\tu1 = TransConvInstNormRelu(rb4, filters=16, kernel_size=3, strides=2)\n",
        "\t\tu2 = TransConvInstNormRelu(u1, filters=8, kernel_size=3, strides=2)\n",
        "\n",
        "\t\t# final layer block\n",
        "\t\tout = tf.layers.conv2d_transpose(\n",
        "\t\t\t\t\t\tinputs=u2,\n",
        "\t\t\t\t\t\tfilters=x.get_shape()[-1].value, # or 3 if RGB image\n",
        "\t\t\t\t\t\tkernel_size=3,\n",
        "\t\t\t\t\t\tstrides=1,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\t\t# out = tf.contrib.layers.instance_norm(out)\n",
        "\n",
        "\t\treturn tf.nn.tanh(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKYLa86WtyLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Discriminator\n",
        "'''\n",
        "\tDiscriminator definition for AdvGAN\n",
        "\tref: https://arxiv.org/pdf/1801.02610.pdf\n",
        "'''\n",
        "\n",
        "#import tensorflow as tf\n",
        "\n",
        "def discriminator(x, training):\n",
        "\twith tf.variable_scope('d_weights', reuse=tf.AUTO_REUSE):\n",
        "\t\t# input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "\t\tconv1 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\t\tfilters=8,\n",
        "\t\t\t\t\t\t\tkernel_size=4,\n",
        "\t\t\t\t\t\t\tstrides=2,\n",
        "\t\t\t\t\t\t\tpadding=\"valid\",\n",
        "\t\t\t\t\t\t\tactivation=None)\n",
        "\t\tconv1 = tf.nn.leaky_relu(conv1, alpha=0.2)\n",
        "\n",
        "\t\t\n",
        "\t\tconv2 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\t\tinputs=conv1,\n",
        "\t\t\t\t\t\t\tfilters=16,\n",
        "\t\t\t\t\t\t\tkernel_size=4,\n",
        "\t\t\t\t\t\t\tstrides=2,\n",
        "\t\t\t\t\t\t\tpadding=\"valid\",\n",
        "\t\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\t\tin1 = tf.contrib.layers.instance_norm(conv2)\n",
        "\t\tconv2 = tf.nn.leaky_relu(in1, alpha=0.2)\n",
        "\n",
        "\t\tconv3 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\t\tinputs=conv2,\n",
        "\t\t\t\t\t\t\tfilters=32,\n",
        "\t\t\t\t\t\t\tkernel_size=4,\n",
        "\t\t\t\t\t\t\tstrides=2,\n",
        "\t\t\t\t\t\t\tpadding=\"valid\",\n",
        "\t\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\t\t#in2 = tf.contrib.layers.instance_norm(conv3)\n",
        "\t\tin2 = tf.contrib.layers.instance_norm(conv3)\n",
        "\t\tconv3 = tf.nn.leaky_relu(in2, alpha=0.2)\n",
        "\t\tflat = tf.layers.flatten(conv3)\n",
        "\t\tlogits = tf.layers.dense(flat, 1)\n",
        "\n",
        "\t\tprobs = tf.nn.sigmoid(logits)\n",
        "\n",
        "\t\treturn logits, probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "288LCGWawCVl",
        "colab_type": "code",
        "outputId": "6b537aa4-6b39-4878-ca0f-0fbf74bfdb8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4927
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "#from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os, sys\n",
        "import random\n",
        "\n",
        "#make sure GAN_setup.py is in connected folder\n",
        "#from GAN_setup import generator, discriminator\n",
        "\n",
        "#ctargon created class Target and defined/trained his target model in there, then called here\n",
        "#import Target as target_model\n",
        "\n",
        "\n",
        "# get the next batch based on x, y, and the iteration (based on batch_size)\n",
        "def next_batch(X, Y, i, batch_size):\n",
        "    idx = i * batch_size\n",
        "    idx_n = i * batch_size + batch_size\n",
        "    return X[idx:idx_n], Y[idx:idx_n]\n",
        "\n",
        "# loss function to encourage misclassification after perturbation\n",
        "def adv_loss(preds, labels, is_targeted):\n",
        "    real = tf.reduce_sum(labels * preds, 1)\n",
        "    other = tf.reduce_max((1 - labels) * preds - (labels * 10000), 1)\n",
        "    if is_targeted:\n",
        "        return tf.reduce_sum(tf.maximum(0.0, other - real))\n",
        "    return tf.reduce_sum(tf.maximum(0.0, real - other))\n",
        "\n",
        "# loss function to influence the perturbation to be as close to 0 as possible\n",
        "def perturb_loss(preds, thresh=0.3):\n",
        "    zeros = tf.zeros((tf.shape(preds)[0]))\n",
        "    return tf.reduce_mean(tf.maximum(zeros, tf.norm(tf.reshape(preds, (tf.shape(preds)[0], -1)), axis=1) - thresh))\n",
        "\n",
        "\n",
        "# function that defines ops, graphs, and training procedure for AdvGAN framework\n",
        "def AdvGAN(X, y, X_test, y_test, epochs=50, batch_size=128, target=3):\n",
        "    #print(X_train.shape)\n",
        "    #print(y.shape[-1]) is num_images\n",
        "    print(\"y shape\")\n",
        "    print(y.shape)\n",
        "    print(\"y_test shape\")\n",
        "    print(y_test.shape)\n",
        "    \n",
        "    # placeholder definitions\n",
        "    x_pl = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]]) # image placeholder\n",
        "    t = tf.placeholder(tf.float32, [None, y.shape[-1]]) # target placeholder\n",
        "    print(\"t shape)\")\n",
        "    print(t.shape)\n",
        "    is_training = tf.placeholder(tf.bool, [])\n",
        "\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    # MODEL DEFINITIONS\n",
        "    is_targeted = False\n",
        "    if target in range(0, y.shape[-1]):\n",
        "        is_targeted = True\n",
        "\n",
        "    # gather target model\n",
        "    f = Target()\n",
        "    print(\"is targeted boolean\")\n",
        "    print(is_targeted)\n",
        "    \n",
        "    thresh = 0.3\n",
        "\n",
        "    # generate perturbation, add to original input image(s)\n",
        "    perturb = tf.clip_by_value(generator(x_pl, is_training), -thresh, thresh)\n",
        "    x_perturbed = perturb + x_pl\n",
        "    x_perturbed = tf.clip_by_value(x_perturbed, 0, 1)\n",
        "    print(x_perturbed.shape)\n",
        "\n",
        "    # pass real and perturbed image to discriminator and the target model\n",
        "    d_real_logits, d_real_probs = discriminator(x_pl, is_training)\n",
        "    d_fake_logits, d_fake_probs = discriminator(x_perturbed, is_training)\n",
        "    print(d_fake_probs.shape)#1\n",
        "    # pass real and perturbed images to the model we are trying to fool\n",
        "    f_real_logits, f_real_probs = f.Model(x_pl)\n",
        "    f_fake_logits, f_fake_probs = f.Model(x_perturbed)\n",
        "    print(f_fake_probs.shape) #43\n",
        "\n",
        "    # generate labels for discriminator (optionally smooth labels for stability)\n",
        "    smooth = 0.0\n",
        "    d_labels_real = tf.ones_like(d_real_probs) * (1 - smooth)\n",
        "    d_labels_fake = tf.zeros_like(d_fake_probs)\n",
        "\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    # LOSS DEFINITIONS\n",
        "    # discriminator loss\n",
        "    d_loss_real = tf.losses.mean_squared_error(predictions=d_real_probs, labels=d_labels_real)\n",
        "    d_loss_fake = tf.losses.mean_squared_error(predictions=d_fake_probs, labels=d_labels_fake)\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "    # generator loss\n",
        "    g_loss_fake = tf.losses.mean_squared_error(predictions=d_fake_probs, labels=tf.ones_like(d_fake_probs))\n",
        "\n",
        "    # perturbation loss (minimize overall perturbation)\n",
        "    l_perturb = perturb_loss(perturb, thresh)\n",
        "\n",
        "    # adversarial loss (encourage misclassification)\n",
        "    l_adv = adv_loss(f_fake_probs, t, is_targeted)\n",
        "\n",
        "    # weights for generator loss function\n",
        "    alpha = 1.0\n",
        "    beta = 5.0\n",
        "    g_loss = l_adv + alpha*g_loss_fake + beta*l_perturb \n",
        "\n",
        "    # ----------------------------------------------------------------------------------\n",
        "    # gather variables for training/restoring\n",
        "    t_vars = tf.trainable_variables()\n",
        "    f_vars = [var for var in t_vars if 'Model' in var.name]\n",
        "    d_vars = [var for var in t_vars if 'd_' in var.name]\n",
        "    g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='g_weights')\n",
        "\n",
        "    # define optimizers for discriminator and generator\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        d_opt = tf.train.AdamOptimizer().minimize(d_loss, var_list=d_vars)\n",
        "        g_opt = tf.train.AdamOptimizer(learning_rate=0.001).minimize(g_loss, var_list=g_vars)\n",
        "\n",
        "\t# create saver objects for the target model, generator, and discriminator\n",
        "    saver = tf.train.Saver(f_vars)\n",
        "    g_saver = tf.train.Saver(g_vars)\n",
        "    d_saver = tf.train.Saver(d_vars)\n",
        "\n",
        "    init  = tf.global_variables_initializer()\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    sess.run(init)\n",
        "    \n",
        "    # load the pretrained target model\n",
        "    #try:\n",
        "       # saver.restore(sess, \"./weights/target_model/model\")\n",
        "    #except:\n",
        "       # print(\"make sure to train the target model first...\")\n",
        "       # sys.exit(1)\n",
        "\n",
        "    \n",
        "    new_saver = tf.train.import_meta_graph('./weights/target_model/model.meta')\n",
        "    new_saver.restore(sess, tf.train.latest_checkpoint('./weights/target_model'))\n",
        "    #path_to_ckpt_data = './weights/target_model/model.data-00000-of-00001'\n",
        "    #new_saver.restore(sess, path_to_ckpt_data)\n",
        "    \n",
        "    print(\"Pretrained model loaded\")\n",
        "    \n",
        "    total_batches = int(X.shape[0] / batch_size)\n",
        "\n",
        "    for epoch in range(0, epochs):\n",
        "\n",
        "        loss_D_sum = 0.0\n",
        "        loss_G_fake_sum = 0.0\n",
        "        loss_perturb_sum = 0.0\n",
        "        loss_adv_sum = 0.0\n",
        "\n",
        "        for i in range(total_batches):\n",
        "\n",
        "            batch_x, batch_y = next_batch(X, y, i, batch_size)\n",
        "\n",
        "            # if targeted, create one hot vectors of the target\n",
        "            if is_targeted:\n",
        "                targets = np.full((batch_y.shape[0],), target)\n",
        "                batch_y = np.eye(y.shape[-1])[targets]\n",
        "\n",
        "            # train the discriminator first n times\n",
        "            for _ in range(1):\n",
        "                _, loss_D_batch = sess.run([d_opt, d_loss], feed_dict={x_pl: batch_x, \\\n",
        "                                           is_training: True})\n",
        "\n",
        "            #print(\"batch x\")\n",
        "            #print(batch_x.shape)\n",
        "            #print(\"batch y\")\n",
        "            #print(batch_y.shape)\n",
        "            \n",
        "\t\t\t       # train the generator n times\n",
        "            for _ in range(1):\n",
        "                \n",
        "                _, loss_G_fake_batch, loss_adv_batch, loss_perturb_batch = \\\n",
        "                        sess.run([g_opt, g_loss_fake, l_adv, l_perturb], \\\n",
        "                              feed_dict={x_pl: batch_x, \\\n",
        "                                     t: batch_y, \\\n",
        "                                     is_training: True})\n",
        "            loss_D_sum += loss_D_batch\n",
        "            loss_G_fake_sum += loss_G_fake_batch\n",
        "            loss_perturb_sum += loss_perturb_batch\n",
        "            loss_adv_sum += loss_adv_batch\n",
        "\n",
        "        print(\"epoch %d:\\nloss_D: %.3f, loss_G_fake: %.3f, \\\n",
        "            \\nloss_perturb: %.3f, loss_adv: %.3f, \\n\" %\n",
        "            (epoch + 1, loss_D_sum/total_batches, loss_G_fake_sum/total_batches,\n",
        "            loss_perturb_sum/total_batches, loss_adv_sum/total_batches))\n",
        "    #epoch_losses = np.array([loss_D_sum/totalbatches], [loss_G_fake_sum/total_batches], [loss_perturb_sum/totalbatches], [loss_adv_sum/total_batches])\n",
        "\t\t#np.savetxt(\"epoch_{}_losses.txt\".format(epoch), epoch_losses, delimiter=',')\n",
        "    \n",
        "        if epoch % 10 == 0:\n",
        "            g_saver.save(sess, \"weights/generator/gen\")\n",
        "            d_saver.save(sess, \"weights/discriminator/disc\")\n",
        "\n",
        "    # evaluate the test set\n",
        "    correct_prediction = tf.equal(tf.argmax(f_fake_probs, 1), tf.argmax(t, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    accs = []\n",
        "    total_batches_test = int(X_test.shape[0] / batch_size)\n",
        "    for i in range(total_batches_test):\n",
        "        batch_x, batch_y = next_batch(X_test, y_test, i, batch_size)\n",
        "        acc, x_pert = sess.run([accuracy, x_perturbed], feed_dict={x_pl: batch_x, t: batch_y, is_training: False})\n",
        "        accs.append(acc)\n",
        "\n",
        "    print('accuracy of test set: {}'.format(sum(accs) / len(accs)))\n",
        "  #test_acc = np.array(sum(accs)/len(accs))\n",
        "  #np.savetxt(\"test_accuracy_GD.txt\", test_acc, delimiter=',')\n",
        "\n",
        "\t# plot some images and their perturbed counterparts\n",
        "    f, axarr = plt.subplots(2,2)\n",
        "    axarr[0,0].imshow(np.squeeze(batch_x[2]), cmap='Greys_r')\n",
        "    axarr[0,1].imshow(np.squeeze(x_pert[2]), cmap='Greys_r')\n",
        "    axarr[1,0].imshow(np.squeeze(batch_x[5]), cmap='Greys_r')\n",
        "    axarr[1,1].imshow(np.squeeze(x_pert[5]), cmap='Greys_r')\n",
        "    plt.show()\n",
        "\n",
        "    print('finished training, saving weights')\n",
        "    g_saver.save(sess, \"weights/generator/gen\")\n",
        "    d_saver.save(sess, \"weights/discriminator/disc\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def attack(X, y, batch_size=128, thresh=0.3, target=3):\n",
        "    x_pl = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]]) # image placeholder\n",
        "    t = tf.placeholder(tf.float32, [None, y.shape[-1]]) # target placeholder\n",
        "    is_training = tf.placeholder(tf.bool, [])\n",
        "\n",
        "    is_targeted = False\n",
        "    if target in range(0, y.shape[-1]):\n",
        "        is_targeted = True\n",
        "\n",
        "    perturb = tf.clip_by_value(generator(x_pl, is_training), -thresh, thresh)\n",
        "    x_perturbed = perturb + x_pl\n",
        "    x_perturbed = tf.clip_by_value(x_perturbed, 0, 1)\n",
        "\n",
        "    a = Target()\n",
        "    a_real_logits, a_real_probs = a.Model(x_pl)\n",
        "    a_fake_logits, a_fake_probs = a.Model(x_perturbed)\n",
        "\n",
        "    #t_vars = tf.trainable_variables()\n",
        "    #a_vars = [var for var in t_vars if 'Model' in var.name]\n",
        "    g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='g_weights')\n",
        "\n",
        "    init  = tf.global_variables_initializer()\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    sess.run(init)    \n",
        "    \n",
        "    #just using generator and target model\n",
        "    new_saver2 = tf.train.import_meta_graph('./weights/target_model/model.meta')\n",
        "    new_saver2.restore(sess, tf.train.latest_checkpoint('./weights/target_model'))\n",
        "    \n",
        "    #f_saver2 = tf.train.Saver(a_vars)\n",
        "    #g_saver = tf.train.Saver(g_vars)\n",
        "    #f_saver2.restore(sess, \"./weights/target_model/model\")\n",
        "    g_saver = tf.train.import_meta_graph('./weights/generator/gen.meta')\n",
        "    g_saver.restore(sess, tf.train.latest_checkpoint(\"./weights/generator\"))\n",
        "\n",
        "    rawpert, pert, fake_l, real_l = sess.run([perturb, x_perturbed, a_fake_probs, a_real_probs], \\\n",
        "                          feed_dict={x_pl: X[:32], \\\n",
        "                                 is_training: False})\n",
        "    print('LA: ' + str(np.argmax(y[:32], axis=1)))\n",
        "    print('OG: ' + str(np.argmax(real_l, axis=1)))\n",
        "    print('PB: ' + str(np.argmax(fake_l, axis=1)))\n",
        "\n",
        "    correct_prediction = tf.equal(tf.argmax(a_fake_probs, 1), tf.argmax(t, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    accs = []\n",
        "    total_batches_test = int(X.shape[0] / batch_size)\n",
        "    for i in range(total_batches_test):\n",
        "        batch_x, batch_y = next_batch(X, y, i, batch_size)\n",
        "\n",
        "        if is_targeted:\n",
        "            targets = np.full((batch_y.shape[0],), target)\n",
        "            batch_y = np.eye(y.shape[-1])[targets]\n",
        "\n",
        "        acc, fake_l, x_pert = sess.run([accuracy, a_fake_probs, x_perturbed], feed_dict={x_pl: batch_x, t: batch_y, is_training: False})\n",
        "        accs.append(acc)\n",
        "\n",
        "    print('accuracy of test set: {}'.format(sum(accs) / len(accs)))\n",
        "\n",
        "    f, axarr = plt.subplots(2,2)\n",
        "    axarr[0,0].imshow(np.squeeze(X[3]), cmap='Greys_r')\n",
        "    axarr[0,1].imshow(np.squeeze(pert[3]), cmap='Greys_r')\n",
        "    axarr[1,0].imshow(np.squeeze(X[4]), cmap='Greys_r')\n",
        "    axarr[1,1].imshow(np.squeeze(pert[4]), cmap='Greys_r')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#main code\n",
        "Y_train = to_categorical(Y_train, num_classes=43)\n",
        "Y_test = to_categorical(Y_test, num_classes=43)\n",
        "\n",
        "AdvGAN(X_train, Y_train, X_test, Y_test, batch_size=128, epochs=50, target=3)\n",
        "\n",
        "attack(X_test, Y_test, target=3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y shape\n",
            "(34799, 43)\n",
            "y_test shape\n",
            "(12630, 43)\n",
            "t shape)\n",
            "(?, 43)\n",
            "is targeted boolean\n",
            "True\n",
            "WARNING:tensorflow:From <ipython-input-3-5caa55e74d60>:17: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-3-5caa55e74d60>:49: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n",
            "WARNING:tensorflow:From <ipython-input-3-5caa55e74d60>:32: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d_transpose instead.\n",
            "(?, 32, 32, 1)\n",
            "WARNING:tensorflow:From <ipython-input-4-85d97f4f431c>:44: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-4-85d97f4f431c>:45: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "(?, 1)\n",
            "(?, 43)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./weights/target_model/model\n",
            "Pretrained model loaded\n",
            "epoch 1:\n",
            "loss_D: 0.053, loss_G_fake: 0.923,             \n",
            "loss_perturb: 1.760, loss_adv: 7.464, \n",
            "\n",
            "epoch 2:\n",
            "loss_D: 0.020, loss_G_fake: 0.968,             \n",
            "loss_perturb: 0.168, loss_adv: 7.782, \n",
            "\n",
            "epoch 3:\n",
            "loss_D: 0.015, loss_G_fake: 0.987,             \n",
            "loss_perturb: 0.072, loss_adv: 7.786, \n",
            "\n",
            "epoch 4:\n",
            "loss_D: 0.005, loss_G_fake: 0.993,             \n",
            "loss_perturb: 0.008, loss_adv: 7.660, \n",
            "\n",
            "epoch 5:\n",
            "loss_D: 0.004, loss_G_fake: 0.996,             \n",
            "loss_perturb: 0.009, loss_adv: 7.558, \n",
            "\n",
            "epoch 6:\n",
            "loss_D: 0.002, loss_G_fake: 0.995,             \n",
            "loss_perturb: 0.004, loss_adv: 7.467, \n",
            "\n",
            "epoch 7:\n",
            "loss_D: 0.002, loss_G_fake: 0.997,             \n",
            "loss_perturb: 0.006, loss_adv: 7.442, \n",
            "\n",
            "epoch 8:\n",
            "loss_D: 0.002, loss_G_fake: 0.997,             \n",
            "loss_perturb: 0.006, loss_adv: 7.446, \n",
            "\n",
            "epoch 9:\n",
            "loss_D: 0.002, loss_G_fake: 0.998,             \n",
            "loss_perturb: 0.005, loss_adv: 7.496, \n",
            "\n",
            "epoch 10:\n",
            "loss_D: 0.364, loss_G_fake: 0.546,             \n",
            "loss_perturb: 0.006, loss_adv: 7.794, \n",
            "\n",
            "epoch 11:\n",
            "loss_D: 0.502, loss_G_fake: 0.251,             \n",
            "loss_perturb: 0.000, loss_adv: 7.782, \n",
            "\n",
            "epoch 12:\n",
            "loss_D: 0.501, loss_G_fake: 0.251,             \n",
            "loss_perturb: 0.001, loss_adv: 7.783, \n",
            "\n",
            "epoch 13:\n",
            "loss_D: 0.500, loss_G_fake: 0.250,             \n",
            "loss_perturb: 0.000, loss_adv: 7.781, \n",
            "\n",
            "epoch 14:\n",
            "loss_D: 0.500, loss_G_fake: 0.250,             \n",
            "loss_perturb: 0.001, loss_adv: 7.779, \n",
            "\n",
            "epoch 15:\n",
            "loss_D: 0.500, loss_G_fake: 0.250,             \n",
            "loss_perturb: 0.001, loss_adv: 7.778, \n",
            "\n",
            "epoch 16:\n",
            "loss_D: 0.500, loss_G_fake: 0.251,             \n",
            "loss_perturb: 0.004, loss_adv: 7.776, \n",
            "\n",
            "epoch 17:\n",
            "loss_D: 0.499, loss_G_fake: 0.251,             \n",
            "loss_perturb: 0.123, loss_adv: 7.712, \n",
            "\n",
            "epoch 18:\n",
            "loss_D: 0.280, loss_G_fake: 0.542,             \n",
            "loss_perturb: 6.020, loss_adv: 4.968, \n",
            "\n",
            "epoch 19:\n",
            "loss_D: 0.146, loss_G_fake: 0.760,             \n",
            "loss_perturb: 6.686, loss_adv: 4.683, \n",
            "\n",
            "epoch 20:\n",
            "loss_D: 0.249, loss_G_fake: 0.646,             \n",
            "loss_perturb: 3.406, loss_adv: 6.046, \n",
            "\n",
            "epoch 21:\n",
            "loss_D: 0.260, loss_G_fake: 0.620,             \n",
            "loss_perturb: 2.865, loss_adv: 6.302, \n",
            "\n",
            "epoch 22:\n",
            "loss_D: 0.258, loss_G_fake: 0.622,             \n",
            "loss_perturb: 2.701, loss_adv: 6.383, \n",
            "\n",
            "epoch 23:\n",
            "loss_D: 0.252, loss_G_fake: 0.628,             \n",
            "loss_perturb: 2.607, loss_adv: 6.429, \n",
            "\n",
            "epoch 24:\n",
            "loss_D: 0.249, loss_G_fake: 0.633,             \n",
            "loss_perturb: 2.550, loss_adv: 6.456, \n",
            "\n",
            "epoch 25:\n",
            "loss_D: 0.242, loss_G_fake: 0.643,             \n",
            "loss_perturb: 2.532, loss_adv: 6.465, \n",
            "\n",
            "epoch 26:\n",
            "loss_D: 0.242, loss_G_fake: 0.645,             \n",
            "loss_perturb: 2.441, loss_adv: 6.511, \n",
            "\n",
            "epoch 27:\n",
            "loss_D: 0.238, loss_G_fake: 0.651,             \n",
            "loss_perturb: 2.409, loss_adv: 6.527, \n",
            "\n",
            "epoch 28:\n",
            "loss_D: 0.236, loss_G_fake: 0.654,             \n",
            "loss_perturb: 2.357, loss_adv: 6.553, \n",
            "\n",
            "epoch 29:\n",
            "loss_D: 0.234, loss_G_fake: 0.656,             \n",
            "loss_perturb: 2.315, loss_adv: 6.574, \n",
            "\n",
            "epoch 30:\n",
            "loss_D: 0.232, loss_G_fake: 0.660,             \n",
            "loss_perturb: 2.292, loss_adv: 6.585, \n",
            "\n",
            "epoch 31:\n",
            "loss_D: 0.232, loss_G_fake: 0.662,             \n",
            "loss_perturb: 2.240, loss_adv: 6.611, \n",
            "\n",
            "epoch 32:\n",
            "loss_D: 0.230, loss_G_fake: 0.665,             \n",
            "loss_perturb: 2.230, loss_adv: 6.616, \n",
            "\n",
            "epoch 33:\n",
            "loss_D: 0.230, loss_G_fake: 0.665,             \n",
            "loss_perturb: 2.178, loss_adv: 6.642, \n",
            "\n",
            "epoch 34:\n",
            "loss_D: 0.229, loss_G_fake: 0.666,             \n",
            "loss_perturb: 2.155, loss_adv: 6.654, \n",
            "\n",
            "epoch 35:\n",
            "loss_D: 0.228, loss_G_fake: 0.671,             \n",
            "loss_perturb: 2.161, loss_adv: 6.651, \n",
            "\n",
            "epoch 36:\n",
            "loss_D: 0.242, loss_G_fake: 0.653,             \n",
            "loss_perturb: 1.965, loss_adv: 6.751, \n",
            "\n",
            "epoch 37:\n",
            "loss_D: 0.253, loss_G_fake: 0.639,             \n",
            "loss_perturb: 1.820, loss_adv: 6.825, \n",
            "\n",
            "epoch 38:\n",
            "loss_D: 0.253, loss_G_fake: 0.639,             \n",
            "loss_perturb: 1.789, loss_adv: 6.841, \n",
            "\n",
            "epoch 39:\n",
            "loss_D: 0.258, loss_G_fake: 0.634,             \n",
            "loss_perturb: 1.727, loss_adv: 6.873, \n",
            "\n",
            "epoch 40:\n",
            "loss_D: 0.254, loss_G_fake: 0.636,             \n",
            "loss_perturb: 1.721, loss_adv: 6.876, \n",
            "\n",
            "epoch 41:\n",
            "loss_D: 0.258, loss_G_fake: 0.633,             \n",
            "loss_perturb: 1.693, loss_adv: 6.891, \n",
            "\n",
            "epoch 42:\n",
            "loss_D: 0.256, loss_G_fake: 0.635,             \n",
            "loss_perturb: 1.677, loss_adv: 6.899, \n",
            "\n",
            "epoch 43:\n",
            "loss_D: 0.257, loss_G_fake: 0.633,             \n",
            "loss_perturb: 1.649, loss_adv: 6.913, \n",
            "\n",
            "epoch 44:\n",
            "loss_D: 0.255, loss_G_fake: 0.635,             \n",
            "loss_perturb: 1.645, loss_adv: 6.915, \n",
            "\n",
            "epoch 45:\n",
            "loss_D: 0.256, loss_G_fake: 0.635,             \n",
            "loss_perturb: 1.622, loss_adv: 6.927, \n",
            "\n",
            "epoch 46:\n",
            "loss_D: 0.255, loss_G_fake: 0.636,             \n",
            "loss_perturb: 1.610, loss_adv: 6.933, \n",
            "\n",
            "epoch 47:\n",
            "loss_D: 0.257, loss_G_fake: 0.634,             \n",
            "loss_perturb: 1.585, loss_adv: 6.947, \n",
            "\n",
            "epoch 48:\n",
            "loss_D: 0.258, loss_G_fake: 0.634,             \n",
            "loss_perturb: 1.567, loss_adv: 6.956, \n",
            "\n",
            "epoch 49:\n",
            "loss_D: 0.258, loss_G_fake: 0.633,             \n",
            "loss_perturb: 1.546, loss_adv: 6.967, \n",
            "\n",
            "epoch 50:\n",
            "loss_D: 0.259, loss_G_fake: 0.631,             \n",
            "loss_perturb: 1.519, loss_adv: 6.981, \n",
            "\n",
            "accuracy of test set: 0.016980229591836735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAD8CAYAAADub8g7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "finished training, saving weights\n",
            "INFO:tensorflow:Restoring parameters from ./weights/target_model/model\n",
            "INFO:tensorflow:Restoring parameters from ./weights/generator/gen\n",
            "LA: [16  1 38 33 11 38 18 12 25 35 12  7 23  7  4  9 21 20 27 38  4 33  9  3\n",
            "  1 11 13 10  9 11  5 17]\n",
            "OG: [ 7 18  7 18  7  7  7  7 18 18  7 18  7  7  7 18  7  7  7  7 18  8  7  7\n",
            " 18 33 18 18  7  7 18 18]\n",
            "PB: [ 3  3  3  3  3  3  3  3  3  3  3  3 29  3 20 29  3  3  3  3 29  3 20  3\n",
            " 20  3  3  3  3  3  3  3]\n",
            "accuracy of test set: 0.8395248724489796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAD8CAYAAADub8g7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCSGiQwOfECI6NPAJIaJDA58QIjr+H1/rzm4VY+OFAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_fullGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tortoisehare/TSR-GAN/blob/master/keras_fullGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABnslutLuWVE",
        "colab_type": "code",
        "outputId": "f8fa4e7d-003e-4a89-d502-44cf43f2fa44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "### DOES NOT WORK YET - KERAS CANNOT HANDLE SYMBOLIC TENSORS FROM TF.PLACEHOLDER\n",
        "\n",
        "!mkdir weights\n",
        "!mkdir weights/target_model\n",
        "!mkdir weights/generator\n",
        "!mkdir weights/discriminator\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘weights’: File exists\n",
            "mkdir: cannot create directory ‘weights/target_model’: File exists\n",
            "mkdir: cannot create directory ‘weights/generator’: File exists\n",
            "mkdir: cannot create directory ‘weights/discriminator’: File exists\n",
            "sample_data  test.p  train.p  valid.p  weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGOxsi0OYfas",
        "colab_type": "code",
        "outputId": "2587e217-a65f-448c-b4ae-c2d77a368c89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1511
        }
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(1187) #to help reproduce\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import csv\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import keras\n",
        "#from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "    \n",
        "def create_model():\n",
        "    #hyperparameters\n",
        "    mu = 0\n",
        "    sigma = 0.1\n",
        "    n_out = 43\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    #Start model\n",
        "    model = Sequential()\n",
        "\n",
        "    # LAYER 1 - Conv: Input 32x32x1, Output 28x28x6\n",
        "    model.add(Conv2D(filters=6, \n",
        "                 kernel_size=5, \n",
        "                 strides=1, \n",
        "                 activation='relu', \n",
        "                 input_shape=(32,32,1)))\n",
        "\n",
        "\n",
        "    # POOLING 1: Input = 28x28x6. Output = 14x14x6.\n",
        "    #conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "    model.add(Dropout(0.2))\n",
        "    # LAYER 2 - CONV: Input 14x14x6, Output 10x10x16\n",
        "    model.add(Conv2D(filters=16,\n",
        "                kernel_size=5,\n",
        "                strides=1,\n",
        "                activation='relu',\n",
        "                input_shape=(14,14,6)))\n",
        "\n",
        "\n",
        "    # POOLING 2: Input 10x10x16, Output 5x5x16\n",
        "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # FLATTEN\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # LAYER 3: FC Input 400, Output 120\n",
        "    model.add(Dense(units=120, activation='relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # LAYER 4: FC Input 120, Output 84\n",
        "    model.add(Dense(units=84, activation='relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # LAYER 5: SOFTMAX Input 84, Output 43\n",
        "    model.add(Dense(units=43, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "      \n",
        "import pickle\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    training_file = 'train.p'\n",
        "    testing_file = 'test.p'\n",
        "    validation_file = 'valid.p'\n",
        "\n",
        "    with open(training_file, mode='rb') as f:\n",
        "        tstrain = pickle.load(f)\n",
        "    with open(testing_file, mode='rb') as f:\n",
        "        tstest = pickle.load(f)\n",
        "    with open(validation_file, mode='rb') as f:\n",
        "        tsvalid = pickle.load(f)\n",
        "\n",
        "    X_train, Y_train = tstrain['features'], tstrain['labels']\n",
        "    X_valid, Y_valid = tsvalid['features'], tsvalid['labels']\n",
        "    X_test, Y_test = tstest['features'], tstest['labels']\n",
        "\n",
        "    #shuffle training set\n",
        "    X_train, Y_train = shuffle(X_train, Y_train, random_state=33)\n",
        "    X_test, Y_test = shuffle(X_test, Y_test, random_state=33)\n",
        "    X_valid, Y_valid = shuffle(X_valid, Y_valid, random_state=33)\n",
        "\n",
        "    #grayscale images\n",
        "    grayscale = [0.299,0.587,0.144]\n",
        "\n",
        "    X_test = np.dot(X_test, grayscale)\n",
        "    X_train = np.dot(X_train, grayscale)\n",
        "    X_valid = np.dot(X_valid, grayscale)\n",
        "\n",
        "\n",
        "    #normalize\n",
        "    X_train = np.array(X_train)/255\n",
        "    X_test = np.array(X_test)/255\n",
        "    X_valid = np.array(X_valid)/255\n",
        "\n",
        "    #expand dimensions to fit 4D input array\n",
        "    X_train = np.expand_dims(X_train,-1)\n",
        "    X_test = np.expand_dims(X_test,-1)\n",
        "    X_valid = np.expand_dims(X_valid,-1)\n",
        "\n",
        "    assert(len(X_train)==len(Y_train))\n",
        "    n_train = len(X_train)\n",
        "    assert(len(X_test)==len(Y_test))\n",
        "    n_test = len(X_test)\n",
        "    \n",
        "    Y_train = to_categorical(Y_train, num_classes=43)\n",
        "    Y_test = to_categorical(Y_test, num_classes=43)\n",
        "    Y_valid = to_categorical(Y_valid, num_classes=43)\n",
        "    \n",
        "    checkpoint_path = \"weights/target_model/model.ckpt\"\n",
        "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "    \n",
        "    cp_callback = keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
        "                                     save_weights_only=True, \n",
        "                                     verbose=1)\n",
        "    model = create_model()\n",
        "    model.summary\n",
        "    history = model.fit(X_train, Y_train, epochs=10, batch_size=20, \n",
        "              validation_data = (X_valid, Y_valid), \n",
        "              callbacks=[cp_callback])\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "    loss, acc = model.evaluate(X_test, Y_test)\n",
        "    print(\"Test accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 34799 samples, validate on 4410 samples\n",
            "Epoch 1/10\n",
            "34799/34799 [==============================] - 11s 324us/step - loss: 2.4806 - acc: 0.3094 - val_loss: 1.0817 - val_acc: 0.6937\n",
            "\n",
            "Epoch 00001: saving model to weights/target_model/model.ckpt\n",
            "Epoch 2/10\n",
            "34799/34799 [==============================] - 9s 254us/step - loss: 1.0217 - acc: 0.6774 - val_loss: 0.5037 - val_acc: 0.8522\n",
            "\n",
            "Epoch 00002: saving model to weights/target_model/model.ckpt\n",
            "Epoch 3/10\n",
            "34799/34799 [==============================] - 9s 252us/step - loss: 0.6990 - acc: 0.7817 - val_loss: 0.3087 - val_acc: 0.9145\n",
            "\n",
            "Epoch 00003: saving model to weights/target_model/model.ckpt\n",
            "Epoch 4/10\n",
            "34799/34799 [==============================] - 9s 252us/step - loss: 0.5668 - acc: 0.8229 - val_loss: 0.2562 - val_acc: 0.9252\n",
            "\n",
            "Epoch 00004: saving model to weights/target_model/model.ckpt\n",
            "Epoch 5/10\n",
            "34799/34799 [==============================] - 9s 252us/step - loss: 0.4873 - acc: 0.8492 - val_loss: 0.2565 - val_acc: 0.9306\n",
            "\n",
            "Epoch 00005: saving model to weights/target_model/model.ckpt\n",
            "Epoch 6/10\n",
            "34799/34799 [==============================] - 9s 251us/step - loss: 0.4327 - acc: 0.8668 - val_loss: 0.2212 - val_acc: 0.9365\n",
            "\n",
            "Epoch 00006: saving model to weights/target_model/model.ckpt\n",
            "Epoch 7/10\n",
            "34799/34799 [==============================] - 9s 258us/step - loss: 0.4084 - acc: 0.8759 - val_loss: 0.1953 - val_acc: 0.9447\n",
            "\n",
            "Epoch 00007: saving model to weights/target_model/model.ckpt\n",
            "Epoch 8/10\n",
            "34799/34799 [==============================] - 9s 252us/step - loss: 0.3822 - acc: 0.8822 - val_loss: 0.2151 - val_acc: 0.9374\n",
            "\n",
            "Epoch 00008: saving model to weights/target_model/model.ckpt\n",
            "Epoch 9/10\n",
            "34799/34799 [==============================] - 9s 252us/step - loss: 0.3616 - acc: 0.8904 - val_loss: 0.2146 - val_acc: 0.9363\n",
            "\n",
            "Epoch 00009: saving model to weights/target_model/model.ckpt\n",
            "Epoch 10/10\n",
            "34799/34799 [==============================] - 9s 252us/step - loss: 0.3450 - acc: 0.8955 - val_loss: 0.1963 - val_acc: 0.9435\n",
            "\n",
            "Epoch 00010: saving model to weights/target_model/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYUnBGGNMhCUFY4wxEZYUjDHGRPx/dDQ6F6Aj1LIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "12630/12630 [==============================] - 1s 50us/step\n",
            "Test accuracy: 93.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33TQxVDWdoHH",
        "colab_type": "code",
        "outputId": "39c03ac0-0ebc-43a2-a790-f97d6c41c75c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "modeltest = create_model()\n",
        "\n",
        "#Check untrained first\n",
        "loss, acc = modeltest.evaluate(X_test, Y_test)\n",
        "print(\"Untrained model, accuracy: {:5.2f}%\".format(100*acc))\n",
        "\n",
        "#Now restore model and check accuracy\n",
        "modeltest.load_weights(checkpoint_path)\n",
        "loss,acc = modeltest.evaluate(X_test, Y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12630/12630 [==============================] - 1s 59us/step\n",
            "Untrained model, accuracy:  1.51%\n",
            "12630/12630 [==============================] - 1s 48us/step\n",
            "Restored model, accuracy: 93.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2laXHD7-tmk0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6354b94e-4f30-44f7-aab1-6d559e999f4d"
      },
      "source": [
        "\n",
        "#Generator\n",
        "'''\n",
        "\tGenerator definition for AdvGAN\n",
        "\tref: https://arxiv.org/pdf/1801.02610.pdf\n",
        "'''\n",
        "'''\n",
        "\n",
        "#helper function if we use residual blocks\n",
        "class ResBlock(Layer):\n",
        "\tdef __init__(self, channels_in, kernel, **kwargs):\n",
        "        super(Residual, self).__init__(**kwargs)\n",
        "        self.channels_in = channels_in\n",
        "        self.kernel = kernel\n",
        "\n",
        "    def call(self, x):\n",
        "        # the residual block using Keras functional API\n",
        "        first_layer =   Activation(\"linear\", trainable=False)(x)\n",
        "        x =             Conv2D( self.channels_in,\n",
        "                                self.kernel,\n",
        "                                padding=\"same\")(first_layer)\n",
        "        x = BatchNormalization()(x)\n",
        "        x =             Activation(\"relu\")(x)\n",
        "        x =             Conv2D( self.channels_in,\n",
        "                                self.kernel,\n",
        "                                padding=\"same\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        residual =      Add()([x, first_layer])\n",
        "        x =             Activation(\"relu\")(residual)\n",
        "        return x\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "    \n",
        "\n",
        "def create_generator():\n",
        "    \n",
        "    generator = Sequential()\n",
        "    generator.add(Conv2D(filters=32, kernel_size=3, strides=1))\n",
        "    generator.add(LeakyRelu(0.2))\n",
        "    generator.add(InstanceNormalization())\n",
        "\n",
        "    generator.add(Conv2D(filters=64, kernel_size=3, strides=2))\n",
        "    generator.add(LeakyRelu(0.2))\n",
        "    generator.add(InstanceNormalization())\n",
        "\n",
        "    generator.add(Conv2D(filters=128, kernel_size=3, strides=2))\n",
        "    generator.add(LeakyRelu(0.2))\n",
        "    generator.add(InstanceNormalization())\n",
        "\n",
        "    #generator.add(ResBlock(128, (3,3)))\n",
        "    #generator.add(ResBlock(128, (3,3)))\n",
        "    #generator.add(ResBlock(128, (3,3)))\n",
        "    #generator.add(ResBlock(128, (3,3)))\n",
        "\n",
        "    generator.add(Conv2DTranspose(filters=64, kernel_size=3, strides=2))\n",
        "    generator.add(InstanceNormalization())\n",
        "    generator.add(LeakyRelu(0.2))\n",
        "    generator.add(Conv2DTranspose(filters=32, kernel_size=3, strides=2))\n",
        "    generator.add(InstanceNormalization())\n",
        "    generator.add(LeakyRelu(0.2))\n",
        "    \n",
        "    generator.add(Conv2DTranspose(filters=1, kernel_size=3, \n",
        "                                  strides=1, activation='tanh'))\n",
        "    \n",
        "    generator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    \n",
        "    return generator\n",
        "\n",
        "gtest = create_generator()\n",
        "gtest.summary()\n",
        "'''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n#helper function if we use residual blocks\\nclass ResBlock(Layer):\\n\\tdef __init__(self, channels_in, kernel, **kwargs):\\n        super(Residual, self).__init__(**kwargs)\\n        self.channels_in = channels_in\\n        self.kernel = kernel\\n\\n    def call(self, x):\\n        # the residual block using Keras functional API\\n        first_layer =   Activation(\"linear\", trainable=False)(x)\\n        x =             Conv2D( self.channels_in,\\n                                self.kernel,\\n                                padding=\"same\")(first_layer)\\n        x = BatchNormalization()(x)\\n        x =             Activation(\"relu\")(x)\\n        x =             Conv2D( self.channels_in,\\n                                self.kernel,\\n                                padding=\"same\")(x)\\n        x = BatchNormalization()(x)\\n        residual =      Add()([x, first_layer])\\n        x =             Activation(\"relu\")(residual)\\n        return x\\n\\n    def compute_output_shape(self, input_shape):\\n        return input_shape\\n    \\n\\ndef create_generator():\\n    \\n    generator = Sequential()\\n    generator.add(Conv2D(filters=32, kernel_size=3, strides=1))\\n    generator.add(LeakyRelu(0.2))\\n    generator.add(InstanceNormalization())\\n\\n    generator.add(Conv2D(filters=64, kernel_size=3, strides=2))\\n    generator.add(LeakyRelu(0.2))\\n    generator.add(InstanceNormalization())\\n\\n    generator.add(Conv2D(filters=128, kernel_size=3, strides=2))\\n    generator.add(LeakyRelu(0.2))\\n    generator.add(InstanceNormalization())\\n\\n    #generator.add(ResBlock(128, (3,3)))\\n    #generator.add(ResBlock(128, (3,3)))\\n    #generator.add(ResBlock(128, (3,3)))\\n    #generator.add(ResBlock(128, (3,3)))\\n\\n    generator.add(Conv2DTranspose(filters=64, kernel_size=3, strides=2))\\n    generator.add(InstanceNormalization())\\n    generator.add(LeakyRelu(0.2))\\n    generator.add(Conv2DTranspose(filters=32, kernel_size=3, strides=2))\\n    generator.add(InstanceNormalization())\\n    generator.add(LeakyRelu(0.2))\\n    \\n    generator.add(Conv2DTranspose(filters=1, kernel_size=3, \\n                                  strides=1, activation=\\'tanh\\'))\\n    \\n    generator.compile(loss=\\'binary_crossentropy\\', optimizer=\\'adam\\')\\n    \\n    return generator\\n\\ngtest = create_generator()\\ngtest.summary()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKYLa86WtyLc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c54863f7-4c63-412c-c0d9-05af2659b97f"
      },
      "source": [
        "#Discriminator\n",
        "'''\n",
        "\tDiscriminator definition for AdvGAN\n",
        "\tref: https://arxiv.org/pdf/1801.02610.pdf\n",
        "'''\n",
        "'''\n",
        "#import tensorflow as tf\n",
        "\n",
        "\n",
        "def create_discriminator():\n",
        "    discriminator = Sequential()\n",
        "    discriminator.add(Conv2D(filters=8, kernel_size=4, \n",
        "                             strides=2, padding='valid'))\n",
        "    discriminator.add(LeakyRelu(0.2))\n",
        "    \n",
        "    discriminator.add(Conv2D(filters=16, kernel_size=4, \n",
        "                             strides=2, padding='valid'))\n",
        "    discriminator.add(InstanceNormalization())\n",
        "    discriminator.add(LeakyRely(0.2))\n",
        "    discriminator.add(Conv2D(filters=32, kernel_size=4, \n",
        "                             strides=2, padding='valid'))\n",
        "    discriminator.add(InstanceNormalization())\n",
        "    discriminator.add(LeakyRely(0.2))\n",
        "    discriminator.add(Flatten())\n",
        "    discriminator.add(Dense(units=1, activation='sigmoid'))\n",
        "    \n",
        "    return discriminator\n",
        "\n",
        "dtest = create_discriminator()\n",
        "dtest.summary()\n",
        "'''"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#import tensorflow as tf\\n\\n\\ndef create_discriminator():\\n    discriminator = Sequential()\\n    discriminator.add(Conv2D(filters=8, kernel_size=4, \\n                             strides=2, padding='valid'))\\n    discriminator.add(LeakyRelu(0.2))\\n    \\n    discriminator.add(Conv2D(filters=16, kernel_size=4, \\n                             strides=2, padding='valid'))\\n    discriminator.add(InstanceNormalization())\\n    discriminator.add(LeakyRely(0.2))\\n    discriminator.add(Conv2D(filters=32, kernel_size=4, \\n                             strides=2, padding='valid'))\\n    discriminator.add(InstanceNormalization())\\n    discriminator.add(LeakyRely(0.2))\\n    discriminator.add(Flatten())\\n    discriminator.add(Dense(units=1, activation='sigmoid'))\\n    \\n    return discriminator\\n\\ndtest = create_discriminator()\\ndtest.summary()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knlTbiFS1tkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generator\n",
        "'''\n",
        "\tGenerator definition for AdvGAN\n",
        "\tref: https://arxiv.org/pdf/1801.02610.pdf\n",
        "'''\n",
        "\n",
        "#import tensorflow as tf\n",
        "#from keras import layers\n",
        "\n",
        "# helper function for convolution -> instance norm -> relu\n",
        "def ConvInstNormRelu(x, filters, kernel_size=3, strides=1):\n",
        "\tConv = tf.layers.conv2d(\n",
        "\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tInstNorm = tf.contrib.layers.instance_norm(Conv)\n",
        "\n",
        "\treturn tf.nn.relu(InstNorm)\n",
        "\n",
        "\n",
        "# helper function for trans convolution -> instance norm -> relu\n",
        "def TransConvInstNormRelu(x, filters, kernel_size=3, strides=2):\n",
        "\tTransConv = tf.layers.conv2d_transpose(\n",
        "\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tInstNorm = tf.contrib.layers.instance_norm(TransConv)\n",
        "\n",
        "\treturn tf.nn.relu(InstNorm)\n",
        "\n",
        "# helper function for residual block of 2 convolutions with same num filters\n",
        "# in the same style as ConvInstNormRelu\n",
        "def ResBlock(x, training, filters=32, kernel_size=3, strides=1):\n",
        "\tconv1 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tconv1_norm = tf.layers.batch_normalization(conv1, training=training)\n",
        "\n",
        "\tconv1_relu = tf.nn.relu(conv1_norm)\n",
        "\n",
        "\tconv2 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\tinputs=conv1_relu,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tconv2_norm = tf.layers.batch_normalization(conv2, training=training)\n",
        "\n",
        "\n",
        "\treturn x + conv2_norm\n",
        "\n",
        "\n",
        "def generator(x, training):\n",
        "\twith tf.variable_scope('g_weights', reuse=tf.AUTO_REUSE): #True\n",
        "\t\t# input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
        "        \n",
        "        #6/3 increased all filters 4x\n",
        "\t\t# define first three conv + inst + relu layers\n",
        "\t\tc1 = ConvInstNormRelu(x, filters=32, kernel_size=3, strides=1)\n",
        "\t\td1 = ConvInstNormRelu(c1, filters=64, kernel_size=3, strides=2)\n",
        "\t\td2 = ConvInstNormRelu(d1, filters=128, kernel_size=3, strides=2)\n",
        "        \n",
        "        #then stride\n",
        "\t\t# define residual blocks\n",
        "\t\trb1 = ResBlock(d2, training, filters=128)\n",
        "\t\trb2 = ResBlock(rb1, training, filters=128)\n",
        "\t\trb3 = ResBlock(rb2, training, filters=128)\n",
        "\t\trb4 = ResBlock(rb3, training, filters=128)\n",
        "\n",
        "\t\t# upsample using conv transpose\n",
        "\t\tu1 = TransConvInstNormRelu(rb4, filters=64, kernel_size=3, strides=2)\n",
        "\t\tu2 = TransConvInstNormRelu(u1, filters=32, kernel_size=3, strides=2)\n",
        "\n",
        "\t\t# final layer block\n",
        "\t\tout = tf.layers.conv2d_transpose(\n",
        "\t\t\t\t\t\tinputs=u2,\n",
        "\t\t\t\t\t\tfilters=x.get_shape()[-1].value, # or 3 if RGB image\n",
        "\t\t\t\t\t\tkernel_size=3,\n",
        "\t\t\t\t\t\tstrides=1,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\t\t# out = tf.contrib.layers.instance_norm(out)\n",
        "\n",
        "\t\treturn tf.nn.tanh(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jq0H_yB1uXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Discriminator\n",
        "'''\n",
        "\tDiscriminator definition for AdvGAN\n",
        "\tref: https://arxiv.org/pdf/1801.02610.pdf\n",
        "'''\n",
        "\n",
        "#import tensorflow as tf\n",
        "\n",
        "def discriminator(x, training):\n",
        "\twith tf.variable_scope('d_weights', reuse=tf.AUTO_REUSE):\n",
        "\t\t# input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "\t\tconv1 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\t\tfilters=8,\n",
        "\t\t\t\t\t\t\tkernel_size=4,\n",
        "\t\t\t\t\t\t\tstrides=2,\n",
        "\t\t\t\t\t\t\tpadding=\"valid\",\n",
        "\t\t\t\t\t\t\tactivation=None)\n",
        "\t\tconv1 = tf.nn.leaky_relu(conv1, alpha=0.2)\n",
        "\n",
        "\t\t\n",
        "\t\tconv2 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\t\tinputs=conv1,\n",
        "\t\t\t\t\t\t\tfilters=16,\n",
        "\t\t\t\t\t\t\tkernel_size=4,\n",
        "\t\t\t\t\t\t\tstrides=2,\n",
        "\t\t\t\t\t\t\tpadding=\"valid\",\n",
        "\t\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\t\tin1 = tf.contrib.layers.instance_norm(conv2)\n",
        "\t\tconv2 = tf.nn.leaky_relu(in1, alpha=0.2)\n",
        "\n",
        "\t\tconv3 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\t\tinputs=conv2,\n",
        "\t\t\t\t\t\t\tfilters=32,\n",
        "\t\t\t\t\t\t\tkernel_size=4,\n",
        "\t\t\t\t\t\t\tstrides=2,\n",
        "\t\t\t\t\t\t\tpadding=\"valid\",\n",
        "\t\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\t\t#in2 = tf.contrib.layers.instance_norm(conv3)\n",
        "\t\tin2 = tf.contrib.layers.instance_norm(conv3)\n",
        "\t\tconv3 = tf.nn.leaky_relu(in2, alpha=0.2)\n",
        "\t\tflat = tf.layers.flatten(conv3)\n",
        "\t\tlogits = tf.layers.dense(flat, 1)\n",
        "\n",
        "\t\tprobs = tf.nn.sigmoid(logits)\n",
        "\n",
        "\t\treturn logits, probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "288LCGWawCVl",
        "colab_type": "code",
        "outputId": "61018751-79f4-40d7-9a03-fd3a7e5b42b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1007
        }
      },
      "source": [
        "\n",
        "\n",
        "# get the next batch based on x, y, and the iteration (based on batch_size)\n",
        "def next_batch(X, Y, i, batch_size):\n",
        "    idx = i * batch_size\n",
        "    idx_n = i * batch_size + batch_size\n",
        "    return X[idx:idx_n], Y[idx:idx_n]\n",
        "\n",
        "# loss function to encourage misclassification after perturbation\n",
        "def adv_loss(preds, labels, is_targeted):\n",
        "    real = tf.reduce_sum(labels * preds, 1)\n",
        "    other = tf.reduce_max((1 - labels) * preds - (labels * 10000), 1)\n",
        "    if is_targeted:\n",
        "        return tf.reduce_sum(tf.maximum(0.0, other - real))\n",
        "    return tf.reduce_sum(tf.maximum(0.0, real - other))\n",
        "\n",
        "# loss function to influence the perturbation to be as close to 0 as possible\n",
        "def perturb_loss(preds, thresh=0.3):\n",
        "    zeros = tf.zeros((tf.shape(preds)[0]))\n",
        "    return tf.reduce_mean(tf.maximum(zeros, tf.norm(tf.reshape(preds, (tf.shape(preds)[0], -1)), axis=1) - thresh))\n",
        "\n",
        "\n",
        "# function that defines ops, graphs, and training procedure for AdvGAN framework\n",
        "def AdvGAN(X, y, X_test, y_test, epochs=50, batch_size=128, target=3):\n",
        "    #print(X_train.shape)\n",
        "    #print(y.shape[-1]) is num_images\n",
        "    print(\"y shape\")\n",
        "    print(y.shape)\n",
        "    print(\"y_test shape\")\n",
        "    print(y_test.shape)\n",
        "    \n",
        "    # placeholder definitions\n",
        "    x_pl = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]]) # image placeholder\n",
        "    t = tf.placeholder(tf.float32, [None, y.shape[-1]]) # target placeholder\n",
        "    print(\"t shape)\")\n",
        "    print(t.shape)\n",
        "    is_training = tf.placeholder(tf.bool, [])\n",
        "\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    # MODEL DEFINITIONS\n",
        "    is_targeted = False\n",
        "    if target in range(0, y.shape[-1]):\n",
        "        is_targeted = True\n",
        "\n",
        "    #tf.reset_default_graph()\n",
        "    \n",
        "    # gather target model\n",
        "    target_model = create_model()\n",
        "    print(\"is targeted boolean\")\n",
        "    print(is_targeted)\n",
        "    \n",
        "    #test accuracy of restored model\n",
        "    target_model.load_weights(checkpoint_path)\n",
        "    loss,acc = target_model.evaluate(X_test, y_test)\n",
        "    print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n",
        "    \n",
        "    thresh = 0.3\n",
        "\n",
        "    # generate perturbation, add to original input image(s)\n",
        "    perturb = tf.clip_by_value(generator(x_pl, is_training), -thresh, thresh)\n",
        "    x_perturbed = perturb + x_pl\n",
        "    x_perturbed = tf.clip_by_value(x_perturbed, 0, 1)\n",
        "    print(x_perturbed.shape)\n",
        "\n",
        "    # pass real and perturbed image to discriminator and the target model\n",
        "    d_real_logits, d_real_probs = discriminator(x_pl, is_training)\n",
        "    d_fake_logits, d_fake_probs = discriminator(x_perturbed, is_training)\n",
        "    print(d_fake_probs.shape)#1\n",
        "    \n",
        "    # pass real and perturbed images to the model we are trying to fool\n",
        "    f_real_probs = target_model.predict(X)\n",
        "    loss2,acc2 = target_model.evaluate(X, y)\n",
        "    print(\"x_pl model, accuracy: {:5.2f}%\".format(100*acc2))\n",
        "\n",
        "    \n",
        "    f_fake_probs = target_model.predict(x_perturbed)\n",
        "    \n",
        "\n",
        "    #f_real_logits, f_real_probs = f.ModelC(x_pl)\n",
        "    #f_fake_logits, f_fake_probs = f.ModelC(x_perturbed)\n",
        "    #print(f_fake_probs.shape) #43\n",
        "\n",
        "    # generate labels for discriminator (optionally smooth labels for stability)\n",
        "    smooth = 0.0\n",
        "    d_labels_real = tf.ones_like(d_real_probs) * (1 - smooth)\n",
        "    d_labels_fake = tf.zeros_like(d_fake_probs)\n",
        "\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    # LOSS DEFINITIONS\n",
        "    # discriminator loss\n",
        "    d_loss_real = tf.losses.mean_squared_error(predictions=d_real_probs, labels=d_labels_real)\n",
        "    d_loss_fake = tf.losses.mean_squared_error(predictions=d_fake_probs, labels=d_labels_fake)\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "    # generator loss\n",
        "    g_loss_fake = tf.losses.mean_squared_error(predictions=d_fake_probs, labels=tf.ones_like(d_fake_probs))\n",
        "\n",
        "    # perturbation loss (minimize overall perturbation)\n",
        "    l_perturb = perturb_loss(perturb, thresh)\n",
        "\n",
        "    # adversarial loss (encourage misclassification)\n",
        "    l_adv = adv_loss(f_fake_probs, t, is_targeted)\n",
        "\n",
        "    # weights for generator loss function\n",
        "    alpha = 1.0\n",
        "    beta = 5.0\n",
        "    g_loss = l_adv + alpha*g_loss_fake + beta*l_perturb \n",
        "\n",
        "    # ----------------------------------------------------------------------------------\n",
        "    # gather variables for training/restoring\n",
        "    t_vars = tf.trainable_variables()\n",
        "    #f_vars = [var for var in t_vars if 'ModelC_' in var.name]\n",
        "    d_vars = [var for var in t_vars if 'd_' in var.name]\n",
        "    g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='g_weights')\n",
        "\n",
        "    # define optimizers for discriminator and generator\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        d_opt = tf.train.AdamOptimizer().minimize(d_loss, var_list=d_vars)\n",
        "        g_opt = tf.train.AdamOptimizer(learning_rate=0.001).minimize(g_loss, var_list=g_vars)\n",
        "\n",
        "\t# create saver objects for the target model, generator, and discriminator\n",
        "    #saver = tf.train.Saver(f_vars)\n",
        "    g_saver = tf.train.Saver(g_vars)\n",
        "    d_saver = tf.train.Saver(d_vars)\n",
        "  \n",
        "    init  = tf.global_variables_initializer()\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    sess.run(init)\n",
        "\n",
        "    total_batches = int(X.shape[0] / batch_size)\n",
        "\n",
        "    for epoch in range(0, epochs):\n",
        "\n",
        "        loss_D_sum = 0.0\n",
        "        loss_G_fake_sum = 0.0\n",
        "        loss_perturb_sum = 0.0\n",
        "        loss_adv_sum = 0.0\n",
        "\n",
        "        for i in range(total_batches):\n",
        "\n",
        "            batch_x, batch_y = next_batch(X, y, i, batch_size)\n",
        "\n",
        "            # if targeted, create one hot vectors of the target\n",
        "            if is_targeted:\n",
        "                targets = np.full((batch_y.shape[0],), target)\n",
        "                batch_y = np.eye(y.shape[-1])[targets]\n",
        "\n",
        "            # train the discriminator first 1 time\n",
        "            for _ in range(1):\n",
        "                _, loss_D_batch = sess.run([d_opt, d_loss], feed_dict={x_pl: batch_x, \\\n",
        "                                           is_training: True})\n",
        "\n",
        "            \n",
        "\t\t\t# train the generator 1 time (maybe try 10 times (1:10 d:g))\n",
        "            for _ in range(1):\n",
        "                \n",
        "                _, loss_G_fake_batch, loss_adv_batch, loss_perturb_batch = sess.run([g_opt, g_loss_fake, l_adv, l_perturb], \n",
        "                                                feed_dict={x_pl: batch_x, t: batch_y, \n",
        "                                                           is_training: True})\n",
        "\n",
        "            loss_D_sum += loss_D_batch\n",
        "            loss_G_fake_sum += loss_G_fake_batch\n",
        "            loss_perturb_sum += loss_perturb_batch\n",
        "            loss_adv_sum += loss_adv_batch\n",
        "\n",
        "        print(\"epoch %d:\\nloss_D: %.3f, loss_G_fake: %.3f, \\\n",
        "            \\nloss_perturb: %.3f, loss_adv: %.3f, \\n\" %\n",
        "            (epoch + 1, loss_D_sum/total_batches, loss_G_fake_sum/total_batches,\n",
        "            loss_perturb_sum/total_batches, loss_adv_sum/total_batches))\n",
        "    \n",
        "        if epoch % 10 == 0:\n",
        "            g_saver.save(sess, \"weights/generator/gen\")\n",
        "            d_saver.save(sess, \"weights/discriminator/disc\")\n",
        "\n",
        "    # evaluate the test set\n",
        "    correct_prediction = tf.equal(tf.argmax(f_fake_probs, 1), tf.argmax(t, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    accs = []\n",
        "    total_batches_test = int(X_test.shape[0] / batch_size)\n",
        "    for i in range(total_batches_test):\n",
        "        batch_x, batch_y = next_batch(X_test, y_test, i, batch_size)\n",
        "        acc, x_pert = sess.run([accuracy, x_perturbed], feed_dict={x_pl: batch_x, t: batch_y, is_training: False})\n",
        "        accs.append(acc)\n",
        "\n",
        "    print('accuracy of test set: {}'.format(sum(accs) / len(accs)))\n",
        "\n",
        "\t# plot some images and their perturbed counterparts\n",
        "    f, axarr = plt.subplots(2,2)\n",
        "    axarr[0,0].imshow(np.squeeze(batch_x[2]), cmap='Greys_r')\n",
        "    axarr[0,1].imshow(np.squeeze(x_pert[2]), cmap='Greys_r')\n",
        "    axarr[1,0].imshow(np.squeeze(batch_x[5]), cmap='Greys_r')\n",
        "    axarr[1,1].imshow(np.squeeze(x_pert[5]), cmap='Greys_r')\n",
        "    plt.show()\n",
        "\n",
        "    print('finished training, saving weights')\n",
        "    g_saver.save(sess, \"weights/generator/gen\")\n",
        "    d_saver.save(sess, \"weights/discriminator/disc\")\n",
        "\n",
        "\n",
        "\n",
        "def attack(X, y, batch_size=128, thresh=0.3, target=3):\n",
        "    x_pl = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]]) # image placeholder\n",
        "    t = tf.placeholder(tf.float32, [None, y.shape[-1]]) # target placeholder\n",
        "    is_training = tf.placeholder(tf.bool, [])\n",
        "\n",
        "    is_targeted = False\n",
        "    if target in range(0, y.shape[-1]):\n",
        "        is_targeted = True\n",
        "\n",
        "    perturb = tf.clip_by_value(generator(x_pl, is_training), -thresh, thresh)\n",
        "    x_perturbed = perturb + x_pl\n",
        "    x_perturbed = tf.clip_by_value(x_perturbed, 0, 1)\n",
        "\n",
        "    a = Target()\n",
        "    a_real_logits, a_real_probs = a.ModelC(x_pl)\n",
        "    a_fake_logits, a_fake_probs = a.ModelC(x_perturbed)\n",
        "\n",
        "    #t_vars = tf.trainable_variables()\n",
        "    #a_vars = [var for var in t_vars if 'ModelC' in var.name]\n",
        "    g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='g_weights')\n",
        "\n",
        "    init  = tf.global_variables_initializer()\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    sess.run(init)    \n",
        "    \n",
        "    #just using generator and target model\n",
        "    new_saver2 = tf.train.import_meta_graph('./weights/target_model/model.ckpt.meta')\n",
        "    new_saver2.restore(sess, tf.train.latest_checkpoint('./weights/target_model'))\n",
        "    \n",
        "    #f_saver2 = tf.train.Saver(a_vars)\n",
        "    #g_saver = tf.train.Saver(g_vars)\n",
        "    #f_saver2.restore(sess, \"./weights/target_model/model\")\n",
        "    g_saver = tf.train.import_meta_graph('./weights/generator/gen.meta')\n",
        "    g_saver.restore(sess, tf.train.latest_checkpoint(\"./weights/generator\"))\n",
        "\n",
        "    rawpert, pert, fake_l, real_l = sess.run([perturb, x_perturbed, a_fake_probs, a_real_probs], \\\n",
        "                          feed_dict={x_pl: X[:32], \\\n",
        "                                 is_training: False})\n",
        "    \n",
        "    #changed the way the author named these printed lists - please verify that I interpreted his acronyms properly\n",
        "    print('actual labels: ' + str(np.argmax(y[:32], axis=1)))\n",
        "    print('classifier original prediction: ' + str(np.argmax(real_l, axis=1))) \n",
        "    print('classifier prediction after perturbation: ' + str(np.argmax(fake_l, axis=1)))\n",
        "\n",
        "    correct_prediction = tf.equal(tf.argmax(a_fake_probs, 1), tf.argmax(t, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    accs = []\n",
        "    total_batches_test = int(X.shape[0] / batch_size)\n",
        "    for i in range(total_batches_test):\n",
        "        batch_x, batch_y = next_batch(X, y, i, batch_size)\n",
        "\n",
        "        if is_targeted:\n",
        "            targets = np.full((batch_y.shape[0],), target)\n",
        "            batch_y = np.eye(y.shape[-1])[targets]\n",
        "\n",
        "        acc, fake_l, x_pert = sess.run([accuracy, a_fake_probs, x_perturbed], feed_dict={x_pl: batch_x, t: batch_y, is_training: False})\n",
        "        accs.append(acc)\n",
        "\n",
        "    print('accuracy of test set: {}'.format(sum(accs) / len(accs)))\n",
        "\n",
        "    f, axarr = plt.subplots(2,2)\n",
        "    axarr[0,0].imshow(np.squeeze(X[3]), cmap='Greys_r')\n",
        "    axarr[0,1].imshow(np.squeeze(pert[3]), cmap='Greys_r')\n",
        "    axarr[1,0].imshow(np.squeeze(X[4]), cmap='Greys_r')\n",
        "    axarr[1,1].imshow(np.squeeze(pert[4]), cmap='Greys_r')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#main code\n",
        "#Y_train = to_categorical(Y_train, num_classes=43)\n",
        "#Y_test = to_categorical(Y_test, num_classes=43)\n",
        "#Y_valid = to_categorical(Y_valid, num_classes=43)\n",
        "\n",
        "AdvGAN(X_train, Y_train, X_test, Y_test, batch_size=128, epochs=50, target=3)\n",
        "\n",
        "#attack(X_valid, Y_valid, target=3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y shape\n",
            "(34799, 43)\n",
            "y_test shape\n",
            "(12630, 43)\n",
            "t shape)\n",
            "(?, 43)\n",
            "is targeted boolean\n",
            "True\n",
            "12630/12630 [==============================] - 1s 54us/step\n",
            "Restored model, accuracy: 93.67%\n",
            "WARNING:tensorflow:From <ipython-input-6-e3c56d682c97>:17: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-6-e3c56d682c97>:49: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-e3c56d682c97>:32: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d_transpose instead.\n",
            "(?, 32, 32, 1)\n",
            "WARNING:tensorflow:From <ipython-input-7-85d97f4f431c>:44: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-7-85d97f4f431c>:45: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "(?, 1)\n",
            "34799/34799 [==============================] - 2s 50us/step\n",
            "x_pl model, accuracy: 98.69%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6784d5caeb56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;31m#Y_valid = to_categorical(Y_valid, num_classes=43)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m \u001b[0mAdvGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;31m#attack(X_valid, Y_valid, target=3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-6784d5caeb56>\u001b[0m in \u001b[0;36mAdvGAN\u001b[0;34m(X, y, X_test, y_test, epochs, batch_size, target)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mf_fake_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_perturbed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;34m'When feeding symbolic tensors to a model, we expect the'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;34m'tensors to have a static batch size. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 'Got tensor with shape: %s' % str(shape))\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: When feeding symbolic tensors to a model, we expect thetensors to have a static batch size. Got tensor with shape: (None, 32, 32, 1)"
          ]
        }
      ]
    }
  ]
}
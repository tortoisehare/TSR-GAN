{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_fullGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tortoisehare/TSR-GAN/blob/master/keras_fullGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABnslutLuWVE",
        "colab_type": "code",
        "outputId": "62888a6b-a745-4bba-ad12-912dd49188de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!mkdir weights\n",
        "!mkdir weights/target_model\n",
        "!mkdir weights/generator\n",
        "!mkdir weights/discriminator\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘weights’: File exists\n",
            "mkdir: cannot create directory ‘weights/target_model’: File exists\n",
            "mkdir: cannot create directory ‘weights/generator’: File exists\n",
            "mkdir: cannot create directory ‘weights/discriminator’: File exists\n",
            "sample_data  test.p  train.p  valid.p  weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGOxsi0OYfas",
        "colab_type": "code",
        "outputId": "61943db1-9c29-4482-bd04-4256c015f11e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1511
        }
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(1187) #to help reproduce\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import csv\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "    \n",
        "def create_model():\n",
        "    #hyperparameters\n",
        "    mu = 0\n",
        "    sigma = 0.1\n",
        "    n_out = 43\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    #Start model\n",
        "    model = Sequential()\n",
        "\n",
        "    # LAYER 1 - Conv: Input 32x32x1, Output 28x28x6\n",
        "    model.add(Conv2D(filters=6, \n",
        "                 kernel_size=5, \n",
        "                 strides=1, \n",
        "                 activation='relu', \n",
        "                 input_shape=(32,32,1)))\n",
        "\n",
        "\n",
        "    # POOLING 1: Input = 28x28x6. Output = 14x14x6.\n",
        "    #conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "    model.add(Dropout(0.2))\n",
        "    # LAYER 2 - CONV: Input 14x14x6, Output 10x10x16\n",
        "    model.add(Conv2D(filters=16,\n",
        "                kernel_size=5,\n",
        "                strides=1,\n",
        "                activation='relu',\n",
        "                input_shape=(14,14,6)))\n",
        "\n",
        "\n",
        "    # POOLING 2: Input 10x10x16, Output 5x5x16\n",
        "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # FLATTEN\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # LAYER 3: FC Input 400, Output 120\n",
        "    model.add(Dense(units=120, activation='relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # LAYER 4: FC Input 120, Output 84\n",
        "    model.add(Dense(units=84, activation='relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # LAYER 5: SOFTMAX Input 84, Output 43\n",
        "    model.add(Dense(units=43, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "      \n",
        "import pickle\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    training_file = 'train.p'\n",
        "    testing_file = 'test.p'\n",
        "    validation_file = 'valid.p'\n",
        "\n",
        "    with open(training_file, mode='rb') as f:\n",
        "        tstrain = pickle.load(f)\n",
        "    with open(testing_file, mode='rb') as f:\n",
        "        tstest = pickle.load(f)\n",
        "    with open(validation_file, mode='rb') as f:\n",
        "        tsvalid = pickle.load(f)\n",
        "\n",
        "    X_train, Y_train = tstrain['features'], tstrain['labels']\n",
        "    X_valid, Y_valid = tsvalid['features'], tsvalid['labels']\n",
        "    X_test, Y_test = tstest['features'], tstest['labels']\n",
        "\n",
        "    #shuffle training set\n",
        "    X_train, Y_train = shuffle(X_train, Y_train, random_state=33)\n",
        "    X_test, Y_test = shuffle(X_test, Y_test, random_state=33)\n",
        "    X_valid, Y_valid = shuffle(X_valid, Y_valid, random_state=33)\n",
        "\n",
        "    #grayscale images\n",
        "    grayscale = [0.299,0.587,0.144]\n",
        "\n",
        "    X_test = np.dot(X_test, grayscale)\n",
        "    X_train = np.dot(X_train, grayscale)\n",
        "    X_valid = np.dot(X_valid, grayscale)\n",
        "\n",
        "\n",
        "    #normalize\n",
        "    X_train = np.array(X_train)/255\n",
        "    X_test = np.array(X_test)/255\n",
        "    X_valid = np.array(X_valid)/255\n",
        "\n",
        "    #expand dimensions to fit 4D input array\n",
        "    X_train = np.expand_dims(X_train,-1)\n",
        "    X_test = np.expand_dims(X_test,-1)\n",
        "    X_valid = np.expand_dims(X_valid,-1)\n",
        "\n",
        "    assert(len(X_train)==len(Y_train))\n",
        "    n_train = len(X_train)\n",
        "    assert(len(X_test)==len(Y_test))\n",
        "    n_test = len(X_test)\n",
        "    \n",
        "    Y_train = to_categorical(Y_train, num_classes=43)\n",
        "    Y_test = to_categorical(Y_test, num_classes=43)\n",
        "    Y_valid = to_categorical(Y_valid, num_classes=43)\n",
        "    \n",
        "    checkpoint_path = \"weights/target_model/model.ckpt\"\n",
        "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "    \n",
        "    cp_callback = keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
        "                                     save_weights_only=True, \n",
        "                                     verbose=1)\n",
        "    model = create_model()\n",
        "    model.summary\n",
        "    history = model.fit(X_train, Y_train, epochs=10, batch_size=20, \n",
        "              validation_data = (X_valid, Y_valid), \n",
        "              callbacks=[cp_callback])\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "    loss, acc = model.evaluate(X_test, Y_test)\n",
        "    print(\"Test accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 34799 samples, validate on 4410 samples\n",
            "Epoch 1/10\n",
            "34799/34799 [==============================] - 11s 315us/step - loss: 2.4455 - acc: 0.3193 - val_loss: 1.0015 - val_acc: 0.7152\n",
            "\n",
            "Epoch 00001: saving model to weights/target_model/model.ckpt\n",
            "Epoch 2/10\n",
            "34799/34799 [==============================] - 9s 246us/step - loss: 0.9553 - acc: 0.6987 - val_loss: 0.4434 - val_acc: 0.8698\n",
            "\n",
            "Epoch 00002: saving model to weights/target_model/model.ckpt\n",
            "Epoch 3/10\n",
            "34799/34799 [==============================] - 8s 243us/step - loss: 0.6520 - acc: 0.7954 - val_loss: 0.3039 - val_acc: 0.9127\n",
            "\n",
            "Epoch 00003: saving model to weights/target_model/model.ckpt\n",
            "Epoch 4/10\n",
            "34799/34799 [==============================] - 8s 240us/step - loss: 0.5309 - acc: 0.8351 - val_loss: 0.2195 - val_acc: 0.9413\n",
            "\n",
            "Epoch 00004: saving model to weights/target_model/model.ckpt\n",
            "Epoch 5/10\n",
            "34799/34799 [==============================] - 8s 241us/step - loss: 0.4562 - acc: 0.8605 - val_loss: 0.1982 - val_acc: 0.9472\n",
            "\n",
            "Epoch 00005: saving model to weights/target_model/model.ckpt\n",
            "Epoch 6/10\n",
            "34799/34799 [==============================] - 9s 245us/step - loss: 0.4094 - acc: 0.8743 - val_loss: 0.1827 - val_acc: 0.9456\n",
            "\n",
            "Epoch 00006: saving model to weights/target_model/model.ckpt\n",
            "Epoch 7/10\n",
            "34799/34799 [==============================] - 8s 243us/step - loss: 0.3807 - acc: 0.8866 - val_loss: 0.1871 - val_acc: 0.9485\n",
            "\n",
            "Epoch 00007: saving model to weights/target_model/model.ckpt\n",
            "Epoch 8/10\n",
            "34799/34799 [==============================] - 8s 242us/step - loss: 0.3529 - acc: 0.8951 - val_loss: 0.1799 - val_acc: 0.9420\n",
            "\n",
            "Epoch 00008: saving model to weights/target_model/model.ckpt\n",
            "Epoch 9/10\n",
            "34799/34799 [==============================] - 9s 245us/step - loss: 0.3414 - acc: 0.8999 - val_loss: 0.1572 - val_acc: 0.9515\n",
            "\n",
            "Epoch 00009: saving model to weights/target_model/model.ckpt\n",
            "Epoch 10/10\n",
            "34799/34799 [==============================] - 9s 247us/step - loss: 0.3204 - acc: 0.9045 - val_loss: 0.1511 - val_acc: 0.9587\n",
            "\n",
            "Epoch 00010: saving model to weights/target_model/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8XVW9///XJydzm6lJ5ymlLZRS\nhkIpk0KRQSZBEQWuqCDYLyrDFSf0x0UF773o1+Gr0Hu9CCg4gMog1QtW1JRBQNrSMrQ0tJSWplPS\nNEnbzMn5/P7YJ+lpmuG0zelOct7Px+M8zp7Ozienzfrsvdbaa5m7IyIiApAWdgAiIjJwKCmIiEgn\nJQUREemkpCAiIp2UFEREpJOSgoiIdFJSkJRgZqVm5maWnsCxV5vZC4ciLpGBRklBBhwzW29mLWZW\n0mX78ljBXhpOZCJDn5KCDFTvAld2rJjZ0UBueOEMDInc6YgcDCUFGah+CXwqbv3TwEPxB5hZgZk9\nZGZVZrbBzG4zs7TYvoiZfd/MtpvZOuDCbj57v5ltMbNNZvYdM4skEpiZ/d7MtppZnZk9Z2ZHxe3L\nMbMfxOKpM7MXzCwntu99ZvaimdWa2UYzuzq2fbGZXRd3jr2qr2J3R18wszXAmti2H8fOsdPMlpnZ\n++OOj5jZN8zsHTPbFds/0cwWmNkPuvwuC83si4n83pIalBRkoHoZyDezI2OF9RXAr7occzdQABwG\nnEGQRK6J7fsscBEwG5gDXNbls78A2oBpsWPOBa4jMU8D04FRwKvAr+P2fR84ATgVGAF8FYia2eTY\n5+4GRgLHASsS/HkAHwZOAmbG1pfEzjEC+A3wezPLju27heAu6wIgH/gM0AA8CFwZlzhLgLNjnxcJ\nuLteeg2oF7CeoLC6DfhP4DzgGSAdcKAUiAAtwMy4z/0fYHFs+e/A9XH7zo19Nh0YDTQDOXH7rwTK\nYstXAy8kGGth7LwFBBdZjcCx3Rz3deCJHs6xGLgubn2vnx87/wf6iKOm4+cC5cAlPRz3FnBObPkG\n4Kmw/731Glgv1U/KQPZL4DlgCl2qjoASIAPYELdtAzA+tjwO2NhlX4fJsc9uMbOObWldju9W7K7l\n34GPEVzxR+PiyQKygXe6+ejEHrYnaq/YzOzLwLUEv6cT3BF0NMz39rMeBK4iSLJXAT8+iJhkCFL1\nkQxY7r6BoMH5AuDxLru3A60EBXyHScCm2PIWgsIxfl+HjQR3CiXuXhh75bv7UfTtX4BLCO5kCgju\nWgAsFlMTMLWbz23sYTtAPXs3oo/p5pjO4Yxj7QdfBT4OFLl7IVAXi6Gvn/Ur4BIzOxY4EvhDD8dJ\nilJSkIHuWoKqk/r4je7eDvwO+Hczy4vV2d/CnnaH3wE3mdkEMysCbo377BbgL8APzCzfzNLMbKqZ\nnZFAPHkECaWaoCD/j7jzRoEHgB+a2bhYg+8pZpZF0O5wtpl93MzSzazYzI6LfXQFcKmZ5ZrZtNjv\n3FcMbUAVkG5mtxPcKXS4D7jTzKZb4BgzK47FWEHQHvFL4DF3b0zgd5YUoqQgA5q7v+PuS3vYfSPB\nVfY64AWCBtMHYvt+BiwCXiNoDO56p/EpIBNYRVAf/ygwNoGQHiKoitoU++zLXfZ/GXiDoODdAXwX\nSHP39wjueL4U274CODb2mR8RtI9sI6je+TW9WwT8GXg7FksTe1cv/ZAgKf4F2AncD+TE7X8QOJog\nMYjsxdw1yY5IKjGz0wnuqCa7CgDpQncKIinEzDKAm4H7lBCkO0oKIinCzI4Eagmqyf5fyOHIAKXq\nIxER6aQ7BRER6TToHl4rKSnx0tLSsMMQERlUli1btt3dR/Z13KBLCqWlpSxd2lMPRRER6Y6Zbej7\nKFUfiYhIHCUFERHppKQgIiKdBl2bQndaW1upqKigqakp7FAOmezsbCZMmEBGRkbYoYjIEDIkkkJF\nRQV5eXmUlpYSNxTykOXuVFdXU1FRwZQpU8IOR0SGkCFRfdTU1ERxcXFKJAQAM6O4uDil7oxE5NAY\nEkkBSJmE0CHVfl8ROTSGRPWRiMig5w6tjdBUC421e7831QXLh38Qxh+f1DCUFPpBdXU1Z511FgBb\nt24lEokwcmTw4OArr7xCZmZmn+e45ppruPXWWzniiCOSGquIJJE7tDbsW6gn+t7e0vv5h49SUhgM\niouLWbFiBQDf+ta3GD58OF/+8pf3OqZjUuy0tO5r7H7+858nPU4RSZB7UEjXb4f6KmjYkXjhHm3t\n5cQG2fmQXQg5hcF7/ti913t6zy6AtEjSf3UlhSRau3YtF198MbNnz2b58uU888wzfPvb3+bVV1+l\nsbGRyy+/nNtvvx2A973vfdxzzz3MmjWLkpISrr/+ep5++mlyc3N58sknGTVqVMi/jQxa0XZoa4b2\nZmhr6fLeDO2t+26DoPDKKggKo+z84D09GwZre1ZbS1DA11ftKezrK/es767ce1+PhbsF30V8oZ0/\nvu9CPacw+D57uDAcKIZcUvj2H1eyavPOfj3nzHH5fPNDiczpvq/Vq1fz0EMPMWfOHADuuusuRowY\nQVtbG2eeeSaXXXYZM2fO3OszdXV1nHHGGdx1113ccsstPPDAA9x6663dnV4gKNSad+39atkNzTu7\nbO9mW8vuPe8YpKUHV2Px7xa/3rEce1la3HqX/Rbp/nxpkbh9Ha+0Pedraw6qEdpbuhTeLfu+d7et\nawLwaP9912kZexJEVn7cclzyyMrfO5F0HlcYLEf6qdhxD+ra67fHFe7dFfCx5aa67s8TyQqqZYaN\nhLyxMOYYGD4yWB82EoaVQM6IPYV7Vv6AL9gPxpBLCgPN1KlTOxMCwMMPP8z9999PW1sbmzdvZtWq\nVfskhZycHM4//3wATjjhBJ5//vlDGvMh1VgDu6ugZde+BXt3r+4K+7YEu+ZmDoesvD3vWXlBYZCV\nB5nDgmOibcGVdbQ9ttwGHr8evz0aFMId69Fol890d664YzrWe5KWAelZEMns8p4F6ZnBe0ZOUPB2\nt6/zPQsiGd1s6+a8kdjPBGjaGXzXTXV7Xs07g+2dy3Wwfe2e5Zbdff87ZAzbN3l0m0gKgu9ur6v7\nLlfzPdXB54zYU6iPOTqugI97dRT8mcMH791PEgy5pHCgV/TJMmzYsM7lNWvW8OMf/5hXXnmFwsJC\nrrrqqm6fNYhvmI5EIrS1tR2SWA+J5l2w4UV49zlY9yxse6P34zuuTDOHBwVFVh4MHwPF0/YU7B3b\n4wv7jm1ZcYngENTH7jf3PUmjI/l0FNSDsaCKtsclkp09J5L45YYdsOPdPevdFfSRrD0F+fDRMHpW\n9wX8sJGQWxwkNzkgQy4pDGQ7d+4kLy+P/Px8tmzZwqJFizjvvPPCDiu52pqhYkmQAN59FjYtCwrA\nSBZMnAtn3gZFpXGFeUdBHivUO65ahyqzoDqlv6pUwpYWgZyi4HWgWpv2JAhLCwr6rLzBmSQHoSHy\nP3FwOP7445k5cyYzZsxg8uTJnHbaaWGH1P+i7bDltSABvPscbHgJ2hqDP+5xs+HUm2DK6TDp5KDq\nQ6SrjOzgNVydK8Iw6OZonjNnjnedZOett97iyCOPDCmi8AyI39sdtq8JksC6xbD+haBrHsDIGTDl\nDDjsDJh8WtBQJyKhMLNl7j6nr+N0pyD7r65iT3XQu8/Bri3B9oJJcORFMGUeTHk/5I0JNUwR2X9K\nCtK3+mpY/3zsbuBZ2PFOsD23OKgK6rgbKJqiel+RQU5JQfbVvBveeymoDnr3Wdj6JuBBD57Jp8GJ\n1waJYNTMId1fWyQVKSlI8JBTxZKgKujdZ4PlaFvQLXLiSXDmN4IkMP54dfUTGeKUFFJV5WpYsyio\nDnrvpWAQLwzGHQen3BBUB008GTJzw45URA4hJYVU0rwbVj4Orz4U3A0AlBwBs68K2gZK33dw/ctF\nZNBLalIws/OAHwMR4D53v6vL/snAA8BIYAdwlbtXJDOmZOiPobMBHnjgAS644ALGjOnHXjvusOlV\nePVBePOxYBiCkiPg3H+HWZdC/rj++1kiMuglLSmYWQRYAJwDVABLzGyhu6+KO+z7wEPu/qCZfQD4\nT+CTyYopWRIZOjsRDzzwAMcff3z/JIWGHfDG74O7gm1vQkYuHPUROP7TwZPE6iUkcsi0tkdpaGmn\nqbWdhpZ2GlraOpcbW9pp7LLc2BJbb22LW27n6lNLOevI0UmNNZl3CnOBte6+DsDMHgEuAeKTwkzg\nlthyGfCHJMYTigcffJAFCxbQ0tLCqaeeyj333EM0GuWaa65hxYoVuDvz589n9OjRrFixgssvv5yc\nnJz9usPo5B48PPbqg7BqYTBC5rjZcNGPYNZHgwHGRGQf0ajT0BoU1o0t7dQ3BwVyQ8uewrqhs8Bu\ni1tu32t5T+He1rnc1NpOa/v+PSScZpCbmU52RoTczOCVnRGhtb0fR7ztQTKTwnhgY9x6BXBSl2Ne\nAy4lqGL6CJBnZsXuXh1/kJnNB+YDTJo0qfef+vStsLWPQdb215ij4fy7+j6uizfffJMnnniCF198\nkfT0dObPn88jjzzC1KlT2b59O2+8EcRZW1tLYWEhd999N/fccw/HHXfc/v2gXdtgxa9h+S9hx7pg\nGOPjPxW8xh6z33GLDETuTnNbcMVd37yn0G1oaaOhuZ2GWIEdFOix7S3t++zr7rim1v0rbCNpRm5G\nhJzM2Cu2nJsZoSg3M1iO27/Xcufx6eTECv2Oc3QsZ0bSQpuHPeyG5i8D95jZ1cBzwCagvetB7n4v\ncC8Ew1wcygAPxl//+leWLFnSOXR2Y2MjEydO5IMf/CDl5eXcdNNNXHjhhZx77rn7f/KO+Vwf+QSU\nPx2MsDn5NDjjVph5scYVkgGrpS1KbUMLOxpa2FHfQk19KzsaWqipD9aDfa3U1Lews6k17kq9jeh+\n/PXHF9zB1XY6uZkR8rPTGZuf3VkAD8vaUzjHH9d1X1C4p5OTGSEjYqEV2smWzKSwCZgYtz4htq2T\nu28muFPAzIYDH3X32oP6qQdwRZ8s7s5nPvMZ7rzzzn32vf766zz99NMsWLCAxx57jHvvvTexk7Y1\nB+0FDdXBePIb/wmn3gCzPwUl0/r5NxDpXWt7lNqGVmo6C/iWzgK+Jlawdxb4DUEC2N3c81DweVnp\nFA3LpCg3g+LhmUwpGbZvQR1b71rY52ZGyM1K70wEWenhXW0PZslMCkuA6WY2hSAZXAH8S/wBZlYC\n7HD3KPB1gp5IQ8bZZ5/NZZddxs0330xJSQnV1dXU19eTk5NDdnY2H/vYx5g+fTrXXXcdAHl5eeza\ntWvfE3k0NsNUdTAZDQRDSw8rgS+uCiZOEekHTa3tbK1ronJXc1DIN8Re9S3sqI8r/GPvu5p6LuCH\nZUYoGpbJiGGZFOYGBXzRsExG5GbGbc9gRGxbYW4mmel6Qj5sSUsK7t5mZjcAiwi6pD7g7ivN7A5g\nqbsvBOYB/2lmTlB99IVkxROGo48+mm9+85ucffbZRKNRMjIy+OlPf0okEuHaa6/F3TEzvvvd7wJw\nzTXXcN111+1paLZocEfQuGPPE8Z5YyCnOEgElW8pIUjCmtvaqdzZzObaRrbUNbG5rpGtdU1srm1i\nS2y5ur77mcxyMiKMGJZJ0bAMinIzmVycS1FuJkW5mYwYltFZ2Bfm7insszMG4KRG0icNnT3QRNuD\noafrq6G1ns5JwnOL95loZEj93nJQWtujbNvZFBT2tUEB37m8Myj4t+9u3udz+dnpjCvMYWxBNmMK\nchhXkM3YwhxG52cFV/DDgoJfBfzgp6GzB5uWhthdQU3QaBzJCh4syxmh8YZSXFt7lMpdzWypC67w\nt9TGXeXXNbGltpGq3c10vb7Ly0pnbGFQ2M8cm8/YgqDwH1uY3bk8LEtFgOxN/yPCFG0LkkB9dTA7\nGWnBRDS5xcFE8mokSwl1ja1U1DSwcUcjFTUNe67yYwX/tp1N+/S6yc2MMLYgm3GFORx++EjGFu65\nyh9bkM3YgmzysnUxIftvyCSFjvr5Ac8dWur33BXgkJ4DBROCcYfSEvsnGWzVfqlsd3MbFTUNVOxo\nZGNNAxU1jWzcEXuvadinsTYrPa2zSufUqSWMK8xmTEE24wpyOq/y87PTB8f/dxl0hkRSyM7Oprq6\nmuLi4oH9h+IOte8FDccWCe4Icov3eyRSd6e6uprs7OwkBSr7o7GlPSj0a4Ir/Y0d77Er/5qG1r2O\nz8mIMKEohwlFOcwpLYot5zKxKJfxRTkU5WYM7P/HMqQNiaQwYcIEKioqqKqqCjuU3nXcIWTlQ3Y+\n1O4Gdh/QqbKzs5kwYUL/xifdamptZ3NtY2dhH3+lX1HTwPbde/fYyUxP6yzoj5lQwISiXCYU5TBx\nRPBePCxThb4MWEMiKWRkZDBlypSww+hdzQb46bkwehZc/SdIU2+OgcLdqahp5L0dDXtd4XckgW07\n9+61kxExxhcGhf7ZR47uLOyDq/0cSoZnkZamQl8GpyGRFAa89jZ4fH6wfOn/KCGErKUtysrNdSxd\nX8PSDTtYtqFmr6v9SJoxtiCbiUW5nD595N6F/ogcRuVlE1GhL0OUksKh8MKPYOPLcOnPoLCPAf2k\n39U1tvLqezUsW1/DkvU7eK2itnMAtMnFuZx++EhOmFzEYSXDmTgihzH52aRH9GStpCYlhWSrWAaL\n/xNmXQbHfDzsaIY8d2dTbWPnXcDS9TWUb9uFe3AHMGtcPv8ydzInlhZxwuQiRuWrsV4knpJCMjXv\nhsevCx5Cu/AHYUczJLVHnbe27GTp+h0s3VDD0vU1bN3ZBMDwrHRmTyrkgqPHMmdyEcdNKiQ3U//l\nRXqjv5BkWvR12PFu0LCcUxh2NENCfXMbKzbWsmR90Bbw6oYa6luC0dbHFmRz4pQRnXcBM8bkq+5f\nZD8pKSTLW38MpsJ83xeh9H1hRzNobdvZtFdV0KotO2mPOmYwY0w+lx4/gTmlRcwpHcH4Qs0hIXKw\nlBSSYecWWHgTjD0O5n0j7GgGjWjUWVu1O7gLWF/Dkg072LijEYDsjDRmTyzi8/OmMqd0BLMnFZKv\nYRxE+p2SQn+LRuHJz0NbE3z0Pg1t3Yto1Fm+sZaX11WzbEMNyzbUUNcYPP1bMjyLE0uL+PQppZxY\nOoKZ4/LJUI8gkaRTUuhvr/wPvPN3uOhHUDI97GgGpHeqdvOH5Zt4YvkmKmqCO4Fpo4Zz/qwxzCkd\nwZzJRUwuztVTvyIhUFLoT9tWwjPfhCMugBOuCTuaAaV6dzN/fG0zTyzfxGsVdaQZnDathFvOOZwz\njxhF0TDdUYkMBEoK/aW1CR67LpgQ5+K7New1wZhBz6zaxhPLN/Hs21W0R52ZY/P5/y44kouPG8do\nPSMgMuAoKfSXv30bKlfBJx4N5k5OUdGo8/K71fxh+SaefmMru5rbGJOfzXXvn8KlsydwxJi8sEMU\nkV4oKfSHtX+Dl/8L5s6H6eeEHU0o1mzbxePLN/Hk8k1srmtiWGaE848ey6Wzx3PSYcV6XkBkkFBS\nOFj11fCHz8PIGXDOHWFHc0hV7mpi4YqgnWDl5p1E0ozTp5fwtfNncO7MMeRkauA/kcFGSeFguMMf\nbwrmSPjE7yFj6D881dDSxl9WbuPx5Zt4YU0VUYejxxdw+0Uz+dCx4xiZlxV2iCJyEJQUDsbyX8Lq\nP8E5d8LYY8KOJmnao86L72znieWbWPTmVupb2hlfmMPn5k3lI7PHM22U2glEhoqkJgUzOw/4MRAB\n7nP3u7rsnwQ8CBTGjrnV3Z9KZkz9pvodePpWmHI6nHJD2NEkxVtbdvLE8k08uWIT23Y2k5edzoeO\nHceHZ49nbukITSQjMgQlLSmYWQRYAJwDVABLzGyhu6+KO+w24Hfu/t9mNhN4CihNVkz9pr0VHv8s\nRDLgwz+FtKHzpO3WuiaeXBE8WLZ66y7S04x5R4zk9osmcNaRo8jOUDuByFCWzDuFucBad18HYGaP\nAJcA8UnBgfzYcgGwOYnx9J9nvweblsHHHoSC8WFHc9B2N7fx5ze38oflm/jHO9txh+MmFnLHJUdx\n0THjGKEHy0RSRjKTwnhgY9x6BXBSl2O+BfzFzG4EhgFnd3ciM5sPzAeYNCnkmcs2vATPfx+O+wQc\n9eFwYzkIbe1Rnl+7nT8s38SilVtpao0ycUQON35gOh8+bhyHjRwedogiEoKwG5qvBH7h7j8ws1OA\nX5rZLHePxh/k7vcC9wLMmTPHQ4gz0FQHT8wPptQ8/7uhhXGwKmoauPYXSynftouCnAwuPX4Cl84e\nzwmTizTekEiKS2ZS2ARMjFufENsW71rgPAB3f8nMsoESoDKJcR24p74KdZvgM3+GrMHZ42bl5jqu\n+fkSGlvb+cmVs/ngUaPJSlc7gYgEktlCugSYbmZTzCwTuAJY2OWY94CzAMzsSCAbqEpiTAfuzcfg\n9Ufg9K/AxLlhR3NAnl9TxeX/8zKRNOPR60/l4mPHKSGIyF6Sdqfg7m1mdgOwiKC76QPuvtLM7gCW\nuvtC4EvAz8zsiwSNzle7e3jVQz2pq4A/fREmnBgkhUHosWUVfO2x15k2aji/uGYuYwo0GJ2I7Cup\nbQqxZw6e6rLt9rjlVcBpyYzhoEXb4Ynrg/dL74VI2M0w+8fd+a/F7/B/F5Vz6tRifvrJEzRjmYj0\naHCVcGF48W5Y/zxcsgBGHBZ2NPulrT3K7QtX8pt/vseHjxvH9y47lsz0ofNMhYj0PyWF3mxeAX//\nDhx5cdAFdRBpaGnjpoeX89e3KvncvKl85dwj9ASyiPRJSaEnLQ3BU8vDRsKHfjyoJs3ZvruZax9c\nyhsVtdx5yVF88pTSsEMSkUFCSaEnf7kNtr8Nn3oSckeEHU3C1m+v59M/f4WtdU389KoTOPeoMWGH\nJCKDiJJCd8r/DEvvDwa6O2xe2NEkbPl7NVz74FLcnd989mROmFwUdkgiMsgoKXS1uxKe/AKMPhrO\nur3v4weIZ1Zt48aHX2VkXhYPXjNXw1SIyAFRUojnHiSElt3w0Z9B+uCYMOZXL2/g9iffZNb4Au7/\n9Ima6EZEDpiSQrwl98Gav8D534NRR4YdTZ/cne//pZwFZe9w5hEjWfCJ48nN1D+piBw4lSAdqsqD\nxuVpZ8Pc+WFH06eWtii3PvY6jy/fxJVzJ3LnJbNIj+gZBBE5OEoKAG0t8Nh1kDkMLvmvAd/9dFdT\nK5/71au8sHY7t5xzODd+YJpGNxWRfqGkAFD2Hdj6OlzxMOSNDjuaXm3b2cSnH3iFtZW7+b+XHcPH\n5kzs+0MiIglSUnj3OfjHT+CEq2HGBWFH06u3t+3i6gdeoa6xlfuvPpEzDh8ZdkgiMsSkdlJorAkG\nuyueCh/8j7Cj6dXL66qZ/9BSsjIi/Pb/nMKs8QVhhyQiQ1DqJgX3YDjs3dvg2meC9oQB6k+vb+aW\n377GxBE5/OKauUwckRt2SCIyRKVuUnjtEVj5RPCA2vjjw46mR/c9v47v/O9bzJlcxH2fnkNhbmbY\nIYnIEJaaSaFmPTz1FZh8Gpz2r2FH0632qPOd/13Fz/+xnvNnjeFHlx9HdoZmSROR5Eq9pNDeBo/P\nB0uDj/wU0gZeQdvU2s4Xf7uCp9/cyjWnlXLbhTOJaNhrETkEUi8pvPBD2PhPuPQ+KJwUdjT7qG1o\n4bMPLWXJ+hpuu/BIrnv/4JrYR0QGt9RKChVLYfFdcPTH4JiPhR3NPjbuaODqn7/Cxh2N3H3lbD50\n7LiwQxKRFJM6SaF5dzBpTv44uOD7YUezjzc31XHNL5bQ3NrOQ9fO5eTDisMOSURSUOokhRd+CDve\nhav/F3IKw45mL8++XcXnf7WMgpwMfv25Uzl8dF7YIYlIikqdpPD+L8H4E6D0tLAj2cvvl27k64+/\nwbRRw/nFNXMZU5AddkgiksKSOqymmZ1nZuVmttbMbu1m/4/MbEXs9baZ1SYtmMxhMOPCpJ1+f7k7\nP/nbGr7y6OucdNgIfn/9KUoIIhK6pN0pmFkEWACcA1QAS8xsobuv6jjG3b8Yd/yNwOxkxTOQtLVH\n+bcn3+ThVzbykdnj+e5HjyEzXcNei0j4+iyJzOxGMzuQyX7nAmvdfZ27twCPAJf0cvyVwMMH8HMG\nlYaWNub/chkPv7KRz8+byg8/fqwSgogMGImURqMJrvJ/F6sOSvQpqvHAxrj1iti2fZjZZGAK8Pce\n9s83s6VmtrSqqirBHz8w3fmnVSwur+TOD8/iq+fN0DwIIjKg9JkU3P02YDpwP3A1sMbM/sPMpvZj\nHFcAj7p7ew8x3Ovuc9x9zsiRg3e46GjUWbRyGx86dhyfPHly2OGIiOwjoXoLd3dga+zVBhQBj5rZ\n93r52CYgfgaYCbFt3bmCFKg6en1THTvqWzjziFFhhyIi0q1E2hRuNrNlwPeAfwBHu/vngBOAj/by\n0SXAdDObYmaZBAX/wm7OP4Mgybx0APEPKmWrKzGD0zU5jogMUIn0PhoBXOruG+I3unvUzC7q6UPu\n3mZmNwCLgAjwgLuvNLM7gKXu3pEgrgAeid2NDGmL367iuImFjBim4a9FZGBKJCk8DezoWDGzfOBI\nd/+nu7/V2wfd/SngqS7bbu+y/q2Eox3Etu9u5vWKWr549uFhhyIi0qNE2hT+G9gdt747tk32w3Nv\nV+GO2hNEZEBLJClYfNWOu0dJpeEx+klZeRUlw7M4alx+2KGIiPQokaSwzsxuMrOM2OtmYF2yAxtK\n2tqjPPd2FWccPpI0TZYjIgNYIknheuBUgu6kFcBJwPxkBjXUvFZRS11jK2fOUK8jERnY+qwGcvdK\ngh5CcoDKVlcRSTPeP01JQUQGtj6TgpllA9cCRwGdw3i6+2eSGNeQUlZeyQmTiijIzQg7FBGRXiVS\nffRLYAzwQeBZgieTdyUzqKFk284mVm7eyTxVHYnIIJBIUpjm7v8G1Lv7g8CFBO0KkoBny4MB/OYd\nrq6oIjLwJZIUWmPvtWY2CygAVMIlqKy8ktH5WRw5VlNsisjAl8jzBvfG5lO4jWDsouHAvyU1qiGi\ntT3KC2u2c+ExYzVEtogMCr0mBTNLA3a6ew3wHHDYIYlqiFi2oYZdzW3M01PMIjJI9Fp9FHt6+auH\nKJYhp6y8koyIcdq04rBDERFU1dp9AAAQcElEQVRJSCJtCn81sy+b2UQzG9HxSnpkQ8Di1VWcWDqC\nvGx1RRWRwSGRNoXLY+9fiNvmqCqpV5trGynftotvnDAj7FBERBKWyBPNUw5FIEPN4lhXVI2KKiKD\nSSJPNH+qu+3u/lD/hzN0lJVXMr4wh2mjhocdiohIwhKpPjoxbjkbOAt4FVBS6EFzWzv/WLudS48f\nr66oIjKoJFJ9dGP8upkVAo8kLaIhYMm7NTS0tKvqSEQGnUR6H3VVD6idoRdl5ZVkpqdxylR1RRWR\nwSWRNoU/EvQ2giCJzAR+l8ygBruy8kpOmjKC3ExNUCcig0sipdb345bbgA3uXpGkeAa996obWFdV\nz1UnTQ47FBGR/ZZIUngP2OLuTQBmlmNmpe6+PqmRDVKL364E4MwZak8QkcEnkTaF3wPRuPX22LY+\nmdl5ZlZuZmvN7NYejvm4ma0ys5Vm9ptEzjuQla2upLQ4lyklw8IORURkvyVyp5Du7i0dK+7eYmaZ\nfX3IzCLAAuAcgrmdl5jZQndfFXfMdODrwGnuXmNmg/ryuqm1nRffqebKuZPCDkVE5IAkcqdQZWYX\nd6yY2SXA9gQ+NxdY6+7rYknlEeCSLsd8FlgQG4W1Yz7oQeulddU0t0WZd4RmWRORwSmRO4XrgV+b\n2T2x9Qqg26ecuxgPbIxbr2DfGdsOBzCzfwAR4Fvu/ueuJzKz+cB8gEmTBu5V+LPlVWRnpHHyYeqK\nKiKDUyIPr70DnGxmw2Pru/v5508H5hHM/fycmR3t7rVdYrgXuBdgzpw53vUkA4G78/fVlZw6tYTs\njEjY4YiIHJA+q4/M7D/MrNDdd7v7bjMrMrPvJHDuTcDEuPUJsW3xKoCF7t7q7u8CbxMkiUHn3e31\nvLejgTNVdSQig1gibQrnx1+5x+r/L0jgc0uA6WY2JdYwfQXBdJ7x/kBwl4CZlRBUJ61L4NwDTlls\nVFTNsiYig1kiSSFiZlkdK2aWA2T1cjwA7t4G3AAsAt4CfufuK83sjriG60VAtZmtAsqAr7h79f7+\nEgPB4vJKpo0azsQRuWGHIiJywBJpaP418Dcz+zlgwNXAg4mc3N2fAp7qsu32uGUHbom9Bq365jb+\nuW4HnzpFTzGLyOCWSEPzd83sNeBsgjGQFgEq/eK89E41Le1RPcUsIoNeoqOkbiNICB8DPkBQHSQx\nZeWVDMuMMKe0KOxQREQOSo93CmZ2OHBl7LUd+C1g7n7mIYptUHB3FpdXcdq0ErLS1RVVRAa33u4U\nVhPcFVzk7u9z97sJxj2SOGsqd7OptlFVRyIyJPSWFC4FtgBlZvYzMzuLoKFZ4pStDkbm0NAWIjIU\n9JgU3P0P7n4FMIOgu+i/AqPM7L/N7NxDFeBAt7i8ihlj8hhbkBN2KCIiB63PhmZ3r3f337j7hwie\nSl4OfC3pkQ0Cu5paWbJ+hx5YE5EhY7/maHb3Gne/193PSlZAg8k/1m6nLeoa2kJEhoz9Sgqyt7LV\nVeRlp3P8ZHVFFZGhQUnhALk7ZeWVnD59JBkRfY0iMjSoNDtAq7bspHJXM2eo6khEhhAlhQO0uGNU\n1MOVFERk6FBSOECLyyuZNT6fUfnZYYciItJvlBQOQF1DK8s21HCmuqKKyBCjpHAAnltTRdQ1oY6I\nDD1KCgegrLySwtwMjptYGHYoIiL9SklhP0WjzrPlVZw+fSSRNA0FJSJDi5LCfnpzcx3V9S2cOUO9\njkRk6FFS2E9lq6swg9OnKymIyNCjpLCfysorOXZCIcXDs8IORUSk3ykp7Ifq3c28VlGrrqgiMmQp\nKeyH59ZU4Y7aE0RkyEpqUjCz88ys3MzWmtmt3ey/2syqzGxF7HVdMuM5WIvLqygZnsmscQVhhyIi\nkhTpyTqxmUWABcA5QAWwxMwWuvuqLof+1t1vSFYc/aU96jz7dhUfmDGKNHVFFZEhKpl3CnOBte6+\nzt1bgEeAS5L485JqxcZaahta1Z4gIkNaMpPCeGBj3HpFbFtXHzWz183sUTOb2N2JzGy+mS01s6VV\nVVXJiLVPi8srSVNXVBEZ4sJuaP4jUOruxwDPAA92d1BsCtA57j5n5MhwCuWy8kpOmFxEQW5GKD9f\nRORQSGZS2ATEX/lPiG3r5O7V7t4cW70POCGJ8Rywyl1NvLlppwbAE5EhL5lJYQkw3cymmFkmcAWw\nMP4AMxsbt3ox8FYS4zlgz3ZMqKNZ1kRkiEta7yN3bzOzG4BFQAR4wN1XmtkdwFJ3XwjcZGYXA23A\nDuDqZMVzMBaXVzEqL4uZY/PDDkVEJKmSlhQA3P0p4Kku226PW/468PVkxnCwWtujPLemigtmjcVM\nXVFFZGgLu6F5wHt1Qw27mtr0FLOIpAQlhT6UlVeRnmacNq0k7FBERJJOSaEPi8srmVNaRF62uqKK\nyNCnpNCLLXWNrN66S08xi0jKUFLoxeJYV9QzZygpiEhqUFLoRdnqSsYX5jB91PCwQxEROSSUFHrQ\n3NbOP9ZuZ94RI9UVVURShpJCD5aur6G+pV3tCSKSUpQUerC4vJLMSBqnTisOOxQRkUNGSaEHZeVV\nnHTYCHIzk/rQt4jIgKKk0I2NOxpYW7lbo6KKSMpRUujG4vJKAM7UqKgikmKUFLpRVl7F5OJcppQM\nCzsUEZFDSkmhi6bWdl58ZztnHjFKXVFFJOUoKXTxz3d30NQa5QxVHYlIClJS6KJsdSVZ6Wmccpi6\noopI6lFS6GJxeSWnTi0mOyMSdigiIoeckkKcd7fXs766QQPgiUjKUlKIU7Y66Io673AlBRFJTUoK\ncRa/XcVhI4cxqTg37FBEREKhpBDT0NLGy+uqNQCeiKQ0JYWYl96ppqUtqqQgIiktqUnBzM4zs3Iz\nW2tmt/Zy3EfNzM1sTjLj6U1ZeSW5mRFOnFIUVggiIqFLWlIwswiwADgfmAlcaWYzuzkuD7gZ+Gey\nYumLu1O2uorTppWQla6uqCKSupJ5pzAXWOvu69y9BXgEuKSb4+4Evgs0JTGWXr1TtZtNtY2qOhKR\nlJfMpDAe2Bi3XhHb1snMjgcmuvv/9nYiM5tvZkvNbGlVVVW/B1q2OjjnPA1tISIpLrSGZjNLA34I\nfKmvY939Xnef4+5zRo7s/4K7rLySI0bnMa4wp9/PLSIymCQzKWwCJsatT4ht65AHzAIWm9l64GRg\n4aFubN7V1MqS9TuYN0N3CSIiyUwKS4DpZjbFzDKBK4CFHTvdvc7dS9y91N1LgZeBi919aRJj2sc/\n1lbT2u5qTxARIYlJwd3bgBuARcBbwO/cfaWZ3WFmFyfr5+6vxeWV5GWlc8JkdUUVEUnqrPTu/hTw\nVJdtt/dw7LxkxtLDz2RxeRXvP7yEjIie4xMRSemScPXWXWzd2aQB8EREYlI6KZSVB6OiapY1EZFA\nSieFxaurOGpcPqPzs8MORURkQEjZpFDX2Mqy92rU60hEJE7KJoUX1mynPeqcqecTREQ6pWxSKCuv\npCAng+MmqiuqiEiHlEwK0WjQFfX0w0cSSbOwwxERGTBSMims3LyT7bubOVO9jkRE9pKSSaGsvBIz\nOP1wJQURkXgpmxSOmVBIyfCssEMRERlQUi4p7KhvYcXGWlUdiYh0I+WSwvNrqnCHeXo+QURkHymX\nFMpWV1I8LJNjxheEHYqIyICTUkmhPeo8+3YVZxw+kjR1RRUR2UdKJYXXKmqpaWhl3gxVHYmIdCel\nksLi1ZWkGZw+vSTsUEREBqTUSgpvV3H8pCIKczPDDkVEZEBKmaRQtauZ1yvqmKeuqCIiPUqZpPDs\n21WAuqKKiPQmZZJCfnY6584czVHj8sMORURkwEoPO4BD5dyjxnDuUWPCDkNEZEBLmTsFERHpW1KT\ngpmdZ2blZrbWzG7tZv/1ZvaGma0wsxfMbGYy4xERkd4lLSmYWQRYAJwPzASu7KbQ/427H+3uxwHf\nA36YrHhERKRvybxTmAusdfd17t4CPAJcEn+Au++MWx0GeBLjERGRPiSzoXk8sDFuvQI4qetBZvYF\n4BYgE/hAdycys/nAfIBJkyb1e6AiIhIIvaHZ3Re4+1Tga8BtPRxzr7vPcfc5I0fq4TMRkWRJZlLY\nBEyMW58Q29aTR4APJzEeERHpQzKTwhJguplNMbNM4ApgYfwBZjY9bvVCYE0S4xERkT4krU3B3dvM\n7AZgERABHnD3lWZ2B7DU3RcCN5jZ2UArUAN8uq/zLlu2bLuZbTjAsEqA7Qf42aFI38fe9H3soe9i\nb0Ph+5icyEHmnjodfsxsqbvPCTuOgULfx970feyh72JvqfR9hN7QLCIiA4eSgoiIdEq1pHBv2AEM\nMPo+9qbvYw99F3tLme8jpdoURESkd6l2pyAiIr1QUhARkU4pkxT6GsY7VZjZRDMrM7NVZrbSzG4O\nO6aBwMwiZrbczP4UdixhM7NCM3vUzFab2VtmdkrYMYXFzL4Y+zt508weNrPssGNKtpRICgkO450q\n2oAvuftM4GTgCyn8XcS7GXgr7CAGiB8Df3b3GcCxpOj3YmbjgZuAOe4+i+Ah3CvCjSr5UiIpkMAw\n3qnC3be4+6ux5V0Ef/Djw40qXGY2gWCYlfvCjiVsZlYAnA7cD+DuLe5eG25UoUoHcswsHcgFNocc\nT9KlSlLobhjvlC4IAcysFJgN/DPcSEL3/4CvAtGwAxkApgBVwM9j1Wn3mdmwsIMKg7tvAr4PvAds\nAerc/S/hRpV8qZIUpAszGw48Bvxrl8mOUoqZXQRUuvuysGMZINKB44H/dvfZQD2Qkm1wZlZEUKMw\nBRgHDDOzq8KNKvlSJSns7zDeQ5qZZRAkhF+7++NhxxOy04CLzWw9QbXiB8zsV+GGFKoKoMLdO+4e\nHyVIEqnobOBdd69y91bgceDUkGNKulRJCn0O450qzMwI6ovfcveUnxPb3b/u7hPcvZTg/8Xf3X3I\nXw32xN23AhvN7IjYprOAVSGGFKb3gJPNLDf2d3MWKdDonszpOAeMnobxDjmssJwGfBJ4w8xWxLZ9\nw92fCjEmGVhuBH4du4BaB1wTcjyhcPd/mtmjwKsEvfaWkwLDXWiYCxER6ZQq1UciIpIAJQUREemk\npCAiIp2UFEREpJOSgoiIdFJSEOnCzNrNbEXcq9+e6DWzUjN7s7/OJ9LfUuI5BZH91Ojux4UdhEgY\ndKcgkiAzW29m3zOzN8zsFTObFtteamZ/N7PXzexvZjYptn20mT1hZq/FXh1DJETM7Gexcfr/YmY5\nof1SIl0oKYjsK6dL9dHlcfvq3P1o4B6C0VUB7gYedPdjgF8DP4lt/wnwrLsfSzB+UMdT9NOBBe5+\nFFALfDTJv49IwvREs0gXZrbb3Yd3s3098AF3XxcbVHCruxeb2XZgrLu3xrZvcfcSM6sCJrh7c9w5\nSoFn3H16bP1rQIa7fyf5v5lI33SnILJ/vIfl/dEct9yO2vZkAFFSENk/l8e9vxRbfpE90zR+Ang+\ntvw34HPQOQd0waEKUuRA6QpFZF85cSPIQjBfcUe31CIze53gav/K2LYbCWYq+wrBrGUdo4reDNxr\nZtcS3BF8jmAGL5EBS20KIgmKtSnMcfftYccikiyqPhIRkU66UxARkU66UxARkU5KCiIi0klJQURE\nOikpiIhIJyUFERHp9P8DMcD1kjxz8qIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niCT8f94C7LfKhM1oAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "12630/12630 [==============================] - 1s 50us/step\n",
            "Test accuracy: 93.63%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33TQxVDWdoHH",
        "colab_type": "code",
        "outputId": "f9dcff65-7da7-4faa-e8ee-1301919e5682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "model = create_model()\n",
        "\n",
        "#Check untrained first\n",
        "loss, acc = model.evaluate(X_test, Y_test)\n",
        "print(\"Untrained model, accuracy: {:5.2f}%\".format(100*acc))\n",
        "\n",
        "#Now restore model and check accuracy\n",
        "model.load_weights(checkpoint_path)\n",
        "loss,acc = model.evaluate(X_test, Y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12630/12630 [==============================] - 1s 59us/step\n",
            "Untrained model, accuracy:  1.51%\n",
            "12630/12630 [==============================] - 1s 47us/step\n",
            "Restored model, accuracy: 93.63%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2laXHD7-tmk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generator\n",
        "'''\n",
        "\tGenerator definition for AdvGAN\n",
        "\tref: https://arxiv.org/pdf/1801.02610.pdf\n",
        "'''\n",
        "\n",
        "#import tensorflow as tf\n",
        "#from keras import layers\n",
        "\n",
        "# helper function for convolution -> instance norm -> relu\n",
        "def ConvInstNormRelu(x, filters, kernel_size=3, strides=1):\n",
        "\tConv = tf.layers.conv2d(\n",
        "\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tInstNorm = tf.contrib.layers.instance_norm(Conv)\n",
        "\n",
        "\treturn tf.nn.relu(InstNorm)\n",
        "\n",
        "\n",
        "# helper function for trans convolution -> instance norm -> relu\n",
        "def TransConvInstNormRelu(x, filters, kernel_size=3, strides=2):\n",
        "\tTransConv = tf.layers.conv2d_transpose(\n",
        "\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tInstNorm = tf.contrib.layers.instance_norm(TransConv)\n",
        "\n",
        "\treturn tf.nn.relu(InstNorm)\n",
        "\n",
        "# helper function for residual block of 2 convolutions with same num filters\n",
        "# in the same style as ConvInstNormRelu\n",
        "def ResBlock(x, training, filters=32, kernel_size=3, strides=1):\n",
        "\tconv1 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tconv1_norm = tf.layers.batch_normalization(conv1, training=training)\n",
        "\n",
        "\tconv1_relu = tf.nn.relu(conv1_norm)\n",
        "\n",
        "\tconv2 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\tinputs=conv1_relu,\n",
        "\t\t\t\t\t\tfilters=filters,\n",
        "\t\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\t\tstrides=strides,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\tconv2_norm = tf.layers.batch_normalization(conv2, training=training)\n",
        "\n",
        "\n",
        "\treturn x + conv2_norm\n",
        "\n",
        "\n",
        "def generator(x, training):\n",
        "\twith tf.variable_scope('g_weights', reuse=tf.AUTO_REUSE): #True\n",
        "\t\t# input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
        "        \n",
        "        #6/3 increased all filters 4x\n",
        "\t\t# define first three conv + inst + relu layers\n",
        "\t\tc1 = ConvInstNormRelu(x, filters=32, kernel_size=3, strides=1)\n",
        "\t\td1 = ConvInstNormRelu(c1, filters=64, kernel_size=3, strides=2)\n",
        "\t\td2 = ConvInstNormRelu(d1, filters=128, kernel_size=3, strides=2)\n",
        "        \n",
        "        #then stride\n",
        "\t\t# define residual blocks\n",
        "\t\trb1 = ResBlock(d2, training, filters=128)\n",
        "\t\trb2 = ResBlock(rb1, training, filters=128)\n",
        "\t\trb3 = ResBlock(rb2, training, filters=128)\n",
        "\t\trb4 = ResBlock(rb3, training, filters=128)\n",
        "\n",
        "\t\t# upsample using conv transpose\n",
        "\t\tu1 = TransConvInstNormRelu(rb4, filters=64, kernel_size=3, strides=2)\n",
        "\t\tu2 = TransConvInstNormRelu(u1, filters=32, kernel_size=3, strides=2)\n",
        "\n",
        "\t\t# final layer block\n",
        "\t\tout = tf.layers.conv2d_transpose(\n",
        "\t\t\t\t\t\tinputs=u2,\n",
        "\t\t\t\t\t\tfilters=x.get_shape()[-1].value, # or 3 if RGB image\n",
        "\t\t\t\t\t\tkernel_size=3,\n",
        "\t\t\t\t\t\tstrides=1,\n",
        "\t\t\t\t\t\tpadding=\"same\",\n",
        "\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\t\t# out = tf.contrib.layers.instance_norm(out)\n",
        "\n",
        "\t\treturn tf.nn.tanh(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKYLa86WtyLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Discriminator\n",
        "'''\n",
        "\tDiscriminator definition for AdvGAN\n",
        "\tref: https://arxiv.org/pdf/1801.02610.pdf\n",
        "'''\n",
        "\n",
        "#import tensorflow as tf\n",
        "\n",
        "def discriminator(x, training):\n",
        "\twith tf.variable_scope('d_weights', reuse=tf.AUTO_REUSE):\n",
        "\t\t# input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "\t\tconv1 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\t\tinputs=x,\n",
        "\t\t\t\t\t\t\tfilters=8,\n",
        "\t\t\t\t\t\t\tkernel_size=4,\n",
        "\t\t\t\t\t\t\tstrides=2,\n",
        "\t\t\t\t\t\t\tpadding=\"valid\",\n",
        "\t\t\t\t\t\t\tactivation=None)\n",
        "\t\tconv1 = tf.nn.leaky_relu(conv1, alpha=0.2)\n",
        "\n",
        "\t\t\n",
        "\t\tconv2 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\t\tinputs=conv1,\n",
        "\t\t\t\t\t\t\tfilters=16,\n",
        "\t\t\t\t\t\t\tkernel_size=4,\n",
        "\t\t\t\t\t\t\tstrides=2,\n",
        "\t\t\t\t\t\t\tpadding=\"valid\",\n",
        "\t\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\t\tin1 = tf.contrib.layers.instance_norm(conv2)\n",
        "\t\tconv2 = tf.nn.leaky_relu(in1, alpha=0.2)\n",
        "\n",
        "\t\tconv3 = tf.layers.conv2d(\n",
        "\t\t\t\t\t\t\tinputs=conv2,\n",
        "\t\t\t\t\t\t\tfilters=32,\n",
        "\t\t\t\t\t\t\tkernel_size=4,\n",
        "\t\t\t\t\t\t\tstrides=2,\n",
        "\t\t\t\t\t\t\tpadding=\"valid\",\n",
        "\t\t\t\t\t\t\tactivation=None)\n",
        "\n",
        "\t\t#in2 = tf.contrib.layers.instance_norm(conv3)\n",
        "\t\tin2 = tf.contrib.layers.instance_norm(conv3)\n",
        "\t\tconv3 = tf.nn.leaky_relu(in2, alpha=0.2)\n",
        "\t\tflat = tf.layers.flatten(conv3)\n",
        "\t\tlogits = tf.layers.dense(flat, 1)\n",
        "\n",
        "\t\tprobs = tf.nn.sigmoid(logits)\n",
        "\n",
        "\t\treturn logits, probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "288LCGWawCVl",
        "colab_type": "code",
        "outputId": "2beb3c55-f5fe-47b2-c30e-8f15cd131c97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "#from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os, sys\n",
        "import random\n",
        "\n",
        "#make sure GAN_setup.py is in connected folder\n",
        "#from GAN_setup import generator, discriminator\n",
        "\n",
        "#ctargon created class Target and defined/trained his target model in there, then called here\n",
        "#import Target as target_model\n",
        "\n",
        "\n",
        "# get the next batch based on x, y, and the iteration (based on batch_size)\n",
        "def next_batch(X, Y, i, batch_size):\n",
        "    idx = i * batch_size\n",
        "    idx_n = i * batch_size + batch_size\n",
        "    return X[idx:idx_n], Y[idx:idx_n]\n",
        "\n",
        "# loss function to encourage misclassification after perturbation\n",
        "def adv_loss(preds, labels, is_targeted):\n",
        "    real = tf.reduce_sum(labels * preds, 1)\n",
        "    other = tf.reduce_max((1 - labels) * preds - (labels * 10000), 1)\n",
        "    if is_targeted:\n",
        "        return tf.reduce_sum(tf.maximum(0.0, other - real))\n",
        "    return tf.reduce_sum(tf.maximum(0.0, real - other))\n",
        "\n",
        "# loss function to influence the perturbation to be as close to 0 as possible\n",
        "def perturb_loss(preds, thresh=0.3):\n",
        "    zeros = tf.zeros((tf.shape(preds)[0]))\n",
        "    return tf.reduce_mean(tf.maximum(zeros, tf.norm(tf.reshape(preds, (tf.shape(preds)[0], -1)), axis=1) - thresh))\n",
        "\n",
        "\n",
        "# function that defines ops, graphs, and training procedure for AdvGAN framework\n",
        "def AdvGAN(X, y, X_test, y_test, epochs=50, batch_size=128, target=3):\n",
        "    #print(X_train.shape)\n",
        "    #print(y.shape[-1]) is num_images\n",
        "    print(\"y shape\")\n",
        "    print(y.shape)\n",
        "    print(\"y_test shape\")\n",
        "    print(y_test.shape)\n",
        "    \n",
        "    # placeholder definitions\n",
        "    x_pl = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]]) # image placeholder\n",
        "    t = tf.placeholder(tf.float32, [None, y.shape[-1]]) # target placeholder\n",
        "    print(\"t shape)\")\n",
        "    print(t.shape)\n",
        "    is_training = tf.placeholder(tf.bool, [])\n",
        "\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    # MODEL DEFINITIONS\n",
        "    is_targeted = False\n",
        "    if target in range(0, y.shape[-1]):\n",
        "        is_targeted = True\n",
        "\n",
        "    #tf.reset_default_graph()\n",
        "    \n",
        "    # gather target model\n",
        "    f = Target()\n",
        "    print(\"is targeted boolean\")\n",
        "    print(is_targeted)\n",
        "    \n",
        "    thresh = 0.3\n",
        "\n",
        "    # generate perturbation, add to original input image(s)\n",
        "    perturb = tf.clip_by_value(generator(x_pl, is_training), -thresh, thresh)\n",
        "    x_perturbed = perturb + x_pl\n",
        "    x_perturbed = tf.clip_by_value(x_perturbed, 0, 1)\n",
        "    print(x_perturbed.shape)\n",
        "\n",
        "    # pass real and perturbed image to discriminator and the target model\n",
        "    d_real_logits, d_real_probs = discriminator(x_pl, is_training)\n",
        "    d_fake_logits, d_fake_probs = discriminator(x_perturbed, is_training)\n",
        "    print(d_fake_probs.shape)#1\n",
        "    # pass real and perturbed images to the model we are trying to fool\n",
        "    f_real_logits, f_real_probs = f.ModelC(x_pl)\n",
        "    f_fake_logits, f_fake_probs = f.ModelC(x_perturbed)\n",
        "    print(f_fake_probs.shape) #43\n",
        "\n",
        "    # generate labels for discriminator (optionally smooth labels for stability)\n",
        "    smooth = 0.0\n",
        "    d_labels_real = tf.ones_like(d_real_probs) * (1 - smooth)\n",
        "    d_labels_fake = tf.zeros_like(d_fake_probs)\n",
        "\n",
        "    #-----------------------------------------------------------------------------------\n",
        "    # LOSS DEFINITIONS\n",
        "    # discriminator loss\n",
        "    d_loss_real = tf.losses.mean_squared_error(predictions=d_real_probs, labels=d_labels_real)\n",
        "    d_loss_fake = tf.losses.mean_squared_error(predictions=d_fake_probs, labels=d_labels_fake)\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "    # generator loss\n",
        "    g_loss_fake = tf.losses.mean_squared_error(predictions=d_fake_probs, labels=tf.ones_like(d_fake_probs))\n",
        "\n",
        "    # perturbation loss (minimize overall perturbation)\n",
        "    l_perturb = perturb_loss(perturb, thresh)\n",
        "\n",
        "    # adversarial loss (encourage misclassification)\n",
        "    l_adv = adv_loss(f_fake_probs, t, is_targeted)\n",
        "\n",
        "    # weights for generator loss function\n",
        "    alpha = 1.0\n",
        "    beta = 5.0\n",
        "    g_loss = l_adv + alpha*g_loss_fake + beta*l_perturb \n",
        "\n",
        "    # ----------------------------------------------------------------------------------\n",
        "    # gather variables for training/restoring\n",
        "    t_vars = tf.trainable_variables()\n",
        "    f_vars = [var for var in t_vars if 'ModelC' in var.name]\n",
        "    #f_vars = [var for var in t_vars if 'f_' in var.name]\n",
        "    d_vars = [var for var in t_vars if 'd_' in var.name]\n",
        "    g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='g_weights')\n",
        "\n",
        "    # define optimizers for discriminator and generator\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        d_opt = tf.train.AdamOptimizer().minimize(d_loss, var_list=d_vars)\n",
        "        g_opt = tf.train.AdamOptimizer(learning_rate=0.001).minimize(g_loss, var_list=g_vars)\n",
        "\n",
        "\t# create saver objects for the target model, generator, and discriminator\n",
        "    #saver = tf.train.Saver(f_vars)\n",
        "    g_saver = tf.train.Saver(g_vars)\n",
        "    d_saver = tf.train.Saver(d_vars)\n",
        "    saver = tf.train.import_meta_graph('./weights/target_model/model.ckpt.meta')\n",
        "    \n",
        "    init  = tf.global_variables_initializer()\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    sess.run(init)\n",
        "    #variables_names = [v.name for v in tf.trainable_variables()]\n",
        "    #values = sess.run(variables_names)\n",
        "    #for k, v in zip(variables_names, values):\n",
        "        #print(\"Variable: \", k)\n",
        "        #print(\"Shape: \", v.shape)\n",
        "        #print(v)\n",
        "    # load the pretrained target model\n",
        "    #try:\n",
        "       # saver.restore(sess, \"./weights/target_model/model\")\n",
        "    #except:\n",
        "       # print(\"make sure to train the target model first...\")\n",
        "       # sys.exit(1)\n",
        "\n",
        "    #sess.run(tf.local_variables_initializer())\n",
        "    \n",
        "    saver.restore(sess, tf.train.latest_checkpoint('./weights/target_model'))\n",
        "    #path_to_ckpt_data = './weights/target_model/model.data-00000-of-00001'\n",
        "    #new_saver.restore(sess, path_to_ckpt_data)\n",
        "    \n",
        "    print(\"Pretrained model loaded\")\n",
        "    #run it here\n",
        "    correct_prediction = tf.equal(tf.argmax(f_real_probs, 1), tf.argmax(t, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    accs = []\n",
        "    total_batches_test = int(X_test.shape[0] / batch_size)\n",
        "    for i in range(total_batches_test):\n",
        "        batch_x, batch_y = next_batch(X_test, y_test, i, batch_size)\n",
        "        acc, x_real = sess.run([accuracy, x_pl], feed_dict={x_pl: batch_x, t: batch_y, is_training: False})\n",
        "        accs.append(acc)\n",
        "    \n",
        "    print('testing accuracy of pretrained model: {}'.format(sum(accs) / len(accs)))\n",
        "    \n",
        "'''\n",
        "    total_batches = int(X.shape[0] / batch_size)\n",
        "\n",
        "    for epoch in range(0, epochs):\n",
        "\n",
        "        loss_D_sum = 0.0\n",
        "        loss_G_fake_sum = 0.0\n",
        "        loss_perturb_sum = 0.0\n",
        "        loss_adv_sum = 0.0\n",
        "\n",
        "        for i in range(total_batches):\n",
        "\n",
        "            batch_x, batch_y = next_batch(X, y, i, batch_size)\n",
        "\n",
        "            # if targeted, create one hot vectors of the target\n",
        "            if is_targeted:\n",
        "                targets = np.full((batch_y.shape[0],), target)\n",
        "                batch_y = np.eye(y.shape[-1])[targets]\n",
        "\n",
        "            # train the discriminator first 1 time\n",
        "            for _ in range(1):\n",
        "                _, loss_D_batch = sess.run([d_opt, d_loss], feed_dict={x_pl: batch_x, \\\n",
        "                                           is_training: True})\n",
        "\n",
        "            #print(\"batch x\")\n",
        "            #print(batch_x.shape)\n",
        "            #print(\"batch y\")\n",
        "            #print(batch_y.shape)\n",
        "            \n",
        "\t\t\t# train the generator 1 time (maybe try 10 times (1:10 d:g))\n",
        "            for _ in range(1):\n",
        "                \n",
        "                _, loss_G_fake_batch, loss_adv_batch, loss_perturb_batch = \\\n",
        "                        sess.run([g_opt, g_loss_fake, l_adv, l_perturb], \\\n",
        "                              feed_dict={x_pl: batch_x, \\\n",
        "                                     t: batch_y, \\\n",
        "                                     is_training: True})\n",
        "            loss_D_sum += loss_D_batch\n",
        "            loss_G_fake_sum += loss_G_fake_batch\n",
        "            loss_perturb_sum += loss_perturb_batch\n",
        "            loss_adv_sum += loss_adv_batch\n",
        "\n",
        "        print(\"epoch %d:\\nloss_D: %.3f, loss_G_fake: %.3f, \\\n",
        "            \\nloss_perturb: %.3f, loss_adv: %.3f, \\n\" %\n",
        "            (epoch + 1, loss_D_sum/total_batches, loss_G_fake_sum/total_batches,\n",
        "            loss_perturb_sum/total_batches, loss_adv_sum/total_batches))\n",
        "    \n",
        "        if epoch % 10 == 0:\n",
        "            g_saver.save(sess, \"weights/generator/gen\")\n",
        "            d_saver.save(sess, \"weights/discriminator/disc\")\n",
        "\n",
        "    # evaluate the test set\n",
        "    correct_prediction = tf.equal(tf.argmax(f_fake_probs, 1), tf.argmax(t, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    accs = []\n",
        "    total_batches_test = int(X_test.shape[0] / batch_size)\n",
        "    for i in range(total_batches_test):\n",
        "        batch_x, batch_y = next_batch(X_test, y_test, i, batch_size)\n",
        "        acc, x_pert = sess.run([accuracy, x_perturbed], feed_dict={x_pl: batch_x, t: batch_y, is_training: False})\n",
        "        accs.append(acc)\n",
        "\n",
        "    print('accuracy of test set: {}'.format(sum(accs) / len(accs)))\n",
        "\n",
        "\t# plot some images and their perturbed counterparts\n",
        "    f, axarr = plt.subplots(2,2)\n",
        "    axarr[0,0].imshow(np.squeeze(batch_x[2]), cmap='Greys_r')\n",
        "    axarr[0,1].imshow(np.squeeze(x_pert[2]), cmap='Greys_r')\n",
        "    axarr[1,0].imshow(np.squeeze(batch_x[5]), cmap='Greys_r')\n",
        "    axarr[1,1].imshow(np.squeeze(x_pert[5]), cmap='Greys_r')\n",
        "    plt.show()\n",
        "\n",
        "    print('finished training, saving weights')\n",
        "    g_saver.save(sess, \"weights/generator/gen\")\n",
        "    d_saver.save(sess, \"weights/discriminator/disc\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "def attack(X, y, batch_size=128, thresh=0.3, target=3):\n",
        "    x_pl = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]]) # image placeholder\n",
        "    t = tf.placeholder(tf.float32, [None, y.shape[-1]]) # target placeholder\n",
        "    is_training = tf.placeholder(tf.bool, [])\n",
        "\n",
        "    is_targeted = False\n",
        "    if target in range(0, y.shape[-1]):\n",
        "        is_targeted = True\n",
        "\n",
        "    perturb = tf.clip_by_value(generator(x_pl, is_training), -thresh, thresh)\n",
        "    x_perturbed = perturb + x_pl\n",
        "    x_perturbed = tf.clip_by_value(x_perturbed, 0, 1)\n",
        "\n",
        "    a = Target()\n",
        "    a_real_logits, a_real_probs = a.ModelC(x_pl)\n",
        "    a_fake_logits, a_fake_probs = a.ModelC(x_perturbed)\n",
        "\n",
        "    #t_vars = tf.trainable_variables()\n",
        "    #a_vars = [var for var in t_vars if 'ModelC' in var.name]\n",
        "    g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='g_weights')\n",
        "\n",
        "    init  = tf.global_variables_initializer()\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    sess.run(init)    \n",
        "    \n",
        "    #just using generator and target model\n",
        "    new_saver2 = tf.train.import_meta_graph('./weights/target_model/model.ckpt.meta')\n",
        "    new_saver2.restore(sess, tf.train.latest_checkpoint('./weights/target_model'))\n",
        "    \n",
        "    #f_saver2 = tf.train.Saver(a_vars)\n",
        "    #g_saver = tf.train.Saver(g_vars)\n",
        "    #f_saver2.restore(sess, \"./weights/target_model/model\")\n",
        "    g_saver = tf.train.import_meta_graph('./weights/generator/gen.meta')\n",
        "    g_saver.restore(sess, tf.train.latest_checkpoint(\"./weights/generator\"))\n",
        "\n",
        "    rawpert, pert, fake_l, real_l = sess.run([perturb, x_perturbed, a_fake_probs, a_real_probs], \\\n",
        "                          feed_dict={x_pl: X[:32], \\\n",
        "                                 is_training: False})\n",
        "    \n",
        "    #changed the way the author named these printed lists - please verify that I interpreted his acronyms properly\n",
        "    print('actual labels: ' + str(np.argmax(y[:32], axis=1)))\n",
        "    print('classifier original prediction: ' + str(np.argmax(real_l, axis=1))) \n",
        "    print('classifier prediction after perturbation: ' + str(np.argmax(fake_l, axis=1)))\n",
        "\n",
        "    correct_prediction = tf.equal(tf.argmax(a_fake_probs, 1), tf.argmax(t, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    accs = []\n",
        "    total_batches_test = int(X.shape[0] / batch_size)\n",
        "    for i in range(total_batches_test):\n",
        "        batch_x, batch_y = next_batch(X, y, i, batch_size)\n",
        "\n",
        "        if is_targeted:\n",
        "            targets = np.full((batch_y.shape[0],), target)\n",
        "            batch_y = np.eye(y.shape[-1])[targets]\n",
        "\n",
        "        acc, fake_l, x_pert = sess.run([accuracy, a_fake_probs, x_perturbed], feed_dict={x_pl: batch_x, t: batch_y, is_training: False})\n",
        "        accs.append(acc)\n",
        "\n",
        "    print('accuracy of test set: {}'.format(sum(accs) / len(accs)))\n",
        "\n",
        "    f, axarr = plt.subplots(2,2)\n",
        "    axarr[0,0].imshow(np.squeeze(X[3]), cmap='Greys_r')\n",
        "    axarr[0,1].imshow(np.squeeze(pert[3]), cmap='Greys_r')\n",
        "    axarr[1,0].imshow(np.squeeze(X[4]), cmap='Greys_r')\n",
        "    axarr[1,1].imshow(np.squeeze(pert[4]), cmap='Greys_r')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#main code\n",
        "Y_train = to_categorical(Y_train, num_classes=43)\n",
        "Y_test = to_categorical(Y_test, num_classes=43)\n",
        "Y_valid = to_categorical(Y_valid, num_classes=43)\n",
        "\n",
        "AdvGAN(X_train, Y_train, X_test, Y_test, batch_size=128, epochs=50, target=3)\n",
        "\n",
        "#attack(X_valid, Y_valid, target=3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y shape\n",
            "(34799, 43)\n",
            "y_test shape\n",
            "(12630, 43)\n",
            "t shape)\n",
            "(?, 43)\n",
            "is targeted boolean\n",
            "True\n",
            "WARNING:tensorflow:From <ipython-input-3-e3c56d682c97>:17: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-3-e3c56d682c97>:49: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n",
            "WARNING:tensorflow:From <ipython-input-3-e3c56d682c97>:32: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d_transpose instead.\n",
            "(?, 32, 32, 1)\n",
            "WARNING:tensorflow:From <ipython-input-4-85d97f4f431c>:44: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-4-85d97f4f431c>:45: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "(?, 1)\n",
            "(?, 43)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./weights/target_model/model.ckpt\n",
            "Pretrained model loaded\n",
            "testing accuracy of pretrained model: 0.010044642857142858\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_torch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tortoisehare/TSR-GAN/blob/master/GAN_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DerxOridDA0O",
        "colab_type": "text"
      },
      "source": [
        "pert 0.03, 1:1 G:D, 1x filters, 100 epochs, 90% classifier, adv_lambda:pert_lambda 5:1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMMq81LtuW7R",
        "colab_type": "text"
      },
      "source": [
        "Foundation of PyTorch code provided by github: mathcbc\n",
        "\n",
        "https://github.com/mathcbc/advGAN_pytorch/blob/master/main.py\n",
        "\n",
        "Changes made: \n",
        "\n",
        "target model is LeNet 5\n",
        "\n",
        "generator given optimizations to allow it to train more than discriminator\n",
        "\n",
        "parameters like filters/strides changed in GAN\n",
        "\n",
        "traffic sign dataset loading and preprocessing added\n",
        "\n",
        "input_sizes for layers work with traffic sign images (32x32)\n",
        "\n",
        "training accuracy visualization added\n",
        "\n",
        "testing accuracy plus misclassification percentage\n",
        "\n",
        "misclassified images visualization\n",
        "\n",
        "restoration model testing added\n",
        "\n",
        "bugs fixed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTIpcLONWBq9",
        "colab_type": "code",
        "outputId": "94ce117a-5f82-47c4-bb70-7d5adc48eff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#torch.multiprocessing.set_start_method(\"spawn\") \n",
        "\n",
        "import torch.optim\n",
        "print(torch.__version__)\n",
        "#from torch import np\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')       \n",
        "get_ipython().magic('matplotlib inline')\n",
        "from matplotlib import pyplot    \n",
        "from matplotlib.pyplot import subplot     \n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Target Model definition - LeNet 5\n",
        "class target_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(target_net, self).__init__()\n",
        "\n",
        "        # LAYER 1: Convolution, Input 1x32x32, Output 6x28x28\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0, bias=True)\n",
        "        # Max-pooling, Input 6x28x28, Output 6x14x14\n",
        "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Dropout\n",
        "        self.drop1 = torch.nn.Dropout(0.5)\n",
        "       \n",
        "        # LAYER 2: Convolution, Input 6x14x14, Output 16x10x10\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)\n",
        "        # Max-pooling, Input 16x10x10, Output 16x5x5\n",
        "        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=2, stride=2) \n",
        "        # Dropout\n",
        "        self.drop2 = torch.nn.Dropout(0.5)\n",
        "        \n",
        "        # LAYER 3: FC, Input 400, Output 120\n",
        "        self.fc1 = torch.nn.Linear(16*5*5, 120)   # convert matrix with 16*5*5 (= 400) features to a matrix of 120 features (columns)\n",
        "        self.drop3 = torch.nn.Dropout(0.6)\n",
        "        \n",
        "        # LAYER 4: FC, Input 120, Output 84\n",
        "        self.fc2 = torch.nn.Linear(120, 84)       # convert matrix with 120 features to a matrix of 84 features (columns)\n",
        "        self.drop4 = torch.nn.Dropout(0.6)\n",
        "        \n",
        "        # LAYER 5: FC Input 84, Output 43\n",
        "        self.fc3 = torch.nn.Linear(84, 43)        # convert matrix with 84 features to a matrix of 43 features (columns)\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # convolve, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.conv1(x))  \n",
        "        # max-pooling with 2x2 grid \n",
        "        x = self.max_pool_1(x) \n",
        "        # convolve, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.conv2(x))\n",
        "        # max-pooling with 2x2 grid\n",
        "        x = self.max_pool_2(x)\n",
        "        # first flatten 'max_pool_2_out' to contain 16*5*5 columns\n",
        "        # read through https://stackoverflow.com/a/42482819/7551231\n",
        "        \n",
        "        x = x.view(-1, 16*5*5)\n",
        "        # FC-1, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        # FC-2, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.fc2(x))\n",
        "        # FC-3\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, image_nc):\n",
        "        super(Discriminator, self).__init__()\n",
        "        # Traffic Sign Dataset: 1*32x32\n",
        "        model = [\n",
        "            nn.Conv2d(image_nc, 8, kernel_size=4, stride=2, padding=0, bias=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 8*16*16\n",
        "            nn.Conv2d(8, 16, kernel_size=4, stride=2, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 16*5*5\n",
        "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(32, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "            # 32*1*1\n",
        "        ]\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x).squeeze()\n",
        "        return output\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self,\n",
        "                 gen_input_nc,\n",
        "                 image_nc,\n",
        "                 ):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        encoder_lis = [\n",
        "            # Traffic Sign Dataset:1*32x32\n",
        "            nn.Conv2d(gen_input_nc, 8, kernel_size=3, stride=1, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            # 8*26*26\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            # 16*12*12\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            # 32*5*5\n",
        "        ]\n",
        "\n",
        "        bottle_neck_lis = [ResnetBlock(32),\n",
        "                       ResnetBlock(32),\n",
        "                       ResnetBlock(32),\n",
        "                       ResnetBlock(32),]\n",
        "\n",
        "        decoder_lis = [\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=0, bias=False),\n",
        "            nn.InstanceNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            # state size. 16 x 11 x 11\n",
        "            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=0, bias=False),\n",
        "            nn.InstanceNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            # state size. 8 x 23 x 23\n",
        "            nn.ConvTranspose2d(8, image_nc, kernel_size=6, stride=1, padding=0, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. image_nc x 32 x 32\n",
        "        ]\n",
        "\n",
        "        self.encoder = nn.Sequential(*encoder_lis)\n",
        "        self.bottle_neck = nn.Sequential(*bottle_neck_lis)\n",
        "        self.decoder = nn.Sequential(*decoder_lis)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.bottle_neck(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define a resnet block\n",
        "# modified from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, padding_type='reflect', norm_layer=nn.BatchNorm2d, use_dropout=False, use_bias=False):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
        "\n",
        "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        conv_block = []\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim),\n",
        "                       nn.ReLU(True)]\n",
        "        if use_dropout:\n",
        "            conv_block += [nn.Dropout(0.5)]\n",
        "\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim)]\n",
        "\n",
        "        return nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "        return out\n",
        "    \n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hpfccYcHYRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#If you want to show the traffic sign name\n",
        "import csv\n",
        "\n",
        "sign_names = []\n",
        "with open('signnames.csv', 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "        sign_names.append(row[1])\n",
        "    sign_names.reverse()\n",
        "    sign_names.pop()\n",
        "    sign_names.reverse()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBB4luoDK1ZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2eba6d9d-946a-4a64-c907-ddfc57d7fcd5"
      },
      "source": [
        "#Data loading and preprocessing\n",
        "\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "import keras\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "training_file = 'train.p'\n",
        "testing_file = 'test.p'\n",
        "validation_file = 'valid.p'\n",
        "\n",
        "with open(training_file, mode='rb') as f:\n",
        "    tstrain = pickle.load(f)\n",
        "with open(testing_file, mode='rb') as f:\n",
        "    tstest = pickle.load(f)\n",
        "with open(validation_file, mode='rb') as f:\n",
        "    tsvalid = pickle.load(f)\n",
        "\n",
        "X_train, Y_train = tstrain['features'], tstrain['labels']\n",
        "X_valid, Y_valid = tsvalid['features'], tsvalid['labels']\n",
        "X_test, Y_test = tstest['features'], tstest['labels']\n",
        "\n",
        "#shuffle training set\n",
        "X_train, Y_train = shuffle(X_train, Y_train, random_state=33)\n",
        "X_test, Y_test = shuffle(X_test, Y_test, random_state=33)\n",
        "X_valid, Y_valid = shuffle(X_valid, Y_valid, random_state=33)\n",
        "\n",
        "#grayscale images\n",
        "grayscale = [0.299,0.587,0.144]\n",
        "\n",
        "X_test = np.dot(X_test, grayscale)\n",
        "X_train = np.dot(X_train, grayscale)\n",
        "X_valid = np.dot(X_valid, grayscale)\n",
        "\n",
        "\n",
        "#normalize\n",
        "X_train = np.array(X_train)/255\n",
        "X_test = np.array(X_test)/255\n",
        "X_valid = np.array(X_valid)/255\n",
        "\n",
        "X_train = np.concatenate((X_train,X_valid), axis=0)\n",
        "Y_train = np.concatenate((Y_train,Y_valid), axis=0)\n",
        "\n",
        "#expand dimensions to fit 4D input array\n",
        "X_train = np.expand_dims(X_train,-1)\n",
        "X_test = np.expand_dims(X_test,-1)\n",
        "#X_valid = np.expand_dims(X_valid,-1)\n",
        "\n",
        "X_train = np.transpose(X_train, (0,3,1,2))\n",
        "X_test = np.transpose(X_test, (0,3,1,2))\n",
        "#X_valid = np.transpose(X_valid, (0,3,1,2))\n",
        "\n",
        "assert(len(X_train)==len(Y_train))\n",
        "n_train = len(X_train)\n",
        "assert(len(X_test)==len(Y_test))\n",
        "n_test = len(X_test)\n",
        "\n",
        "#Y_train = Y_train.reshape(Y_train.shape[0],1)\n",
        "#Y_test = Y_test.reshape(Y_test.shape[0],1)\n",
        "#Y_valid = Y_valid.reshape(Y_valid.shape[0],1)\n",
        "Y_train = to_categorical(Y_train, num_classes=43)\n",
        "Y_test = to_categorical(Y_test, num_classes=43)\n",
        "#Y_valid = to_categorical(Y_valid, num_classes=43)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)\n",
        "#print(X_valid.shape)\n",
        "#print(Y_valid.shape)\n",
        "\n",
        "train_x = torch.stack([torch.Tensor(i) for i in X_train])\n",
        "train_y = torch.stack([torch.LongTensor(i) for i in Y_train])\n",
        "\n",
        "test_x = torch.stack([torch.Tensor(i) for i in X_test])\n",
        "test_y = torch.stack([torch.LongTensor(i) for i in Y_test])\n",
        "\n",
        "#valid_x = torch.stack([torch.Tensor(i) for i in X_valid])\n",
        "#valid_y = torch.stack([torch.LongTensor(i) for i in Y_valid])\n",
        "\n",
        "train_dataset = TensorDataset(train_x,train_y)\n",
        "test_dataset = TensorDataset(test_x,test_y)\n",
        "#valid_dataset = TensorDataset(valid_x,valid_y)\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(39209, 1, 32, 32)\n",
            "(39209, 43)\n",
            "(12630, 1, 32, 32)\n",
            "(12630, 43)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90W2JBoXVrZ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1428
        },
        "outputId": "74e6cc7e-6726-400a-eed7-5572434c8e28"
      },
      "source": [
        "#import torch\n",
        "#import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "#import torch.nn.functional as F\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    use_cuda = True\n",
        "    image_nc = 1 #number of channels\n",
        "    batch_size = 64\n",
        "\n",
        "    # Define what device we are using\n",
        "    print(\"CUDA Available: \", torch.cuda.is_available())\n",
        "    device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "    \n",
        "    #Training set\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, \n",
        "                                  batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "\n",
        "    # training the target model\n",
        "    target_model = target_net().to(device)\n",
        "    target_model.train()\n",
        "    opt_model = torch.optim.Adam(target_model.parameters(), lr=0.001)\n",
        "    epochs = 20\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        loss_epoch = 0\n",
        "        num_acc_correct = 0\n",
        "        if epoch == 20:\n",
        "            opt_model = torch.optim.Adam(target_model.parameters(), lr=0.0001)\n",
        "        for i, data in enumerate(train_dataloader, 0):\n",
        "            train_imgs, train_labels = data\n",
        "            train_imgs, train_labels = train_imgs.to(device), train_labels.to(device)\n",
        "            logits_model = target_model(train_imgs)\n",
        "            loss_model = F.cross_entropy(logits_model, torch.max(train_labels,1)[1])\n",
        "            loss_epoch += loss_model\n",
        "            \n",
        "            #stuff for accuracy\n",
        "            train_label = torch.argmax(train_labels,1)\n",
        "            pred_train = torch.argmax(logits_model,1)\n",
        "            num_acc_correct += torch.sum(pred_train==train_label,0)\n",
        "            \n",
        "            opt_model.zero_grad()\n",
        "            loss_model.backward()\n",
        "            opt_model.step()\n",
        "        \n",
        "        print('loss in epoch %d: %f\\n' % (epoch, loss_epoch.item()))\n",
        "        print('Train accuracy so far: %f\\n'%(100*num_acc_correct.item()/len(train_dataset)))\n",
        "\n",
        "    # save model\n",
        "    targeted_model_file_name = './target_model.pth'\n",
        "    torch.save(target_model.state_dict(), targeted_model_file_name)\n",
        "    target_model.eval()\n",
        "\n",
        "    # TESTING\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, \n",
        "                                 shuffle=False, num_workers=1)\n",
        "    \n",
        "    num_correct = 0\n",
        "    for i, data in enumerate(test_dataloader, 0):\n",
        "        test_img, test_label = data\n",
        "        test_img, test_label = test_img.to(device), test_label.to(device)\n",
        "        test_label = torch.argmax(test_label,1)\n",
        "        pred_lab = torch.argmax(target_model(test_img), 1)\n",
        "        num_correct += torch.sum(pred_lab==test_label,0)\n",
        "\n",
        "    print('accuracy in testing set: %f\\n'%(100*num_correct.item()/len(test_dataset)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available:  True\n",
            "loss in epoch 0: 1318.663208\n",
            "\n",
            "Train accuracy so far: 39.626106\n",
            "\n",
            "loss in epoch 1: 389.805115\n",
            "\n",
            "Train accuracy so far: 81.565457\n",
            "\n",
            "loss in epoch 2: 227.544159\n",
            "\n",
            "Train accuracy so far: 89.696243\n",
            "\n",
            "loss in epoch 3: 153.448517\n",
            "\n",
            "Train accuracy so far: 93.282155\n",
            "\n",
            "loss in epoch 4: 113.491829\n",
            "\n",
            "Train accuracy so far: 95.207733\n",
            "\n",
            "loss in epoch 5: 87.343185\n",
            "\n",
            "Train accuracy so far: 96.459996\n",
            "\n",
            "loss in epoch 6: 69.264786\n",
            "\n",
            "Train accuracy so far: 97.189421\n",
            "\n",
            "loss in epoch 7: 56.622807\n",
            "\n",
            "Train accuracy so far: 97.671453\n",
            "\n",
            "loss in epoch 8: 46.641766\n",
            "\n",
            "Train accuracy so far: 98.094825\n",
            "\n",
            "loss in epoch 9: 40.089954\n",
            "\n",
            "Train accuracy so far: 98.311612\n",
            "\n",
            "loss in epoch 10: 34.564842\n",
            "\n",
            "Train accuracy so far: 98.497794\n",
            "\n",
            "loss in epoch 11: 30.219105\n",
            "\n",
            "Train accuracy so far: 98.638068\n",
            "\n",
            "loss in epoch 12: 24.161337\n",
            "\n",
            "Train accuracy so far: 98.916065\n",
            "\n",
            "loss in epoch 13: 24.574877\n",
            "\n",
            "Train accuracy so far: 98.859956\n",
            "\n",
            "loss in epoch 14: 20.119884\n",
            "\n",
            "Train accuracy so far: 99.012982\n",
            "\n",
            "loss in epoch 15: 18.474098\n",
            "\n",
            "Train accuracy so far: 99.145604\n",
            "\n",
            "loss in epoch 16: 16.613565\n",
            "\n",
            "Train accuracy so far: 99.168558\n",
            "\n",
            "loss in epoch 17: 14.407311\n",
            "\n",
            "Train accuracy so far: 99.303731\n",
            "\n",
            "loss in epoch 18: 14.438875\n",
            "\n",
            "Train accuracy so far: 99.252723\n",
            "\n",
            "loss in epoch 19: 11.455852\n",
            "\n",
            "Train accuracy so far: 99.444005\n",
            "\n",
            "accuracy in testing set: 90.554236\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1FSu-q8WRLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "\n",
        "class AdvGAN_Attack:\n",
        "    def __init__(self,\n",
        "                 device,\n",
        "                 model,\n",
        "                 model_num_labels,\n",
        "                 image_nc,\n",
        "                 box_min,\n",
        "                 box_max):\n",
        "        output_nc = image_nc\n",
        "        self.device = device\n",
        "        self.model_num_labels = model_num_labels\n",
        "        self.model = model\n",
        "        self.input_nc = image_nc\n",
        "        self.output_nc = output_nc\n",
        "        self.box_min = box_min\n",
        "        self.box_max = box_max\n",
        "\n",
        "        self.gen_input_nc = image_nc\n",
        "        self.netG = Generator(self.gen_input_nc, image_nc).to(device)\n",
        "        self.netDisc = Discriminator(image_nc).to(device)\n",
        "\n",
        "        # initialize all weights\n",
        "        self.netG.apply(weights_init)\n",
        "        self.netDisc.apply(weights_init)\n",
        "\n",
        "        # initialize optimizers\n",
        "        self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
        "                                            lr=0.001)\n",
        "        self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
        "                                            lr=0.001)\n",
        "\n",
        "    def train_batch(self, x, labels):\n",
        "        # optimize D\n",
        "        for i in range(1):\n",
        "            perturbation = self.netG(x)\n",
        "            pert = 0.03\n",
        "            \n",
        "            # add a clipping trick\n",
        "            adv_images = torch.clamp(perturbation, -pert, pert) + x\n",
        "            adv_images = torch.clamp(adv_images, self.box_min, self.box_max)\n",
        "\n",
        "            self.optimizer_D.zero_grad()\n",
        "            pred_real = self.netDisc(x)\n",
        "            loss_D_real = F.mse_loss(pred_real, torch.ones_like(pred_real, device=self.device))\n",
        "            loss_D_real.backward()\n",
        "\n",
        "            pred_fake = self.netDisc(adv_images.detach())\n",
        "            loss_D_fake = F.mse_loss(pred_fake, torch.zeros_like(pred_fake, device=self.device))\n",
        "            loss_D_fake.backward()\n",
        "            loss_D_GAN = loss_D_fake + loss_D_real\n",
        "            self.optimizer_D.step()\n",
        "\n",
        "        # optimize G\n",
        "        for i in range(1):\n",
        "            self.optimizer_G.zero_grad()\n",
        "\n",
        "            # cal G's loss in GAN\n",
        "            pred_fake = self.netDisc(adv_images)\n",
        "            loss_G_fake = F.mse_loss(pred_fake, torch.ones_like(pred_fake, device=self.device))\n",
        "            loss_G_fake.backward(retain_graph=True)\n",
        "\n",
        "            # calculate perturbation norm\n",
        "            C = 0.1\n",
        "            loss_perturb = torch.mean(torch.norm(perturbation.view(perturbation.shape[0], -1), 2, dim=1))\n",
        "            # loss_perturb = torch.max(loss_perturb - C, torch.zeros(1, device=self.device))\n",
        "            \n",
        "            \n",
        "            # cal adv loss\n",
        "            logits_model = self.model(adv_images)\n",
        "            probs_model = F.softmax(logits_model, dim=1)\n",
        "            #print(\"logits model\")\n",
        "            #print(logits_model.shape)\n",
        "            onehot_labels = labels.type(torch.cuda.FloatTensor)   #torch.eye(self.model_num_labels, device=self.device)[labels]\n",
        "            #print(onehot_labels.shape)\n",
        "            #print(\"one hot labels\")\n",
        "            #print(onehot_labels)\n",
        "            #print(probs_model.shape)\n",
        "            #print('probs_model')\n",
        "            #print(probs_model)\n",
        "\n",
        "            # C&W loss function\n",
        "            \n",
        "            real = torch.sum(onehot_labels * probs_model, dim=1)\n",
        "            other, _ = torch.max((1 - onehot_labels) * probs_model - onehot_labels * 10000, dim=1)\n",
        "            zeros = torch.zeros_like(other)\n",
        "            loss_adv = torch.max(real - other, zeros)\n",
        "            loss_adv = torch.sum(loss_adv)\n",
        "\n",
        "            # maximize cross_entropy loss\n",
        "            # loss_adv = -F.mse_loss(logits_model, onehot_labels)\n",
        "            # loss_adv = - F.cross_entropy(logits_model, labels)\n",
        "\n",
        "            adv_lambda = 5\n",
        "            pert_lambda = 1\n",
        "            loss_G = adv_lambda * loss_adv + pert_lambda * loss_perturb #ADD WEIGHTS TO PERB\n",
        "            loss_G.backward(retain_graph=True)\n",
        "            self.optimizer_G.step()\n",
        "\n",
        "        return loss_D_GAN.item(), loss_G_fake.item(), loss_perturb.item(), loss_adv.item()\n",
        "\n",
        "    def train(self, train_dataloader, epochs):\n",
        "        t0 = time.time()\n",
        "        for epoch in range(1, epochs+1):\n",
        "\n",
        "            if epoch == 50:\n",
        "                self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
        "                                                    lr=0.0001)\n",
        "                self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
        "                                                    lr=0.0001)\n",
        "            if epoch == 80:\n",
        "                self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
        "                                                    lr=0.00001)\n",
        "                self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
        "                                                    lr=0.00001)\n",
        "            loss_D_sum = 0\n",
        "            loss_G_fake_sum = 0\n",
        "            loss_perturb_sum = 0\n",
        "            loss_adv_sum = 0\n",
        "            \n",
        "            \n",
        "            #For accuracy calculation\n",
        "            num_correct = 0\n",
        "             \n",
        "            \n",
        "            for i, data in enumerate(train_dataloader, start=0):\n",
        "                images, labels = data\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "                \n",
        "                perturbation = self.netG(images)\n",
        "                pert = 0.03\n",
        "                # add a clipping trick\n",
        "                adv_images = torch.clamp(perturbation, -pert, pert) + images\n",
        "                adv_images = torch.clamp(adv_images, self.box_min, self.box_max)\n",
        "                \n",
        "\n",
        "                #print('labels shape')\n",
        "                #print(labels.shape)\n",
        "                #print('images shape')\n",
        "                #print(images.shape)\n",
        "                \n",
        "                loss_D_batch, loss_G_fake_batch, loss_perturb_batch, loss_adv_batch = \\\n",
        "                    self.train_batch(images, labels)\n",
        "                loss_D_sum += loss_D_batch\n",
        "                loss_G_fake_sum += loss_G_fake_batch\n",
        "                loss_perturb_sum += loss_perturb_batch\n",
        "                loss_adv_sum += loss_adv_batch\n",
        "                \n",
        "                #calculate accuracy\n",
        "                train_label = torch.argmax(labels,1)\n",
        "                pred_label = torch.argmax(targeted_model(adv_images), 1)\n",
        "                num_correct += torch.sum(pred_label==train_label,0)\n",
        "                \n",
        "\n",
        "            # print statistics\n",
        "            num_batch = len(train_dataloader)\n",
        "            print(\"epoch %d:\\nloss_D: %.3f, loss_G_fake: %.3f,\\\n",
        "             \\nloss_perturb: %.3f, loss_adv: %.3f, \\n\" %\n",
        "                  (epoch, loss_D_sum/num_batch, loss_G_fake_sum/num_batch,\n",
        "                   loss_perturb_sum/num_batch, loss_adv_sum/num_batch))\n",
        "            print('accuracy in training set after perturbation: %f\\n'%(100*num_correct.item()/len(train_dataset)))\n",
        "            \n",
        "            print('{} seconds'.format(time.time() - t0))\n",
        "            #plt.imshow(images[10].cpu().permute(1, 2, 0))\n",
        "            #plt.imshow(adv_images[10].cpu().permute(1, 2, 0))\n",
        "           \n",
        "\n",
        "            # save generator\n",
        "            if epoch%20==0:\n",
        "                netG_file_name = models_path + 'netG_epoch_' + str(epoch) + '.pth'\n",
        "                torch.save(self.netG.state_dict(), netG_file_name)\n",
        "\n",
        "\n",
        "    def test(self, test_dataloader):\n",
        "        num_correct_test = 0\n",
        "        num_correct_all = 0\n",
        "        \n",
        "        miss = [] #defaultdict(list)\n",
        "        \n",
        "        for i, data in enumerate(test_dataloader, 0):\n",
        "            test_img, test_label = data\n",
        "            test_img, test_label = test_img.to(device), test_label.to(device)\n",
        "            #test_img.requires_grad=True\n",
        "            \n",
        "            perturbation = self.netG(test_img)\n",
        "            pert = 0.03\n",
        "            # add a clipping trick\n",
        "            adv_images_test = torch.clamp(perturbation, -pert, pert) + test_img\n",
        "            adv_images_test = torch.clamp(adv_images_test, self.box_min, self.box_max)\n",
        "            pred_label = targeted_model(adv_images_test)\n",
        "            pred_lab_test = torch.argmax(pred_label,1)\n",
        "            \n",
        "            #first see if the model will just get it wrong on the real image\n",
        "            output = targeted_model(test_img)\n",
        "            init_pred = torch.argmax(output,1)\n",
        "            test_label = torch.argmax(test_label,1)\n",
        "            \n",
        "            num_correct_all += torch.sum(pred_lab_test==test_label,0)\n",
        "            \n",
        "            \n",
        "            for j in range(len(init_pred)):\n",
        "            #if so, move on without adding perturbations\n",
        "                if (init_pred[j] != test_label[j]):\n",
        "                    continue\n",
        "                    \n",
        "                else:\n",
        "                    num_correct_test += torch.sum(pred_lab_test[j]==test_label[j],0)\n",
        "            \n",
        "                    if pred_lab_test[j] != test_label[j]:\n",
        "                        if len(miss)<10:\n",
        "                            adv_ex = adv_images_test[j].squeeze().detach().cpu().numpy()\n",
        "                            real_im = test_img[j].squeeze().detach().cpu().numpy()\n",
        "                            miss.append((sign_names[test_label[j]], real_im, sign_names[pred_lab_test[j]], adv_ex))\n",
        "        \n",
        "        print('accuracy in test set on all perturbed images: %f\\n'%(100*num_correct_all.item()/len(test_dataset)))\n",
        "        \n",
        "        print('misclassifications on previously correctly classified images: %f\\n'%(100-(100*num_correct_test.item()/len(test_dataset))))\n",
        "        \n",
        "        #print 10 misclassified images\n",
        "        #ctr=0\n",
        "        for i in range(len(miss)):\n",
        "            real,r_image,fake,f_image = miss[i]\n",
        "            f, axarr = plt.subplots(1,2)\n",
        "            axarr[0].imshow(r_image, cmap='gray')\n",
        "            axarr[0].set_title('Actual: %s'%(real))\n",
        "            axarr[1].imshow(f_image, cmap='gray')\n",
        "            axarr[1].set_title('Classified as: %s'%(fake))\n",
        "            f.tight_layout()\n",
        "            #plt.title(\"{} -> {}\".format(real,fake))\n",
        "            #plt.imshow(image,cmap='gray')\n",
        "\n",
        "            plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jw6eaS3WMlD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "ddd139e2-5e7e-4363-d0bc-439761b50c13"
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "#import torch\n",
        "import torchvision\n",
        "import torchvision.datasets\n",
        "#import torchvision.transforms as transforms\n",
        "#from torch.utils.data import DataLoader\n",
        "\n",
        "use_cuda=True\n",
        "image_nc=1\n",
        "epochs = 100\n",
        "\n",
        "batch_size = 128\n",
        "BOX_MIN = 0\n",
        "BOX_MAX = 1\n",
        "\n",
        "# Define what device we are using\n",
        "print(\"CUDA Available: \",torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "pretrained_model = \"./target_model.pth\"\n",
        "targeted_model = target_net().to(device)\n",
        "targeted_model.load_state_dict(torch.load(pretrained_model))\n",
        "targeted_model.eval()\n",
        "\n",
        "#test to make sure restoring model worked\n",
        "num_correct2 = 0\n",
        "for i, data in enumerate(test_dataloader, 0):\n",
        "    test_img2, test_label2 = data\n",
        "    test_img2, test_label2 = test_img2.to(device), test_label2.to(device)\n",
        "    test_label2 = torch.argmax(test_label2,1)\n",
        "    pred_lab2 = torch.argmax(targeted_model(test_img2), 1)\n",
        "    num_correct2 += torch.sum(pred_lab2==test_label2,0)\n",
        "\n",
        "print('accuracy in testing set: %f\\n'%(100*num_correct2.item()/len(test_dataset)))\n",
        "\n",
        "\n",
        "model_num_labels = 43\n",
        "\n",
        "models_path = \"./models/\"\n",
        "if not os.path.exists(models_path):\n",
        "    os.makedirs(models_path)\n",
        "            \n",
        "# train dataset and dataloader declaration\n",
        "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "advGAN = AdvGAN_Attack(device,\n",
        "                          targeted_model,\n",
        "                          model_num_labels,\n",
        "                          image_nc,\n",
        "                          BOX_MIN,\n",
        "                          BOX_MAX)\n",
        "\n",
        "advGAN.train(dataloader, epochs)\n",
        "\n",
        "advGAN.test(test_dataloader)\n",
        "\n",
        "#ideas for optimizing:\n",
        "#improve target classifier - stop it from overfitting\n",
        "\n",
        "#Stephanie is doing:\n",
        "#color\n",
        "#p 0.005, 0.01, 0.03, 0.05, 0.1\n",
        "\n",
        "#Koosha is doing:\n",
        "#G:D ratio\n",
        "#filters in G\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available:  True\n",
            "accuracy in testing set: 90.554236\n",
            "\n",
            "epoch 1:\n",
            "loss_D: 0.453, loss_G_fake: 0.282,             \n",
            "loss_perturb: 11.464, loss_adv: 82.455, \n",
            "\n",
            "accuracy in training set after perturbation: 67.813512\n",
            "\n",
            "8.2579026222229 seconds\n",
            "epoch 2:\n",
            "loss_D: 0.154, loss_G_fake: 0.601,             \n",
            "loss_perturb: 11.220, loss_adv: 70.804, \n",
            "\n",
            "accuracy in training set after perturbation: 58.708460\n",
            "\n",
            "16.469627380371094 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
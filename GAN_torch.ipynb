{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_torch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tortoisehare/TSR-GAN/blob/master/GAN_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTIpcLONWBq9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "303f9128-8a95-411c-fe64-768034ebdbaa"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#torch.multiprocessing.set_start_method(\"spawn\") \n",
        "\n",
        "import torch.optim\n",
        "print(torch.__version__)\n",
        "#from torch import np\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')       \n",
        "get_ipython().magic('matplotlib inline')\n",
        "from matplotlib import pyplot    \n",
        "from matplotlib.pyplot import subplot     \n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Target Model definition - LeNet 5\n",
        "class target_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(target_net, self).__init__()\n",
        "\n",
        "        # LAYER 1: Convolution, Input 1x32x32, Output 6x28x28\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0, bias=True)\n",
        "        # Max-pooling, Input 6x28x28, Output 6x14x14\n",
        "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Dropout\n",
        "        self.drop1 = torch.nn.Dropout(0.2)\n",
        "       \n",
        "        # LAYER 2: Convolution, Input 6x14x14, Output 16x10x10\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)\n",
        "        # Max-pooling, Input 16x10x10, Output 16x5x5\n",
        "        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=2, stride=2) \n",
        "        # Dropout\n",
        "        self.drop2 = torch.nn.Dropout(0.2)\n",
        "        \n",
        "        # LAYER 3: FC, Input 400, Output 120\n",
        "        self.fc1 = torch.nn.Linear(16*5*5, 120)   # convert matrix with 16*5*5 (= 400) features to a matrix of 120 features (columns)\n",
        "        self.drop3 = torch.nn.Dropout(0.4)\n",
        "        \n",
        "        # LAYER 4: FC, Input 120, Output 84\n",
        "        self.fc2 = torch.nn.Linear(120, 84)       # convert matrix with 120 features to a matrix of 84 features (columns)\n",
        "        self.drop4 = torch.nn.Dropout(0.4)\n",
        "        \n",
        "        # LAYER 5: FC Input 84, Output 43\n",
        "        self.fc3 = torch.nn.Linear(84, 43)        # convert matrix with 84 features to a matrix of 43 features (columns)\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # convolve, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.conv1(x))  \n",
        "        # max-pooling with 2x2 grid \n",
        "        x = self.max_pool_1(x) \n",
        "        # convolve, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.conv2(x))\n",
        "        # max-pooling with 2x2 grid\n",
        "        x = self.max_pool_2(x)\n",
        "        # first flatten 'max_pool_2_out' to contain 16*5*5 columns\n",
        "        # read through https://stackoverflow.com/a/42482819/7551231\n",
        "        \n",
        "        x = x.view(-1, 16*5*5)\n",
        "        # FC-1, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        # FC-2, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.fc2(x))\n",
        "        # FC-3\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, image_nc):\n",
        "        super(Discriminator, self).__init__()\n",
        "        # Traffic Sign Dataset: 1*32x32\n",
        "        model = [\n",
        "            nn.Conv2d(image_nc, 8, kernel_size=4, stride=2, padding=0, bias=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 8*16*16\n",
        "            nn.Conv2d(8, 16, kernel_size=4, stride=2, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 16*5*5\n",
        "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(32, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "            # 32*1*1\n",
        "        ]\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x).squeeze()\n",
        "        return output\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self,\n",
        "                 gen_input_nc,\n",
        "                 image_nc,\n",
        "                 ):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        encoder_lis = [\n",
        "            # Traffic Sign Dataset:1*32x32\n",
        "            nn.Conv2d(gen_input_nc, 8, kernel_size=3, stride=1, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            # 8*26*26\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            # 16*12*12\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            # 32*5*5\n",
        "        ]\n",
        "\n",
        "        bottle_neck_lis = [ResnetBlock(32),\n",
        "                       ResnetBlock(32),\n",
        "                       ResnetBlock(32),\n",
        "                       ResnetBlock(32),]\n",
        "\n",
        "        decoder_lis = [\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=0, bias=False),\n",
        "            nn.InstanceNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            # state size. 16 x 11 x 11\n",
        "            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=0, bias=False),\n",
        "            nn.InstanceNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            # state size. 8 x 23 x 23\n",
        "            nn.ConvTranspose2d(8, image_nc, kernel_size=6, stride=1, padding=0, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. image_nc x 32 x 32\n",
        "        ]\n",
        "\n",
        "        self.encoder = nn.Sequential(*encoder_lis)\n",
        "        self.bottle_neck = nn.Sequential(*bottle_neck_lis)\n",
        "        self.decoder = nn.Sequential(*decoder_lis)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.bottle_neck(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define a resnet block\n",
        "# modified from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, padding_type='reflect', norm_layer=nn.BatchNorm2d, use_dropout=False, use_bias=False):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
        "\n",
        "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        conv_block = []\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim),\n",
        "                       nn.ReLU(True)]\n",
        "        if use_dropout:\n",
        "            conv_block += [nn.Dropout(0.5)]\n",
        "\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim)]\n",
        "\n",
        "        return nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "        return out\n",
        "    \n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBB4luoDK1ZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "216a9e07-d00c-4621-cdc7-4b07b6b83334"
      },
      "source": [
        "#Data loading and preprocessing\n",
        "\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "import keras\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "training_file = 'train.p'\n",
        "testing_file = 'test.p'\n",
        "validation_file = 'valid.p'\n",
        "\n",
        "with open(training_file, mode='rb') as f:\n",
        "    tstrain = pickle.load(f)\n",
        "with open(testing_file, mode='rb') as f:\n",
        "    tstest = pickle.load(f)\n",
        "with open(validation_file, mode='rb') as f:\n",
        "    tsvalid = pickle.load(f)\n",
        "\n",
        "X_train, Y_train = tstrain['features'], tstrain['labels']\n",
        "X_valid, Y_valid = tsvalid['features'], tsvalid['labels']\n",
        "X_test, Y_test = tstest['features'], tstest['labels']\n",
        "\n",
        "#shuffle training set\n",
        "X_train, Y_train = shuffle(X_train, Y_train, random_state=33)\n",
        "X_test, Y_test = shuffle(X_test, Y_test, random_state=33)\n",
        "X_valid, Y_valid = shuffle(X_valid, Y_valid, random_state=33)\n",
        "\n",
        "#grayscale images\n",
        "grayscale = [0.299,0.587,0.144]\n",
        "\n",
        "X_test = np.dot(X_test, grayscale)\n",
        "X_train = np.dot(X_train, grayscale)\n",
        "X_valid = np.dot(X_valid, grayscale)\n",
        "\n",
        "\n",
        "#normalize\n",
        "X_train = np.array(X_train)/255\n",
        "X_test = np.array(X_test)/255\n",
        "X_valid = np.array(X_valid)/255\n",
        "\n",
        "#expand dimensions to fit 4D input array\n",
        "X_train = np.expand_dims(X_train,-1)\n",
        "X_test = np.expand_dims(X_test,-1)\n",
        "X_valid = np.expand_dims(X_valid,-1)\n",
        "\n",
        "X_train = np.transpose(X_train, (0,3,1,2))\n",
        "X_test = np.transpose(X_test, (0,3,1,2))\n",
        "X_valid = np.transpose(X_valid, (0,3,1,2))\n",
        "\n",
        "assert(len(X_train)==len(Y_train))\n",
        "n_train = len(X_train)\n",
        "assert(len(X_test)==len(Y_test))\n",
        "n_test = len(X_test)\n",
        "\n",
        "#Y_train = Y_train.reshape(Y_train.shape[0],1)\n",
        "#Y_test = Y_test.reshape(Y_test.shape[0],1)\n",
        "#Y_valid = Y_valid.reshape(Y_valid.shape[0],1)\n",
        "Y_train = to_categorical(Y_train, num_classes=43)\n",
        "Y_test = to_categorical(Y_test, num_classes=43)\n",
        "Y_valid = to_categorical(Y_valid, num_classes=43)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)\n",
        "print(X_valid.shape)\n",
        "print(Y_valid.shape)\n",
        "\n",
        "train_x = torch.stack([torch.Tensor(i) for i in X_train])\n",
        "train_y = torch.stack([torch.LongTensor(i) for i in Y_train])\n",
        "\n",
        "test_x = torch.stack([torch.Tensor(i) for i in X_test])\n",
        "test_y = torch.stack([torch.LongTensor(i) for i in Y_test])\n",
        "\n",
        "valid_x = torch.stack([torch.Tensor(i) for i in X_valid])\n",
        "valid_y = torch.stack([torch.LongTensor(i) for i in Y_valid])\n",
        "\n",
        "train_dataset = TensorDataset(train_x,train_y)\n",
        "test_dataset = TensorDataset(test_x,test_y)\n",
        "valid_dataset = TensorDataset(valid_x,valid_y)\n",
        "\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(34799, 1, 32, 32)\n",
            "(34799, 43)\n",
            "(12630, 1, 32, 32)\n",
            "(12630, 43)\n",
            "(4410, 1, 32, 32)\n",
            "(4410, 43)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90W2JBoXVrZ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2788
        },
        "outputId": "1e9bbe92-4d4e-4739-9688-1818565aef71"
      },
      "source": [
        "import torch\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    use_cuda = True\n",
        "    image_nc = 1 #number of channels\n",
        "    batch_size = 128\n",
        "\n",
        "    # Define what device we are using\n",
        "    print(\"CUDA Available: \", torch.cuda.is_available())\n",
        "    device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "    \n",
        "    #Training set\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, \n",
        "                                  batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "\n",
        "    # training the target model\n",
        "    target_model = target_net().to(device)\n",
        "    target_model.train()\n",
        "    opt_model = torch.optim.Adam(target_model.parameters(), lr=0.001)\n",
        "    epochs = 40\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        loss_epoch = 0\n",
        "        num_acc_correct = 0\n",
        "        if epoch == 20:\n",
        "            opt_model = torch.optim.Adam(target_model.parameters(), lr=0.0001)\n",
        "        for i, data in enumerate(train_dataloader, 0):\n",
        "            train_imgs, train_labels = data\n",
        "            train_imgs, train_labels = train_imgs.to(device), train_labels.to(device)\n",
        "            logits_model = target_model(train_imgs)\n",
        "            loss_model = F.cross_entropy(logits_model, torch.max(train_labels,1)[1])\n",
        "            loss_epoch += loss_model\n",
        "            \n",
        "            #stuff for accuracy\n",
        "            train_label = torch.argmax(train_labels,1)\n",
        "            pred_train = torch.argmax(logits_model,1)\n",
        "            num_acc_correct += torch.sum(pred_train==train_label,0)\n",
        "            \n",
        "            opt_model.zero_grad()\n",
        "            loss_model.backward()\n",
        "            opt_model.step()\n",
        "        \n",
        "        print('loss in epoch %d: %f\\n' % (epoch, loss_epoch.item()))\n",
        "        print('Train accuracy so far: %f\\n'%(100*num_acc_correct.item()/len(train_dataset)))\n",
        "\n",
        "    # save model\n",
        "    targeted_model_file_name = './target_model.pth'\n",
        "    torch.save(target_model.state_dict(), targeted_model_file_name)\n",
        "    target_model.eval()\n",
        "\n",
        "    # TESTING\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, \n",
        "                                 shuffle=False, num_workers=1)\n",
        "    \n",
        "    num_correct = 0\n",
        "    for i, data in enumerate(test_dataloader, 0):\n",
        "        test_img, test_label = data\n",
        "        test_img, test_label = test_img.to(device), test_label.to(device)\n",
        "        test_label = torch.argmax(test_label,1)\n",
        "        pred_lab = torch.argmax(target_model(test_img), 1)\n",
        "        num_correct += torch.sum(pred_lab==test_label,0)\n",
        "\n",
        "    print('accuracy in testing set: %f\\n'%(100*num_correct.item()/len(test_dataset)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available:  True\n",
            "loss in epoch 0: 794.957947\n",
            "\n",
            "Train accuracy so far: 20.529326\n",
            "\n",
            "loss in epoch 1: 297.946167\n",
            "\n",
            "Train accuracy so far: 68.096784\n",
            "\n",
            "loss in epoch 2: 170.910034\n",
            "\n",
            "Train accuracy so far: 82.100060\n",
            "\n",
            "loss in epoch 3: 118.834618\n",
            "\n",
            "Train accuracy so far: 88.022644\n",
            "\n",
            "loss in epoch 4: 88.450668\n",
            "\n",
            "Train accuracy so far: 91.269864\n",
            "\n",
            "loss in epoch 5: 68.882370\n",
            "\n",
            "Train accuracy so far: 93.347510\n",
            "\n",
            "loss in epoch 6: 54.268536\n",
            "\n",
            "Train accuracy so far: 94.942383\n",
            "\n",
            "loss in epoch 7: 44.038906\n",
            "\n",
            "Train accuracy so far: 95.907928\n",
            "\n",
            "loss in epoch 8: 36.160286\n",
            "\n",
            "Train accuracy so far: 96.770022\n",
            "\n",
            "loss in epoch 9: 30.066370\n",
            "\n",
            "Train accuracy so far: 97.339004\n",
            "\n",
            "loss in epoch 10: 25.820719\n",
            "\n",
            "Train accuracy so far: 97.657979\n",
            "\n",
            "loss in epoch 11: 23.188267\n",
            "\n",
            "Train accuracy so far: 97.876376\n",
            "\n",
            "loss in epoch 12: 21.217857\n",
            "\n",
            "Train accuracy so far: 98.002816\n",
            "\n",
            "loss in epoch 13: 17.420486\n",
            "\n",
            "Train accuracy so far: 98.364896\n",
            "\n",
            "loss in epoch 14: 14.956275\n",
            "\n",
            "Train accuracy so far: 98.603408\n",
            "\n",
            "loss in epoch 15: 13.877322\n",
            "\n",
            "Train accuracy so far: 98.629271\n",
            "\n",
            "loss in epoch 16: 13.387686\n",
            "\n",
            "Train accuracy so far: 98.698238\n",
            "\n",
            "loss in epoch 17: 13.791438\n",
            "\n",
            "Train accuracy so far: 98.614903\n",
            "\n",
            "loss in epoch 18: 11.730803\n",
            "\n",
            "Train accuracy so far: 98.833300\n",
            "\n",
            "loss in epoch 19: 9.066566\n",
            "\n",
            "Train accuracy so far: 99.114917\n",
            "\n",
            "loss in epoch 20: 5.181170\n",
            "\n",
            "Train accuracy so far: 99.554585\n",
            "\n",
            "loss in epoch 21: 3.912695\n",
            "\n",
            "Train accuracy so far: 99.712635\n",
            "\n",
            "loss in epoch 22: 3.463987\n",
            "\n",
            "Train accuracy so far: 99.770108\n",
            "\n",
            "loss in epoch 23: 3.145477\n",
            "\n",
            "Train accuracy so far: 99.798845\n",
            "\n",
            "loss in epoch 24: 2.897637\n",
            "\n",
            "Train accuracy so far: 99.818960\n",
            "\n",
            "loss in epoch 25: 2.674439\n",
            "\n",
            "Train accuracy so far: 99.841949\n",
            "\n",
            "loss in epoch 26: 2.476757\n",
            "\n",
            "Train accuracy so far: 99.856318\n",
            "\n",
            "loss in epoch 27: 2.290430\n",
            "\n",
            "Train accuracy so far: 99.867812\n",
            "\n",
            "loss in epoch 28: 2.119624\n",
            "\n",
            "Train accuracy so far: 99.876433\n",
            "\n",
            "loss in epoch 29: 1.958495\n",
            "\n",
            "Train accuracy so far: 99.885054\n",
            "\n",
            "loss in epoch 30: 1.823571\n",
            "\n",
            "Train accuracy so far: 99.902296\n",
            "\n",
            "loss in epoch 31: 1.691456\n",
            "\n",
            "Train accuracy so far: 99.913791\n",
            "\n",
            "loss in epoch 32: 1.578558\n",
            "\n",
            "Train accuracy so far: 99.925285\n",
            "\n",
            "loss in epoch 33: 1.463388\n",
            "\n",
            "Train accuracy so far: 99.931033\n",
            "\n",
            "loss in epoch 34: 1.363104\n",
            "\n",
            "Train accuracy so far: 99.945401\n",
            "\n",
            "loss in epoch 35: 1.266850\n",
            "\n",
            "Train accuracy so far: 99.954022\n",
            "\n",
            "loss in epoch 36: 1.178910\n",
            "\n",
            "Train accuracy so far: 99.965516\n",
            "\n",
            "loss in epoch 37: 1.096355\n",
            "\n",
            "Train accuracy so far: 99.968390\n",
            "\n",
            "loss in epoch 38: 1.018765\n",
            "\n",
            "Train accuracy so far: 99.971264\n",
            "\n",
            "loss in epoch 39: 0.948079\n",
            "\n",
            "Train accuracy so far: 99.971264\n",
            "\n",
            "accuracy in testing set: 89.319082\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1FSu-q8WRLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import os\n",
        "\n",
        "#models_path = './models/'\n",
        "\n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "\n",
        "class AdvGAN_Attack:\n",
        "    def __init__(self,\n",
        "                 device,\n",
        "                 model,\n",
        "                 model_num_labels,\n",
        "                 image_nc,\n",
        "                 box_min,\n",
        "                 box_max):\n",
        "        output_nc = image_nc\n",
        "        self.device = device\n",
        "        self.model_num_labels = model_num_labels\n",
        "        self.model = model\n",
        "        self.input_nc = image_nc\n",
        "        self.output_nc = output_nc\n",
        "        self.box_min = box_min\n",
        "        self.box_max = box_max\n",
        "\n",
        "        self.gen_input_nc = image_nc\n",
        "        self.netG = Generator(self.gen_input_nc, image_nc).to(device)\n",
        "        self.netDisc = Discriminator(image_nc).to(device)\n",
        "\n",
        "        # initialize all weights\n",
        "        self.netG.apply(weights_init)\n",
        "        self.netDisc.apply(weights_init)\n",
        "\n",
        "        # initialize optimizers\n",
        "        self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
        "                                            lr=0.001)\n",
        "        self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
        "                                            lr=0.001)\n",
        "\n",
        "    def train_batch(self, x, labels):\n",
        "        # optimize D\n",
        "        for i in range(1):\n",
        "            perturbation = self.netG(x)\n",
        "\n",
        "            # add a clipping trick\n",
        "            adv_images = torch.clamp(perturbation, -0.3, 0.3) + x\n",
        "            adv_images = torch.clamp(adv_images, self.box_min, self.box_max)\n",
        "\n",
        "            self.optimizer_D.zero_grad()\n",
        "            pred_real = self.netDisc(x)\n",
        "            loss_D_real = F.mse_loss(pred_real, torch.ones_like(pred_real, device=self.device))\n",
        "            loss_D_real.backward()\n",
        "\n",
        "            pred_fake = self.netDisc(adv_images.detach())\n",
        "            loss_D_fake = F.mse_loss(pred_fake, torch.zeros_like(pred_fake, device=self.device))\n",
        "            loss_D_fake.backward()\n",
        "            loss_D_GAN = loss_D_fake + loss_D_real\n",
        "            self.optimizer_D.step()\n",
        "\n",
        "        # optimize G\n",
        "        for i in range(1):\n",
        "            self.optimizer_G.zero_grad()\n",
        "\n",
        "            # cal G's loss in GAN\n",
        "            pred_fake = self.netDisc(adv_images)\n",
        "            loss_G_fake = F.mse_loss(pred_fake, torch.ones_like(pred_fake, device=self.device))\n",
        "            loss_G_fake.backward(retain_graph=True)\n",
        "\n",
        "            # calculate perturbation norm\n",
        "            C = 0.1\n",
        "            loss_perturb = torch.mean(torch.norm(perturbation.view(perturbation.shape[0], -1), 2, dim=1))\n",
        "            # loss_perturb = torch.max(loss_perturb - C, torch.zeros(1, device=self.device))\n",
        "            \n",
        "            \n",
        "            # cal adv loss\n",
        "            logits_model = self.model(adv_images)\n",
        "            probs_model = F.softmax(logits_model, dim=1)\n",
        "            #print(\"logits model\")\n",
        "            #print(logits_model.shape)\n",
        "            onehot_labels = labels.type(torch.cuda.FloatTensor)   #torch.eye(self.model_num_labels, device=self.device)[labels]\n",
        "            #print(onehot_labels.shape)\n",
        "            #print(\"one hot labels\")\n",
        "            #print(onehot_labels)\n",
        "            #print(probs_model.shape)\n",
        "            #print('probs_model')\n",
        "            #print(probs_model)\n",
        "\n",
        "            # C&W loss function\n",
        "            \n",
        "            real = torch.sum(onehot_labels * probs_model, dim=1)\n",
        "            other, _ = torch.max((1 - onehot_labels) * probs_model - onehot_labels * 10000, dim=1)\n",
        "            zeros = torch.zeros_like(other)\n",
        "            loss_adv = torch.max(real - other, zeros)\n",
        "            loss_adv = torch.sum(loss_adv)\n",
        "\n",
        "            # maximize cross_entropy loss\n",
        "            # loss_adv = -F.mse_loss(logits_model, onehot_labels)\n",
        "            # loss_adv = - F.cross_entropy(logits_model, labels)\n",
        "\n",
        "            adv_lambda = 10\n",
        "            pert_lambda = 1\n",
        "            loss_G = adv_lambda * loss_adv + pert_lambda * loss_perturb\n",
        "            loss_G.backward()\n",
        "            self.optimizer_G.step()\n",
        "\n",
        "        return loss_D_GAN.item(), loss_G_fake.item(), loss_perturb.item(), loss_adv.item()\n",
        "\n",
        "    def train(self, train_dataloader, epochs):\n",
        "        for epoch in range(1, epochs+1):\n",
        "\n",
        "            if epoch == 50:\n",
        "                self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
        "                                                    lr=0.0001)\n",
        "                self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
        "                                                    lr=0.0001)\n",
        "            if epoch == 80:\n",
        "                self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
        "                                                    lr=0.00001)\n",
        "                self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
        "                                                    lr=0.00001)\n",
        "            loss_D_sum = 0\n",
        "            loss_G_fake_sum = 0\n",
        "            loss_perturb_sum = 0\n",
        "            loss_adv_sum = 0\n",
        "            for i, data in enumerate(train_dataloader, start=0):\n",
        "                images, labels = data\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "                #print('labels shape')\n",
        "                #print(labels.shape)\n",
        "                #print('images shape')\n",
        "                #print(images.shape)\n",
        "                \n",
        "                loss_D_batch, loss_G_fake_batch, loss_perturb_batch, loss_adv_batch = \\\n",
        "                    self.train_batch(images, labels)\n",
        "                loss_D_sum += loss_D_batch\n",
        "                loss_G_fake_sum += loss_G_fake_batch\n",
        "                loss_perturb_sum += loss_perturb_batch\n",
        "                loss_adv_sum += loss_adv_batch\n",
        "\n",
        "            # print statistics\n",
        "            num_batch = len(train_dataloader)\n",
        "            print(\"epoch %d:\\nloss_D: %.3f, loss_G_fake: %.3f,\\\n",
        "             \\nloss_perturb: %.3f, loss_adv: %.3f, \\n\" %\n",
        "                  (epoch, loss_D_sum/num_batch, loss_G_fake_sum/num_batch,\n",
        "                   loss_perturb_sum/num_batch, loss_adv_sum/num_batch))\n",
        "\n",
        "            # save generator\n",
        "            if epoch%20==0:\n",
        "                netG_file_name = models_path + 'netG_epoch_' + str(epoch) + '.pth'\n",
        "                torch.save(self.netG.state_dict(), netG_file_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jw6eaS3WMlD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3604
        },
        "outputId": "1f8e0f3e-aebd-4a4c-fba1-3557a614d9d9"
      },
      "source": [
        "import torch\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "use_cuda=True\n",
        "image_nc=1\n",
        "epochs = 60\n",
        "batch_size = 128\n",
        "BOX_MIN = 0\n",
        "BOX_MAX = 1\n",
        "\n",
        "# Define what device we are using\n",
        "print(\"CUDA Available: \",torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "pretrained_model = \"./target_model.pth\"\n",
        "targeted_model = target_net().to(device)\n",
        "targeted_model.load_state_dict(torch.load(pretrained_model))\n",
        "targeted_model.eval()\n",
        "\n",
        "#test to make sure restoring model worked\n",
        "num_correct = 0\n",
        "for i, data in enumerate(test_dataloader, 0):\n",
        "    test_img, test_label = data\n",
        "    test_img, test_label = test_img.to(device), test_label.to(device)\n",
        "    test_label = torch.argmax(test_label,1)\n",
        "    pred_lab = torch.argmax(targeted_model(test_img), 1)\n",
        "    num_correct += torch.sum(pred_lab==test_label,0)\n",
        "\n",
        "print('accuracy in testing set: %f\\n'%(100*num_correct.item()/len(test_dataset)))\n",
        "\n",
        "\n",
        "model_num_labels = 43\n",
        "\n",
        "models_path = \"./models\"\n",
        "if not os.path.exists(models_path):\n",
        "    os.makedirs(models_path)\n",
        "            \n",
        "# train dataset and dataloader declaration\n",
        "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "advGAN = AdvGAN_Attack(device,\n",
        "                          targeted_model,\n",
        "                          model_num_labels,\n",
        "                          image_nc,\n",
        "                          BOX_MIN,\n",
        "                          BOX_MAX)\n",
        "\n",
        "advGAN.train(dataloader, epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available:  True\n",
            "accuracy in testing set: 89.319082\n",
            "\n",
            "epoch 1:\n",
            "loss_D: 0.337, loss_G_fake: 0.382,             \n",
            "loss_perturb: 11.494, loss_adv: 3.634, \n",
            "\n",
            "epoch 2:\n",
            "loss_D: 0.059, loss_G_fake: 0.726,             \n",
            "loss_perturb: 13.931, loss_adv: 1.829, \n",
            "\n",
            "epoch 3:\n",
            "loss_D: 0.014, loss_G_fake: 0.862,             \n",
            "loss_perturb: 13.839, loss_adv: 1.740, \n",
            "\n",
            "epoch 4:\n",
            "loss_D: 0.010, loss_G_fake: 0.890,             \n",
            "loss_perturb: 15.183, loss_adv: 2.931, \n",
            "\n",
            "epoch 5:\n",
            "loss_D: 0.007, loss_G_fake: 0.915,             \n",
            "loss_perturb: 15.661, loss_adv: 2.823, \n",
            "\n",
            "epoch 6:\n",
            "loss_D: 0.008, loss_G_fake: 0.930,             \n",
            "loss_perturb: 15.007, loss_adv: 0.827, \n",
            "\n",
            "epoch 7:\n",
            "loss_D: 0.007, loss_G_fake: 0.938,             \n",
            "loss_perturb: 14.209, loss_adv: 1.160, \n",
            "\n",
            "epoch 8:\n",
            "loss_D: 0.003, loss_G_fake: 0.944,             \n",
            "loss_perturb: 12.869, loss_adv: 1.049, \n",
            "\n",
            "epoch 9:\n",
            "loss_D: 0.001, loss_G_fake: 0.966,             \n",
            "loss_perturb: 13.785, loss_adv: 1.087, \n",
            "\n",
            "epoch 10:\n",
            "loss_D: 0.001, loss_G_fake: 0.975,             \n",
            "loss_perturb: 13.955, loss_adv: 0.909, \n",
            "\n",
            "epoch 11:\n",
            "loss_D: 0.009, loss_G_fake: 0.941,             \n",
            "loss_perturb: 12.365, loss_adv: 0.612, \n",
            "\n",
            "epoch 12:\n",
            "loss_D: 0.001, loss_G_fake: 0.970,             \n",
            "loss_perturb: 12.511, loss_adv: 1.123, \n",
            "\n",
            "epoch 13:\n",
            "loss_D: 0.000, loss_G_fake: 0.981,             \n",
            "loss_perturb: 12.946, loss_adv: 1.221, \n",
            "\n",
            "epoch 14:\n",
            "loss_D: 0.000, loss_G_fake: 0.985,             \n",
            "loss_perturb: 12.004, loss_adv: 1.108, \n",
            "\n",
            "epoch 15:\n",
            "loss_D: 0.000, loss_G_fake: 0.986,             \n",
            "loss_perturb: 10.248, loss_adv: 0.582, \n",
            "\n",
            "epoch 16:\n",
            "loss_D: 0.000, loss_G_fake: 0.984,             \n",
            "loss_perturb: 12.657, loss_adv: 1.134, \n",
            "\n",
            "epoch 17:\n",
            "loss_D: 0.000, loss_G_fake: 0.985,             \n",
            "loss_perturb: 11.765, loss_adv: 1.211, \n",
            "\n",
            "epoch 18:\n",
            "loss_D: 0.000, loss_G_fake: 0.985,             \n",
            "loss_perturb: 13.459, loss_adv: 1.124, \n",
            "\n",
            "epoch 19:\n",
            "loss_D: 0.000, loss_G_fake: 0.987,             \n",
            "loss_perturb: 13.646, loss_adv: 1.066, \n",
            "\n",
            "epoch 20:\n",
            "loss_D: 0.000, loss_G_fake: 0.989,             \n",
            "loss_perturb: 10.480, loss_adv: 0.257, \n",
            "\n",
            "epoch 21:\n",
            "loss_D: 0.000, loss_G_fake: 0.988,             \n",
            "loss_perturb: 9.267, loss_adv: 0.396, \n",
            "\n",
            "epoch 22:\n",
            "loss_D: 0.008, loss_G_fake: 0.982,             \n",
            "loss_perturb: 10.707, loss_adv: 0.676, \n",
            "\n",
            "epoch 23:\n",
            "loss_D: 0.003, loss_G_fake: 0.959,             \n",
            "loss_perturb: 10.449, loss_adv: 0.352, \n",
            "\n",
            "epoch 24:\n",
            "loss_D: 0.001, loss_G_fake: 0.979,             \n",
            "loss_perturb: 9.891, loss_adv: 0.766, \n",
            "\n",
            "epoch 25:\n",
            "loss_D: 0.000, loss_G_fake: 0.989,             \n",
            "loss_perturb: 8.696, loss_adv: 0.413, \n",
            "\n",
            "epoch 26:\n",
            "loss_D: 0.000, loss_G_fake: 0.987,             \n",
            "loss_perturb: 7.948, loss_adv: 0.327, \n",
            "\n",
            "epoch 27:\n",
            "loss_D: 0.002, loss_G_fake: 0.978,             \n",
            "loss_perturb: 9.090, loss_adv: 0.303, \n",
            "\n",
            "epoch 28:\n",
            "loss_D: 0.000, loss_G_fake: 0.983,             \n",
            "loss_perturb: 9.935, loss_adv: 0.380, \n",
            "\n",
            "epoch 29:\n",
            "loss_D: 0.004, loss_G_fake: 0.963,             \n",
            "loss_perturb: 11.628, loss_adv: 0.761, \n",
            "\n",
            "epoch 30:\n",
            "loss_D: 0.001, loss_G_fake: 0.981,             \n",
            "loss_perturb: 11.142, loss_adv: 0.518, \n",
            "\n",
            "epoch 31:\n",
            "loss_D: 0.000, loss_G_fake: 0.989,             \n",
            "loss_perturb: 10.116, loss_adv: 0.343, \n",
            "\n",
            "epoch 32:\n",
            "loss_D: 0.000, loss_G_fake: 0.991,             \n",
            "loss_perturb: 8.585, loss_adv: 0.259, \n",
            "\n",
            "epoch 33:\n",
            "loss_D: 0.000, loss_G_fake: 0.992,             \n",
            "loss_perturb: 9.048, loss_adv: 0.296, \n",
            "\n",
            "epoch 34:\n",
            "loss_D: 0.000, loss_G_fake: 0.994,             \n",
            "loss_perturb: 8.063, loss_adv: 0.215, \n",
            "\n",
            "epoch 35:\n",
            "loss_D: 0.000, loss_G_fake: 0.994,             \n",
            "loss_perturb: 6.593, loss_adv: 0.253, \n",
            "\n",
            "epoch 36:\n",
            "loss_D: 0.000, loss_G_fake: 0.993,             \n",
            "loss_perturb: 7.375, loss_adv: 0.249, \n",
            "\n",
            "epoch 37:\n",
            "loss_D: 0.000, loss_G_fake: 0.996,             \n",
            "loss_perturb: 7.918, loss_adv: 0.238, \n",
            "\n",
            "epoch 38:\n",
            "loss_D: 0.000, loss_G_fake: 0.997,             \n",
            "loss_perturb: 9.032, loss_adv: 0.656, \n",
            "\n",
            "epoch 39:\n",
            "loss_D: 0.000, loss_G_fake: 0.993,             \n",
            "loss_perturb: 9.161, loss_adv: 0.500, \n",
            "\n",
            "epoch 40:\n",
            "loss_D: 0.000, loss_G_fake: 0.995,             \n",
            "loss_perturb: 8.834, loss_adv: 0.213, \n",
            "\n",
            "epoch 41:\n",
            "loss_D: 0.000, loss_G_fake: 0.997,             \n",
            "loss_perturb: 7.440, loss_adv: 0.248, \n",
            "\n",
            "epoch 42:\n",
            "loss_D: 0.000, loss_G_fake: 0.997,             \n",
            "loss_perturb: 7.803, loss_adv: 0.298, \n",
            "\n",
            "epoch 43:\n",
            "loss_D: 0.000, loss_G_fake: 0.996,             \n",
            "loss_perturb: 5.736, loss_adv: 0.177, \n",
            "\n",
            "epoch 44:\n",
            "loss_D: 0.009, loss_G_fake: 0.969,             \n",
            "loss_perturb: 7.318, loss_adv: 0.283, \n",
            "\n",
            "epoch 45:\n",
            "loss_D: 0.001, loss_G_fake: 0.983,             \n",
            "loss_perturb: 7.571, loss_adv: 0.328, \n",
            "\n",
            "epoch 46:\n",
            "loss_D: 0.000, loss_G_fake: 0.992,             \n",
            "loss_perturb: 7.116, loss_adv: 0.203, \n",
            "\n",
            "epoch 47:\n",
            "loss_D: 0.000, loss_G_fake: 0.996,             \n",
            "loss_perturb: 7.027, loss_adv: 0.233, \n",
            "\n",
            "epoch 48:\n",
            "loss_D: 0.000, loss_G_fake: 0.994,             \n",
            "loss_perturb: 5.687, loss_adv: 0.173, \n",
            "\n",
            "epoch 49:\n",
            "loss_D: 0.000, loss_G_fake: 0.993,             \n",
            "loss_perturb: 5.966, loss_adv: 0.197, \n",
            "\n",
            "epoch 50:\n",
            "loss_D: 0.000, loss_G_fake: 0.992,             \n",
            "loss_perturb: 5.927, loss_adv: 0.259, \n",
            "\n",
            "epoch 51:\n",
            "loss_D: 0.000, loss_G_fake: 0.995,             \n",
            "loss_perturb: 5.468, loss_adv: 0.164, \n",
            "\n",
            "epoch 52:\n",
            "loss_D: 0.000, loss_G_fake: 0.996,             \n",
            "loss_perturb: 5.223, loss_adv: 0.138, \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_torch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tortoisehare/TSR-GAN/blob/master/GAN_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DerxOridDA0O",
        "colab_type": "text"
      },
      "source": [
        "pert 0.03, 1:1 G:D, 1x filters, 100 epochs, 90% classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTIpcLONWBq9",
        "colab_type": "code",
        "outputId": "26d56f6e-5cfe-4de4-fa02-b55474e48c23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#torch.multiprocessing.set_start_method(\"spawn\") \n",
        "\n",
        "import torch.optim\n",
        "print(torch.__version__)\n",
        "#from torch import np\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')       \n",
        "get_ipython().magic('matplotlib inline')\n",
        "from matplotlib import pyplot    \n",
        "from matplotlib.pyplot import subplot     \n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Target Model definition - LeNet 5\n",
        "class target_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(target_net, self).__init__()\n",
        "\n",
        "        # LAYER 1: Convolution, Input 1x32x32, Output 6x28x28\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0, bias=True)\n",
        "        # Max-pooling, Input 6x28x28, Output 6x14x14\n",
        "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Dropout\n",
        "        self.drop1 = torch.nn.Dropout(0.5)\n",
        "       \n",
        "        # LAYER 2: Convolution, Input 6x14x14, Output 16x10x10\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)\n",
        "        # Max-pooling, Input 16x10x10, Output 16x5x5\n",
        "        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=2, stride=2) \n",
        "        # Dropout\n",
        "        self.drop2 = torch.nn.Dropout(0.5)\n",
        "        \n",
        "        # LAYER 3: FC, Input 400, Output 120\n",
        "        self.fc1 = torch.nn.Linear(16*5*5, 120)   # convert matrix with 16*5*5 (= 400) features to a matrix of 120 features (columns)\n",
        "        self.drop3 = torch.nn.Dropout(0.5)\n",
        "        \n",
        "        # LAYER 4: FC, Input 120, Output 84\n",
        "        self.fc2 = torch.nn.Linear(120, 84)       # convert matrix with 120 features to a matrix of 84 features (columns)\n",
        "        self.drop4 = torch.nn.Dropout(0.5)\n",
        "        \n",
        "        # LAYER 5: FC Input 84, Output 43\n",
        "        self.fc3 = torch.nn.Linear(84, 43)        # convert matrix with 84 features to a matrix of 43 features (columns)\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # convolve, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.conv1(x))  \n",
        "        # max-pooling with 2x2 grid \n",
        "        x = self.max_pool_1(x) \n",
        "        # convolve, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.conv2(x))\n",
        "        # max-pooling with 2x2 grid\n",
        "        x = self.max_pool_2(x)\n",
        "        # first flatten 'max_pool_2_out' to contain 16*5*5 columns\n",
        "        # read through https://stackoverflow.com/a/42482819/7551231\n",
        "        \n",
        "        x = x.view(-1, 16*5*5)\n",
        "        # FC-1, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        # FC-2, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.fc2(x))\n",
        "        # FC-3\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, image_nc):\n",
        "        super(Discriminator, self).__init__()\n",
        "        # Traffic Sign Dataset: 1*32x32\n",
        "        model = [\n",
        "            nn.Conv2d(image_nc, 8, kernel_size=4, stride=2, padding=0, bias=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 8*16*16\n",
        "            nn.Conv2d(8, 16, kernel_size=4, stride=2, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 16*5*5\n",
        "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(32, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "            # 32*1*1\n",
        "        ]\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x).squeeze()\n",
        "        return output\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self,\n",
        "                 gen_input_nc,\n",
        "                 image_nc,\n",
        "                 ):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        encoder_lis = [\n",
        "            # Traffic Sign Dataset:1*32x32\n",
        "            nn.Conv2d(gen_input_nc, 8, kernel_size=3, stride=1, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            # 8*26*26\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            # 16*12*12\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            # 32*5*5\n",
        "        ]\n",
        "\n",
        "        bottle_neck_lis = [ResnetBlock(32),\n",
        "                       ResnetBlock(32),\n",
        "                       ResnetBlock(32),\n",
        "                       ResnetBlock(32),]\n",
        "\n",
        "        decoder_lis = [\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=0, bias=False),\n",
        "            nn.InstanceNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            # state size. 16 x 11 x 11\n",
        "            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=0, bias=False),\n",
        "            nn.InstanceNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            # state size. 8 x 23 x 23\n",
        "            nn.ConvTranspose2d(8, image_nc, kernel_size=6, stride=1, padding=0, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. image_nc x 32 x 32\n",
        "        ]\n",
        "\n",
        "        self.encoder = nn.Sequential(*encoder_lis)\n",
        "        self.bottle_neck = nn.Sequential(*bottle_neck_lis)\n",
        "        self.decoder = nn.Sequential(*decoder_lis)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.bottle_neck(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define a resnet block\n",
        "# modified from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, padding_type='reflect', norm_layer=nn.BatchNorm2d, use_dropout=False, use_bias=False):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
        "\n",
        "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        conv_block = []\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim),\n",
        "                       nn.ReLU(True)]\n",
        "        if use_dropout:\n",
        "            conv_block += [nn.Dropout(0.5)]\n",
        "\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim)]\n",
        "\n",
        "        return nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "        return out\n",
        "    \n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBB4luoDK1ZR",
        "colab_type": "code",
        "outputId": "351963cf-1a2f-4213-d34e-0208336d7abe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Data loading and preprocessing\n",
        "\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "import keras\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "training_file = 'train.p'\n",
        "testing_file = 'test.p'\n",
        "validation_file = 'valid.p'\n",
        "\n",
        "with open(training_file, mode='rb') as f:\n",
        "    tstrain = pickle.load(f)\n",
        "with open(testing_file, mode='rb') as f:\n",
        "    tstest = pickle.load(f)\n",
        "with open(validation_file, mode='rb') as f:\n",
        "    tsvalid = pickle.load(f)\n",
        "\n",
        "X_train, Y_train = tstrain['features'], tstrain['labels']\n",
        "X_valid, Y_valid = tsvalid['features'], tsvalid['labels']\n",
        "X_test, Y_test = tstest['features'], tstest['labels']\n",
        "\n",
        "#shuffle training set\n",
        "X_train, Y_train = shuffle(X_train, Y_train, random_state=33)\n",
        "X_test, Y_test = shuffle(X_test, Y_test, random_state=33)\n",
        "X_valid, Y_valid = shuffle(X_valid, Y_valid, random_state=33)\n",
        "\n",
        "#grayscale images\n",
        "grayscale = [0.299,0.587,0.144]\n",
        "\n",
        "X_test = np.dot(X_test, grayscale)\n",
        "X_train = np.dot(X_train, grayscale)\n",
        "X_valid = np.dot(X_valid, grayscale)\n",
        "\n",
        "\n",
        "#normalize\n",
        "X_train = np.array(X_train)/255\n",
        "X_test = np.array(X_test)/255\n",
        "X_valid = np.array(X_valid)/255\n",
        "\n",
        "X_train = np.concatenate((X_train,X_valid), axis=0)\n",
        "Y_train = np.concatenate((Y_train,Y_valid), axis=0)\n",
        "\n",
        "#expand dimensions to fit 4D input array\n",
        "X_train = np.expand_dims(X_train,-1)\n",
        "X_test = np.expand_dims(X_test,-1)\n",
        "#X_valid = np.expand_dims(X_valid,-1)\n",
        "\n",
        "X_train = np.transpose(X_train, (0,3,1,2))\n",
        "X_test = np.transpose(X_test, (0,3,1,2))\n",
        "#X_valid = np.transpose(X_valid, (0,3,1,2))\n",
        "\n",
        "assert(len(X_train)==len(Y_train))\n",
        "n_train = len(X_train)\n",
        "assert(len(X_test)==len(Y_test))\n",
        "n_test = len(X_test)\n",
        "\n",
        "#Y_train = Y_train.reshape(Y_train.shape[0],1)\n",
        "#Y_test = Y_test.reshape(Y_test.shape[0],1)\n",
        "#Y_valid = Y_valid.reshape(Y_valid.shape[0],1)\n",
        "Y_train = to_categorical(Y_train, num_classes=43)\n",
        "Y_test = to_categorical(Y_test, num_classes=43)\n",
        "#Y_valid = to_categorical(Y_valid, num_classes=43)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)\n",
        "#print(X_valid.shape)\n",
        "#print(Y_valid.shape)\n",
        "\n",
        "train_x = torch.stack([torch.Tensor(i) for i in X_train])\n",
        "train_y = torch.stack([torch.LongTensor(i) for i in Y_train])\n",
        "\n",
        "test_x = torch.stack([torch.Tensor(i) for i in X_test])\n",
        "test_y = torch.stack([torch.LongTensor(i) for i in Y_test])\n",
        "\n",
        "#valid_x = torch.stack([torch.Tensor(i) for i in X_valid])\n",
        "#valid_y = torch.stack([torch.LongTensor(i) for i in Y_valid])\n",
        "\n",
        "train_dataset = TensorDataset(train_x,train_y)\n",
        "test_dataset = TensorDataset(test_x,test_y)\n",
        "#valid_dataset = TensorDataset(valid_x,valid_y)\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(39209, 1, 32, 32)\n",
            "(39209, 43)\n",
            "(12630, 1, 32, 32)\n",
            "(12630, 43)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90W2JBoXVrZ7",
        "colab_type": "code",
        "outputId": "788a72e6-3c9f-40b5-93d5-6afa4128f244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1428
        }
      },
      "source": [
        "import torch\n",
        "#import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    use_cuda = True\n",
        "    image_nc = 1 #number of channels\n",
        "    batch_size = 64\n",
        "\n",
        "    # Define what device we are using\n",
        "    print(\"CUDA Available: \", torch.cuda.is_available())\n",
        "    device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "    \n",
        "    #Training set\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, \n",
        "                                  batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "\n",
        "    # training the target model\n",
        "    target_model = target_net().to(device)\n",
        "    target_model.train()\n",
        "    opt_model = torch.optim.Adam(target_model.parameters(), lr=0.001)\n",
        "    epochs = 20\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        loss_epoch = 0\n",
        "        num_acc_correct = 0\n",
        "        if epoch == 20:\n",
        "            opt_model = torch.optim.Adam(target_model.parameters(), lr=0.0001)\n",
        "        for i, data in enumerate(train_dataloader, 0):\n",
        "            train_imgs, train_labels = data\n",
        "            train_imgs, train_labels = train_imgs.to(device), train_labels.to(device)\n",
        "            logits_model = target_model(train_imgs)\n",
        "            loss_model = F.cross_entropy(logits_model, torch.max(train_labels,1)[1])\n",
        "            loss_epoch += loss_model\n",
        "            \n",
        "            #stuff for accuracy\n",
        "            train_label = torch.argmax(train_labels,1)\n",
        "            pred_train = torch.argmax(logits_model,1)\n",
        "            num_acc_correct += torch.sum(pred_train==train_label,0)\n",
        "            \n",
        "            opt_model.zero_grad()\n",
        "            loss_model.backward()\n",
        "            opt_model.step()\n",
        "        \n",
        "        print('loss in epoch %d: %f\\n' % (epoch, loss_epoch.item()))\n",
        "        print('Train accuracy so far: %f\\n'%(100*num_acc_correct.item()/len(train_dataset)))\n",
        "\n",
        "    # save model\n",
        "    targeted_model_file_name = './target_model.pth'\n",
        "    torch.save(target_model.state_dict(), targeted_model_file_name)\n",
        "    target_model.eval()\n",
        "\n",
        "    # TESTING\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, \n",
        "                                 shuffle=False, num_workers=1)\n",
        "    \n",
        "    num_correct = 0\n",
        "    for i, data in enumerate(test_dataloader, 0):\n",
        "        test_img, test_label = data\n",
        "        test_img, test_label = test_img.to(device), test_label.to(device)\n",
        "        test_label = torch.argmax(test_label,1)\n",
        "        pred_lab = torch.argmax(target_model(test_img), 1)\n",
        "        num_correct += torch.sum(pred_lab==test_label,0)\n",
        "\n",
        "    print('accuracy in testing set: %f\\n'%(100*num_correct.item()/len(test_dataset)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available:  True\n",
            "loss in epoch 0: 1561.952393\n",
            "\n",
            "Train accuracy so far: 29.613099\n",
            "\n",
            "loss in epoch 1: 479.222107\n",
            "\n",
            "Train accuracy so far: 77.454156\n",
            "\n",
            "loss in epoch 2: 272.211243\n",
            "\n",
            "Train accuracy so far: 87.727308\n",
            "\n",
            "loss in epoch 3: 188.802460\n",
            "\n",
            "Train accuracy so far: 91.728940\n",
            "\n",
            "loss in epoch 4: 136.501038\n",
            "\n",
            "Train accuracy so far: 94.248769\n",
            "\n",
            "loss in epoch 5: 103.341858\n",
            "\n",
            "Train accuracy so far: 95.710169\n",
            "\n",
            "loss in epoch 6: 81.990112\n",
            "\n",
            "Train accuracy so far: 96.623224\n",
            "\n",
            "loss in epoch 7: 67.004097\n",
            "\n",
            "Train accuracy so far: 97.242980\n",
            "\n",
            "loss in epoch 8: 54.588589\n",
            "\n",
            "Train accuracy so far: 97.694407\n",
            "\n",
            "loss in epoch 9: 46.107780\n",
            "\n",
            "Train accuracy so far: 98.005560\n",
            "\n",
            "loss in epoch 10: 41.337803\n",
            "\n",
            "Train accuracy so far: 98.196843\n",
            "\n",
            "loss in epoch 11: 33.469933\n",
            "\n",
            "Train accuracy so far: 98.528399\n",
            "\n",
            "loss in epoch 12: 28.778025\n",
            "\n",
            "Train accuracy so far: 98.699278\n",
            "\n",
            "loss in epoch 13: 24.141302\n",
            "\n",
            "Train accuracy so far: 98.977276\n",
            "\n",
            "loss in epoch 14: 21.188192\n",
            "\n",
            "Train accuracy so far: 99.094596\n",
            "\n",
            "loss in epoch 15: 19.811047\n",
            "\n",
            "Train accuracy so far: 99.150705\n",
            "\n",
            "loss in epoch 16: 21.756731\n",
            "\n",
            "Train accuracy so far: 98.967074\n",
            "\n",
            "loss in epoch 17: 19.407175\n",
            "\n",
            "Train accuracy so far: 99.097146\n",
            "\n",
            "loss in epoch 18: 15.292929\n",
            "\n",
            "Train accuracy so far: 99.265475\n",
            "\n",
            "loss in epoch 19: 16.703007\n",
            "\n",
            "Train accuracy so far: 99.145604\n",
            "\n",
            "accuracy in testing set: 90.221694\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1FSu-q8WRLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#import matplotlib.gridspec as gridspec\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "#import numpy as np\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import os\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "#models_path = './models/'\n",
        "\n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "\n",
        "class AdvGAN_Attack:\n",
        "    def __init__(self,\n",
        "                 device,\n",
        "                 model,\n",
        "                 model_num_labels,\n",
        "                 image_nc,\n",
        "                 box_min,\n",
        "                 box_max):\n",
        "        output_nc = image_nc\n",
        "        self.device = device\n",
        "        self.model_num_labels = model_num_labels\n",
        "        self.model = model\n",
        "        self.input_nc = image_nc\n",
        "        self.output_nc = output_nc\n",
        "        self.box_min = box_min\n",
        "        self.box_max = box_max\n",
        "\n",
        "        self.gen_input_nc = image_nc\n",
        "        self.netG = Generator(self.gen_input_nc, image_nc).to(device)\n",
        "        self.netDisc = Discriminator(image_nc).to(device)\n",
        "\n",
        "        # initialize all weights\n",
        "        self.netG.apply(weights_init)\n",
        "        self.netDisc.apply(weights_init)\n",
        "\n",
        "        # initialize optimizers\n",
        "        self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
        "                                            lr=0.001)\n",
        "        self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
        "                                            lr=0.001)\n",
        "\n",
        "    def train_batch(self, x, labels):\n",
        "        # optimize D\n",
        "        for i in range(1):\n",
        "            perturbation = self.netG(x)\n",
        "            pert = 0.03\n",
        "            \n",
        "            # add a clipping trick\n",
        "            adv_images = torch.clamp(perturbation, -pert, pert) + x\n",
        "            adv_images = torch.clamp(adv_images, self.box_min, self.box_max)\n",
        "\n",
        "            self.optimizer_D.zero_grad()\n",
        "            pred_real = self.netDisc(x)\n",
        "            loss_D_real = F.mse_loss(pred_real, torch.ones_like(pred_real, device=self.device))\n",
        "            loss_D_real.backward()\n",
        "\n",
        "            pred_fake = self.netDisc(adv_images.detach())\n",
        "            loss_D_fake = F.mse_loss(pred_fake, torch.zeros_like(pred_fake, device=self.device))\n",
        "            loss_D_fake.backward()\n",
        "            loss_D_GAN = loss_D_fake + loss_D_real\n",
        "            self.optimizer_D.step()\n",
        "\n",
        "        # optimize G\n",
        "        for i in range(1):\n",
        "            self.optimizer_G.zero_grad()\n",
        "\n",
        "            # cal G's loss in GAN\n",
        "            pred_fake = self.netDisc(adv_images)\n",
        "            loss_G_fake = F.mse_loss(pred_fake, torch.ones_like(pred_fake, device=self.device))\n",
        "            loss_G_fake.backward(retain_graph=True)\n",
        "\n",
        "            # calculate perturbation norm\n",
        "            C = 0.1\n",
        "            loss_perturb = torch.mean(torch.norm(perturbation.view(perturbation.shape[0], -1), 2, dim=1))\n",
        "            # loss_perturb = torch.max(loss_perturb - C, torch.zeros(1, device=self.device))\n",
        "            \n",
        "            \n",
        "            # cal adv loss\n",
        "            logits_model = self.model(adv_images)\n",
        "            probs_model = F.softmax(logits_model, dim=1)\n",
        "            #print(\"logits model\")\n",
        "            #print(logits_model.shape)\n",
        "            onehot_labels = labels.type(torch.cuda.FloatTensor)   #torch.eye(self.model_num_labels, device=self.device)[labels]\n",
        "            #print(onehot_labels.shape)\n",
        "            #print(\"one hot labels\")\n",
        "            #print(onehot_labels)\n",
        "            #print(probs_model.shape)\n",
        "            #print('probs_model')\n",
        "            #print(probs_model)\n",
        "\n",
        "            # C&W loss function\n",
        "            \n",
        "            real = torch.sum(onehot_labels * probs_model, dim=1)\n",
        "            other, _ = torch.max((1 - onehot_labels) * probs_model - onehot_labels * 10000, dim=1)\n",
        "            zeros = torch.zeros_like(other)\n",
        "            loss_adv = torch.max(real - other, zeros)\n",
        "            loss_adv = torch.sum(loss_adv)\n",
        "\n",
        "            # maximize cross_entropy loss\n",
        "            # loss_adv = -F.mse_loss(logits_model, onehot_labels)\n",
        "            # loss_adv = - F.cross_entropy(logits_model, labels)\n",
        "\n",
        "            adv_lambda = 5\n",
        "            pert_lambda = 1\n",
        "            loss_G = adv_lambda * loss_adv + pert_lambda * loss_perturb #ADD WEIGHTS TO PERB\n",
        "            loss_G.backward(retain_graph=True)\n",
        "            self.optimizer_G.step()\n",
        "\n",
        "        return loss_D_GAN.item(), loss_G_fake.item(), loss_perturb.item(), loss_adv.item()\n",
        "\n",
        "    def train(self, train_dataloader, epochs):\n",
        "        t0 = time.time()\n",
        "        for epoch in range(1, epochs+1):\n",
        "\n",
        "            if epoch == 50:\n",
        "                self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
        "                                                    lr=0.0001)\n",
        "                self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
        "                                                    lr=0.0001)\n",
        "            if epoch == 80:\n",
        "                self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
        "                                                    lr=0.00001)\n",
        "                self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
        "                                                    lr=0.00001)\n",
        "            loss_D_sum = 0\n",
        "            loss_G_fake_sum = 0\n",
        "            loss_perturb_sum = 0\n",
        "            loss_adv_sum = 0\n",
        "            \n",
        "            \n",
        "            #For accuracy calculation\n",
        "            num_correct = 0\n",
        "             \n",
        "            \n",
        "            for i, data in enumerate(train_dataloader, start=0):\n",
        "                images, labels = data\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "                \n",
        "                perturbation = self.netG(images)\n",
        "                # add a clipping trick\n",
        "                adv_images = torch.clamp(perturbation, -0.01, 0.01) + images\n",
        "                adv_images = torch.clamp(adv_images, self.box_min, self.box_max)\n",
        "                \n",
        "\n",
        "                #print('labels shape')\n",
        "                #print(labels.shape)\n",
        "                #print('images shape')\n",
        "                #print(images.shape)\n",
        "                \n",
        "                loss_D_batch, loss_G_fake_batch, loss_perturb_batch, loss_adv_batch = \\\n",
        "                    self.train_batch(images, labels)\n",
        "                loss_D_sum += loss_D_batch\n",
        "                loss_G_fake_sum += loss_G_fake_batch\n",
        "                loss_perturb_sum += loss_perturb_batch\n",
        "                loss_adv_sum += loss_adv_batch\n",
        "                \n",
        "                #calculate accuracy\n",
        "                train_label = torch.argmax(labels,1)\n",
        "                pred_label = torch.argmax(targeted_model(adv_images), 1)\n",
        "                num_correct += torch.sum(pred_label==train_label,0)\n",
        "                \n",
        "\n",
        "            # print statistics\n",
        "            num_batch = len(train_dataloader)\n",
        "            print(\"epoch %d:\\nloss_D: %.3f, loss_G_fake: %.3f,\\\n",
        "             \\nloss_perturb: %.3f, loss_adv: %.3f, \\n\" %\n",
        "                  (epoch, loss_D_sum/num_batch, loss_G_fake_sum/num_batch,\n",
        "                   loss_perturb_sum/num_batch, loss_adv_sum/num_batch))\n",
        "            print('accuracy in training set after perturbation: %f\\n'%(100*num_correct.item()/len(train_dataset)))\n",
        "            \n",
        "            print('{} seconds'.format(time.time() - t0))\n",
        "            #plt.imshow(images[10].cpu().permute(1, 2, 0))\n",
        "            #plt.imshow(adv_images[10].cpu().permute(1, 2, 0))\n",
        "           \n",
        "\n",
        "            # save generator\n",
        "            if epoch%20==0:\n",
        "                netG_file_name = models_path + 'netG_epoch_' + str(epoch) + '.pth'\n",
        "                torch.save(self.netG.state_dict(), netG_file_name)\n",
        "\n",
        "\n",
        "    def test(self, test_dataloader):\n",
        "        num_correct_test = 0\n",
        "        num_correct_all = 0\n",
        "        \n",
        "        miss = [] #defaultdict(list)\n",
        "        \n",
        "        for i, data in enumerate(test_dataloader, 0):\n",
        "            test_img, test_label = data\n",
        "            test_img, test_label = test_img.to(device), test_label.to(device)\n",
        "            #test_img.requires_grad=True\n",
        "            \n",
        "            perturbation = self.netG(test_img)\n",
        "            pert = 0.03\n",
        "            # add a clipping trick\n",
        "            adv_images_test = torch.clamp(perturbation, -pert, pert) + test_img\n",
        "            adv_images_test = torch.clamp(adv_images_test, self.box_min, self.box_max)\n",
        "            pred_label = targeted_model(adv_images_test)\n",
        "            pred_lab_test = torch.argmax(pred_label,1)\n",
        "            \n",
        "            #first see if the model will just get it wrong on the real image\n",
        "            output = targeted_model(test_img)\n",
        "            init_pred = torch.argmax(output,1)\n",
        "            test_label = torch.argmax(test_label,1)\n",
        "            \n",
        "            num_correct_all += torch.sum(pred_lab_test==test_label,0)\n",
        "            \n",
        "            \n",
        "            for j in range(len(init_pred)):\n",
        "            #if so, move on without adding perturbations\n",
        "                if (init_pred[j] != test_label[j]):\n",
        "                    continue\n",
        "                    \n",
        "                else:\n",
        "                    num_correct_test += torch.sum(pred_lab_test[j]==test_label[j],0)\n",
        "            \n",
        "                    if pred_lab_test[j] != test_label[j]:\n",
        "                        if len(miss)<10:\n",
        "                            adv_ex = adv_images_test[j].squeeze().detach().cpu().numpy()\n",
        "                            real_im = test_img[j].squeeze().detach().cpu().numpy()\n",
        "                            miss.append((test_label[j], real_im, pred_lab_test[j], adv_ex))\n",
        "        \n",
        "        print('accuracy in test set on all perturbed images: %f\\n'%(100*num_correct_all.item()/len(test_dataset)))\n",
        "        \n",
        "        print('misclassifications on previously correctly classified images: %f\\n'%(100-(100*num_correct_test.item()/len(test_dataset))))\n",
        "        \n",
        "        #print 10 misclassified images\n",
        "        #ctr=0\n",
        "        for i in range(len(miss)):\n",
        "            real,r_image,fake,f_image = miss[i]\n",
        "            f, axarr = plt.subplots(1,2)\n",
        "            axarr[0].imshow(r_image, cmap='gray')\n",
        "            axarr[0].set_title('Real Label: %d'%(real))\n",
        "            axarr[1].imshow(f_image, cmap='gray')\n",
        "            axarr[1].set_title('Classified Label: %d'%(fake))\n",
        "            #plt.title(\"{} -> {}\".format(real,fake))\n",
        "            #plt.imshow(image,cmap='gray')\n",
        "\n",
        "            plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jw6eaS3WMlD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6137
        },
        "outputId": "6f59d2d5-089b-467e-ee0a-b8c4ed20be07"
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "use_cuda=True\n",
        "image_nc=1\n",
        "epochs = 100\n",
        "\n",
        "batch_size = 128\n",
        "BOX_MIN = 0\n",
        "BOX_MAX = 1\n",
        "\n",
        "# Define what device we are using\n",
        "print(\"CUDA Available: \",torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "pretrained_model = \"./target_model.pth\"\n",
        "targeted_model = target_net().to(device)\n",
        "targeted_model.load_state_dict(torch.load(pretrained_model))\n",
        "targeted_model.eval()\n",
        "\n",
        "#test to make sure restoring model worked\n",
        "num_correct2 = 0\n",
        "for i, data in enumerate(test_dataloader, 0):\n",
        "    test_img2, test_label2 = data\n",
        "    test_img2, test_label2 = test_img2.to(device), test_label2.to(device)\n",
        "    test_label2 = torch.argmax(test_label2,1)\n",
        "    pred_lab2 = torch.argmax(targeted_model(test_img2), 1)\n",
        "    num_correct2 += torch.sum(pred_lab2==test_label2,0)\n",
        "\n",
        "print('accuracy in testing set: %f\\n'%(100*num_correct2.item()/len(test_dataset)))\n",
        "\n",
        "\n",
        "model_num_labels = 43\n",
        "\n",
        "models_path = \"./models/\"\n",
        "if not os.path.exists(models_path):\n",
        "    os.makedirs(models_path)\n",
        "            \n",
        "# train dataset and dataloader declaration\n",
        "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "advGAN = AdvGAN_Attack(device,\n",
        "                          targeted_model,\n",
        "                          model_num_labels,\n",
        "                          image_nc,\n",
        "                          BOX_MIN,\n",
        "                          BOX_MAX)\n",
        "\n",
        "advGAN.train(dataloader, epochs)\n",
        "\n",
        "advGAN.test(test_dataloader)\n",
        "\n",
        "#ideas for optimizing:\n",
        "#improve target classifier - stop it from overfitting\n",
        "#color\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available:  True\n",
            "accuracy in testing set: 90.221694\n",
            "\n",
            "epoch 1:\n",
            "loss_D: 0.445, loss_G_fake: 0.289,             \n",
            "loss_perturb: 11.489, loss_adv: 92.161, \n",
            "\n",
            "accuracy in training set after perturbation: 92.904690\n",
            "\n",
            "8.720333814620972 seconds\n",
            "epoch 2:\n",
            "loss_D: 0.293, loss_G_fake: 0.420,             \n",
            "loss_perturb: 14.017, loss_adv: 80.329, \n",
            "\n",
            "accuracy in training set after perturbation: 90.596547\n",
            "\n",
            "17.470893383026123 seconds\n",
            "epoch 3:\n",
            "loss_D: 0.173, loss_G_fake: 0.556,             \n",
            "loss_perturb: 13.709, loss_adv: 74.415, \n",
            "\n",
            "accuracy in training set after perturbation: 88.954067\n",
            "\n",
            "26.12267017364502 seconds\n",
            "epoch 4:\n",
            "loss_D: 0.126, loss_G_fake: 0.629,             \n",
            "loss_perturb: 11.729, loss_adv: 71.395, \n",
            "\n",
            "accuracy in training set after perturbation: 87.997654\n",
            "\n",
            "34.735891580581665 seconds\n",
            "epoch 5:\n",
            "loss_D: 0.103, loss_G_fake: 0.670,             \n",
            "loss_perturb: 10.138, loss_adv: 68.302, \n",
            "\n",
            "accuracy in training set after perturbation: 86.666327\n",
            "\n",
            "43.34782886505127 seconds\n",
            "epoch 6:\n",
            "loss_D: 0.098, loss_G_fake: 0.689,             \n",
            "loss_perturb: 9.838, loss_adv: 66.180, \n",
            "\n",
            "accuracy in training set after perturbation: 85.964957\n",
            "\n",
            "51.983845233917236 seconds\n",
            "epoch 7:\n",
            "loss_D: 0.086, loss_G_fake: 0.713,             \n",
            "loss_perturb: 9.253, loss_adv: 63.572, \n",
            "\n",
            "accuracy in training set after perturbation: 85.102910\n",
            "\n",
            "60.599342584609985 seconds\n",
            "epoch 8:\n",
            "loss_D: 0.085, loss_G_fake: 0.723,             \n",
            "loss_perturb: 8.648, loss_adv: 60.671, \n",
            "\n",
            "accuracy in training set after perturbation: 84.182203\n",
            "\n",
            "69.44682908058167 seconds\n",
            "epoch 9:\n",
            "loss_D: 0.057, loss_G_fake: 0.781,             \n",
            "loss_perturb: 7.667, loss_adv: 57.658, \n",
            "\n",
            "accuracy in training set after perturbation: 83.327807\n",
            "\n",
            "78.19478607177734 seconds\n",
            "epoch 10:\n",
            "loss_D: 0.041, loss_G_fake: 0.814,             \n",
            "loss_perturb: 7.463, loss_adv: 56.022, \n",
            "\n",
            "accuracy in training set after perturbation: 82.774363\n",
            "\n",
            "86.815269947052 seconds\n",
            "epoch 11:\n",
            "loss_D: 0.040, loss_G_fake: 0.826,             \n",
            "loss_perturb: 7.673, loss_adv: 54.260, \n",
            "\n",
            "accuracy in training set after perturbation: 82.195414\n",
            "\n",
            "95.39384508132935 seconds\n",
            "epoch 12:\n",
            "loss_D: 0.039, loss_G_fake: 0.829,             \n",
            "loss_perturb: 7.141, loss_adv: 52.379, \n",
            "\n",
            "accuracy in training set after perturbation: 81.715933\n",
            "\n",
            "104.00347232818604 seconds\n",
            "epoch 13:\n",
            "loss_D: 0.032, loss_G_fake: 0.848,             \n",
            "loss_perturb: 7.035, loss_adv: 50.662, \n",
            "\n",
            "accuracy in training set after perturbation: 81.111480\n",
            "\n",
            "112.6577467918396 seconds\n",
            "epoch 14:\n",
            "loss_D: 0.027, loss_G_fake: 0.866,             \n",
            "loss_perturb: 6.657, loss_adv: 48.838, \n",
            "\n",
            "accuracy in training set after perturbation: 80.624346\n",
            "\n",
            "121.31603622436523 seconds\n",
            "epoch 15:\n",
            "loss_D: 0.025, loss_G_fake: 0.872,             \n",
            "loss_perturb: 6.742, loss_adv: 47.834, \n",
            "\n",
            "accuracy in training set after perturbation: 80.741666\n",
            "\n",
            "129.92645907402039 seconds\n",
            "epoch 16:\n",
            "loss_D: 0.022, loss_G_fake: 0.885,             \n",
            "loss_perturb: 6.813, loss_adv: 47.269, \n",
            "\n",
            "accuracy in training set after perturbation: 80.616695\n",
            "\n",
            "138.57829070091248 seconds\n",
            "epoch 17:\n",
            "loss_D: 0.014, loss_G_fake: 0.910,             \n",
            "loss_perturb: 6.589, loss_adv: 46.372, \n",
            "\n",
            "accuracy in training set after perturbation: 80.422862\n",
            "\n",
            "147.3017556667328 seconds\n",
            "epoch 18:\n",
            "loss_D: 0.013, loss_G_fake: 0.916,             \n",
            "loss_perturb: 6.358, loss_adv: 44.821, \n",
            "\n",
            "accuracy in training set after perturbation: 79.963784\n",
            "\n",
            "155.98542547225952 seconds\n",
            "epoch 19:\n",
            "loss_D: 0.011, loss_G_fake: 0.921,             \n",
            "loss_perturb: 6.015, loss_adv: 43.749, \n",
            "\n",
            "accuracy in training set after perturbation: 79.897473\n",
            "\n",
            "164.5711305141449 seconds\n",
            "epoch 20:\n",
            "loss_D: 0.014, loss_G_fake: 0.922,             \n",
            "loss_perturb: 6.190, loss_adv: 43.607, \n",
            "\n",
            "accuracy in training set after perturbation: 79.884720\n",
            "\n",
            "173.22501587867737 seconds\n",
            "epoch 21:\n",
            "loss_D: 0.012, loss_G_fake: 0.926,             \n",
            "loss_perturb: 6.375, loss_adv: 42.637, \n",
            "\n",
            "accuracy in training set after perturbation: 79.777602\n",
            "\n",
            "181.89277839660645 seconds\n",
            "epoch 22:\n",
            "loss_D: 0.007, loss_G_fake: 0.943,             \n",
            "loss_perturb: 6.045, loss_adv: 42.021, \n",
            "\n",
            "accuracy in training set after perturbation: 79.627126\n",
            "\n",
            "190.52479124069214 seconds\n",
            "epoch 23:\n",
            "loss_D: 0.008, loss_G_fake: 0.943,             \n",
            "loss_perturb: 5.857, loss_adv: 41.268, \n",
            "\n",
            "accuracy in training set after perturbation: 79.165498\n",
            "\n",
            "199.21433687210083 seconds\n",
            "epoch 24:\n",
            "loss_D: 0.008, loss_G_fake: 0.944,             \n",
            "loss_perturb: 5.768, loss_adv: 40.129, \n",
            "\n",
            "accuracy in training set after perturbation: 78.678365\n",
            "\n",
            "207.9106571674347 seconds\n",
            "epoch 25:\n",
            "loss_D: 0.013, loss_G_fake: 0.933,             \n",
            "loss_perturb: 5.575, loss_adv: 39.604, \n",
            "\n",
            "accuracy in training set after perturbation: 78.810987\n",
            "\n",
            "216.56492686271667 seconds\n",
            "epoch 26:\n",
            "loss_D: 0.005, loss_G_fake: 0.954,             \n",
            "loss_perturb: 5.354, loss_adv: 39.202, \n",
            "\n",
            "accuracy in training set after perturbation: 78.747226\n",
            "\n",
            "225.30530309677124 seconds\n",
            "epoch 27:\n",
            "loss_D: 0.006, loss_G_fake: 0.956,             \n",
            "loss_perturb: 5.395, loss_adv: 38.888, \n",
            "\n",
            "accuracy in training set after perturbation: 78.752327\n",
            "\n",
            "234.14798951148987 seconds\n",
            "epoch 28:\n",
            "loss_D: 0.004, loss_G_fake: 0.963,             \n",
            "loss_perturb: 5.511, loss_adv: 38.645, \n",
            "\n",
            "accuracy in training set after perturbation: 78.660512\n",
            "\n",
            "243.09563517570496 seconds\n",
            "epoch 29:\n",
            "loss_D: 0.007, loss_G_fake: 0.957,             \n",
            "loss_perturb: 5.796, loss_adv: 38.834, \n",
            "\n",
            "accuracy in training set after perturbation: 78.586549\n",
            "\n",
            "251.8145592212677 seconds\n",
            "epoch 30:\n",
            "loss_D: 0.010, loss_G_fake: 0.949,             \n",
            "loss_perturb: 5.234, loss_adv: 38.042, \n",
            "\n",
            "accuracy in training set after perturbation: 78.402918\n",
            "\n",
            "260.3631694316864 seconds\n",
            "epoch 31:\n",
            "loss_D: 0.004, loss_G_fake: 0.964,             \n",
            "loss_perturb: 5.583, loss_adv: 37.313, \n",
            "\n",
            "accuracy in training set after perturbation: 78.265194\n",
            "\n",
            "268.9372594356537 seconds\n",
            "epoch 32:\n",
            "loss_D: 0.001, loss_G_fake: 0.975,             \n",
            "loss_perturb: 5.341, loss_adv: 37.262, \n",
            "\n",
            "accuracy in training set after perturbation: 78.244791\n",
            "\n",
            "277.46226811408997 seconds\n",
            "epoch 33:\n",
            "loss_D: 0.010, loss_G_fake: 0.953,             \n",
            "loss_perturb: 5.209, loss_adv: 36.778, \n",
            "\n",
            "accuracy in training set after perturbation: 78.152975\n",
            "\n",
            "285.9472908973694 seconds\n",
            "epoch 34:\n",
            "loss_D: 0.004, loss_G_fake: 0.968,             \n",
            "loss_perturb: 5.236, loss_adv: 36.486, \n",
            "\n",
            "accuracy in training set after perturbation: 78.114719\n",
            "\n",
            "294.4504780769348 seconds\n",
            "epoch 35:\n",
            "loss_D: 0.004, loss_G_fake: 0.968,             \n",
            "loss_perturb: 5.130, loss_adv: 35.908, \n",
            "\n",
            "accuracy in training set after perturbation: 78.135122\n",
            "\n",
            "303.01966285705566 seconds\n",
            "epoch 36:\n",
            "loss_D: 0.003, loss_G_fake: 0.970,             \n",
            "loss_perturb: 4.882, loss_adv: 35.520, \n",
            "\n",
            "accuracy in training set after perturbation: 77.737254\n",
            "\n",
            "311.7093312740326 seconds\n",
            "epoch 37:\n",
            "loss_D: 0.001, loss_G_fake: 0.977,             \n",
            "loss_perturb: 4.789, loss_adv: 35.232, \n",
            "\n",
            "accuracy in training set after perturbation: 77.607182\n",
            "\n",
            "320.2425580024719 seconds\n",
            "epoch 38:\n",
            "loss_D: 0.001, loss_G_fake: 0.981,             \n",
            "loss_perturb: 4.768, loss_adv: 35.257, \n",
            "\n",
            "accuracy in training set after perturbation: 77.798465\n",
            "\n",
            "328.8048357963562 seconds\n",
            "epoch 39:\n",
            "loss_D: 0.005, loss_G_fake: 0.968,             \n",
            "loss_perturb: 4.967, loss_adv: 35.114, \n",
            "\n",
            "accuracy in training set after perturbation: 77.821419\n",
            "\n",
            "337.31514406204224 seconds\n",
            "epoch 40:\n",
            "loss_D: 0.002, loss_G_fake: 0.975,             \n",
            "loss_perturb: 4.747, loss_adv: 34.613, \n",
            "\n",
            "accuracy in training set after perturbation: 77.502614\n",
            "\n",
            "345.8675539493561 seconds\n",
            "epoch 41:\n",
            "loss_D: 0.004, loss_G_fake: 0.971,             \n",
            "loss_perturb: 4.765, loss_adv: 34.552, \n",
            "\n",
            "accuracy in training set after perturbation: 77.632686\n",
            "\n",
            "354.3587052822113 seconds\n",
            "epoch 42:\n",
            "loss_D: 0.003, loss_G_fake: 0.973,             \n",
            "loss_perturb: 4.976, loss_adv: 34.527, \n",
            "\n",
            "accuracy in training set after perturbation: 77.647989\n",
            "\n",
            "362.9008755683899 seconds\n",
            "epoch 43:\n",
            "loss_D: 0.002, loss_G_fake: 0.977,             \n",
            "loss_perturb: 4.847, loss_adv: 34.396, \n",
            "\n",
            "accuracy in training set after perturbation: 77.650539\n",
            "\n",
            "371.43166756629944 seconds\n",
            "epoch 44:\n",
            "loss_D: 0.002, loss_G_fake: 0.979,             \n",
            "loss_perturb: 5.275, loss_adv: 34.707, \n",
            "\n",
            "accuracy in training set after perturbation: 77.961692\n",
            "\n",
            "380.1721730232239 seconds\n",
            "epoch 45:\n",
            "loss_D: 0.009, loss_G_fake: 0.962,             \n",
            "loss_perturb: 5.011, loss_adv: 33.913, \n",
            "\n",
            "accuracy in training set after perturbation: 77.642888\n",
            "\n",
            "388.80521607398987 seconds\n",
            "epoch 46:\n",
            "loss_D: 0.002, loss_G_fake: 0.980,             \n",
            "loss_perturb: 4.782, loss_adv: 33.799, \n",
            "\n",
            "accuracy in training set after perturbation: 77.492412\n",
            "\n",
            "397.4244878292084 seconds\n",
            "epoch 47:\n",
            "loss_D: 0.001, loss_G_fake: 0.980,             \n",
            "loss_perturb: 5.176, loss_adv: 33.590, \n",
            "\n",
            "accuracy in training set after perturbation: 77.551072\n",
            "\n",
            "405.97946405410767 seconds\n",
            "epoch 48:\n",
            "loss_D: 0.001, loss_G_fake: 0.983,             \n",
            "loss_perturb: 4.509, loss_adv: 33.100, \n",
            "\n",
            "accuracy in training set after perturbation: 77.084343\n",
            "\n",
            "414.4480652809143 seconds\n",
            "epoch 49:\n",
            "loss_D: 0.001, loss_G_fake: 0.986,             \n",
            "loss_perturb: 4.697, loss_adv: 33.031, \n",
            "\n",
            "accuracy in training set after perturbation: 77.288378\n",
            "\n",
            "422.9733555316925 seconds\n",
            "epoch 50:\n",
            "loss_D: 0.000, loss_G_fake: 0.989,             \n",
            "loss_perturb: 4.475, loss_adv: 32.021, \n",
            "\n",
            "accuracy in training set after perturbation: 76.515596\n",
            "\n",
            "431.4909098148346 seconds\n",
            "epoch 51:\n",
            "loss_D: 0.000, loss_G_fake: 0.989,             \n",
            "loss_perturb: 4.191, loss_adv: 31.417, \n",
            "\n",
            "accuracy in training set after perturbation: 76.377872\n",
            "\n",
            "440.0267596244812 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
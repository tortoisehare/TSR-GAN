{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_torch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tortoisehare/TSR-GAN/blob/master/GAN_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DerxOridDA0O",
        "colab_type": "text"
      },
      "source": [
        "pert 0.03, 1:1 G:D, 1x filters, 100 epochs, 90% classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMMq81LtuW7R",
        "colab_type": "text"
      },
      "source": [
        "Foundation of PyTorch code provided by github: mathcbc\n",
        "\n",
        "https://github.com/mathcbc/advGAN_pytorch/blob/master/main.py\n",
        "\n",
        "Changes made: \n",
        "\n",
        "target model is LeNet 5\n",
        "\n",
        "generator given optimizations to allow it to train more than discriminator\n",
        "\n",
        "parameters like filters/strides changed in GAN\n",
        "\n",
        "traffic sign dataset loading and preprocessing added\n",
        "\n",
        "input_sizes for layers work with traffic sign images (32x32)\n",
        "\n",
        "training accuracy visualization added\n",
        "\n",
        "testing accuracy plus misclassification percentage\n",
        "\n",
        "misclassified images visualization\n",
        "\n",
        "restoration model testing added\n",
        "\n",
        "bugs fixed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTIpcLONWBq9",
        "colab_type": "code",
        "outputId": "94ce117a-5f82-47c4-bb70-7d5adc48eff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#torch.multiprocessing.set_start_method(\"spawn\") \n",
        "\n",
        "import torch.optim\n",
        "print(torch.__version__)\n",
        "#from torch import np\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')       \n",
        "get_ipython().magic('matplotlib inline')\n",
        "from matplotlib import pyplot    \n",
        "from matplotlib.pyplot import subplot     \n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Target Model definition - LeNet 5\n",
        "class target_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(target_net, self).__init__()\n",
        "\n",
        "        # LAYER 1: Convolution, Input 1x32x32, Output 6x28x28\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0, bias=True)\n",
        "        # Max-pooling, Input 6x28x28, Output 6x14x14\n",
        "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Dropout\n",
        "        self.drop1 = torch.nn.Dropout(0.5)\n",
        "       \n",
        "        # LAYER 2: Convolution, Input 6x14x14, Output 16x10x10\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)\n",
        "        # Max-pooling, Input 16x10x10, Output 16x5x5\n",
        "        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=2, stride=2) \n",
        "        # Dropout\n",
        "        self.drop2 = torch.nn.Dropout(0.5)\n",
        "        \n",
        "        # LAYER 3: FC, Input 400, Output 120\n",
        "        self.fc1 = torch.nn.Linear(16*5*5, 120)   # convert matrix with 16*5*5 (= 400) features to a matrix of 120 features (columns)\n",
        "        self.drop3 = torch.nn.Dropout(0.6)\n",
        "        \n",
        "        # LAYER 4: FC, Input 120, Output 84\n",
        "        self.fc2 = torch.nn.Linear(120, 84)       # convert matrix with 120 features to a matrix of 84 features (columns)\n",
        "        self.drop4 = torch.nn.Dropout(0.6)\n",
        "        \n",
        "        # LAYER 5: FC Input 84, Output 43\n",
        "        self.fc3 = torch.nn.Linear(84, 43)        # convert matrix with 84 features to a matrix of 43 features (columns)\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # convolve, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.conv1(x))  \n",
        "        # max-pooling with 2x2 grid \n",
        "        x = self.max_pool_1(x) \n",
        "        # convolve, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.conv2(x))\n",
        "        # max-pooling with 2x2 grid\n",
        "        x = self.max_pool_2(x)\n",
        "        # first flatten 'max_pool_2_out' to contain 16*5*5 columns\n",
        "        # read through https://stackoverflow.com/a/42482819/7551231\n",
        "        \n",
        "        x = x.view(-1, 16*5*5)\n",
        "        # FC-1, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        # FC-2, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.fc2(x))\n",
        "        # FC-3\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, image_nc):\n",
        "        super(Discriminator, self).__init__()\n",
        "        # Traffic Sign Dataset: 1*32x32\n",
        "        model = [\n",
        "            nn.Conv2d(image_nc, 8, kernel_size=4, stride=2, padding=0, bias=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 8*16*16\n",
        "            nn.Conv2d(8, 16, kernel_size=4, stride=2, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 16*5*5\n",
        "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(32, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "            # 32*1*1\n",
        "        ]\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x).squeeze()\n",
        "        return output\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self,\n",
        "                 gen_input_nc,\n",
        "                 image_nc,\n",
        "                 ):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        encoder_lis = [\n",
        "            # Traffic Sign Dataset:1*32x32\n",
        "            nn.Conv2d(gen_input_nc, 8, kernel_size=3, stride=1, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            # 8*26*26\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            # 16*12*12\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            # 32*5*5\n",
        "        ]\n",
        "\n",
        "        bottle_neck_lis = [ResnetBlock(32),\n",
        "                       ResnetBlock(32),\n",
        "                       ResnetBlock(32),\n",
        "                       ResnetBlock(32),]\n",
        "\n",
        "        decoder_lis = [\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=0, bias=False),\n",
        "            nn.InstanceNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            # state size. 16 x 11 x 11\n",
        "            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=0, bias=False),\n",
        "            nn.InstanceNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            # state size. 8 x 23 x 23\n",
        "            nn.ConvTranspose2d(8, image_nc, kernel_size=6, stride=1, padding=0, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. image_nc x 32 x 32\n",
        "        ]\n",
        "\n",
        "        self.encoder = nn.Sequential(*encoder_lis)\n",
        "        self.bottle_neck = nn.Sequential(*bottle_neck_lis)\n",
        "        self.decoder = nn.Sequential(*decoder_lis)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.bottle_neck(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define a resnet block\n",
        "# modified from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, padding_type='reflect', norm_layer=nn.BatchNorm2d, use_dropout=False, use_bias=False):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
        "\n",
        "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        conv_block = []\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim),\n",
        "                       nn.ReLU(True)]\n",
        "        if use_dropout:\n",
        "            conv_block += [nn.Dropout(0.5)]\n",
        "\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim)]\n",
        "\n",
        "        return nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "        return out\n",
        "    \n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hpfccYcHYRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#If you want to show the traffic sign name\n",
        "import csv\n",
        "\n",
        "sign_names = []\n",
        "with open('signnames.csv', 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "        sign_names.append(row[1])\n",
        "    sign_names.reverse()\n",
        "    sign_names.pop()\n",
        "    sign_names.reverse()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBB4luoDK1ZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2eba6d9d-946a-4a64-c907-ddfc57d7fcd5"
      },
      "source": [
        "#Data loading and preprocessing\n",
        "\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "import keras\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "training_file = 'train.p'\n",
        "testing_file = 'test.p'\n",
        "validation_file = 'valid.p'\n",
        "\n",
        "with open(training_file, mode='rb') as f:\n",
        "    tstrain = pickle.load(f)\n",
        "with open(testing_file, mode='rb') as f:\n",
        "    tstest = pickle.load(f)\n",
        "with open(validation_file, mode='rb') as f:\n",
        "    tsvalid = pickle.load(f)\n",
        "\n",
        "X_train, Y_train = tstrain['features'], tstrain['labels']\n",
        "X_valid, Y_valid = tsvalid['features'], tsvalid['labels']\n",
        "X_test, Y_test = tstest['features'], tstest['labels']\n",
        "\n",
        "#shuffle training set\n",
        "X_train, Y_train = shuffle(X_train, Y_train, random_state=33)\n",
        "X_test, Y_test = shuffle(X_test, Y_test, random_state=33)\n",
        "X_valid, Y_valid = shuffle(X_valid, Y_valid, random_state=33)\n",
        "\n",
        "#grayscale images\n",
        "grayscale = [0.299,0.587,0.144]\n",
        "\n",
        "X_test = np.dot(X_test, grayscale)\n",
        "X_train = np.dot(X_train, grayscale)\n",
        "X_valid = np.dot(X_valid, grayscale)\n",
        "\n",
        "\n",
        "#normalize\n",
        "X_train = np.array(X_train)/255\n",
        "X_test = np.array(X_test)/255\n",
        "X_valid = np.array(X_valid)/255\n",
        "\n",
        "X_train = np.concatenate((X_train,X_valid), axis=0)\n",
        "Y_train = np.concatenate((Y_train,Y_valid), axis=0)\n",
        "\n",
        "#expand dimensions to fit 4D input array\n",
        "X_train = np.expand_dims(X_train,-1)\n",
        "X_test = np.expand_dims(X_test,-1)\n",
        "#X_valid = np.expand_dims(X_valid,-1)\n",
        "\n",
        "X_train = np.transpose(X_train, (0,3,1,2))\n",
        "X_test = np.transpose(X_test, (0,3,1,2))\n",
        "#X_valid = np.transpose(X_valid, (0,3,1,2))\n",
        "\n",
        "assert(len(X_train)==len(Y_train))\n",
        "n_train = len(X_train)\n",
        "assert(len(X_test)==len(Y_test))\n",
        "n_test = len(X_test)\n",
        "\n",
        "#Y_train = Y_train.reshape(Y_train.shape[0],1)\n",
        "#Y_test = Y_test.reshape(Y_test.shape[0],1)\n",
        "#Y_valid = Y_valid.reshape(Y_valid.shape[0],1)\n",
        "Y_train = to_categorical(Y_train, num_classes=43)\n",
        "Y_test = to_categorical(Y_test, num_classes=43)\n",
        "#Y_valid = to_categorical(Y_valid, num_classes=43)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)\n",
        "#print(X_valid.shape)\n",
        "#print(Y_valid.shape)\n",
        "\n",
        "train_x = torch.stack([torch.Tensor(i) for i in X_train])\n",
        "train_y = torch.stack([torch.LongTensor(i) for i in Y_train])\n",
        "\n",
        "test_x = torch.stack([torch.Tensor(i) for i in X_test])\n",
        "test_y = torch.stack([torch.LongTensor(i) for i in Y_test])\n",
        "\n",
        "#valid_x = torch.stack([torch.Tensor(i) for i in X_valid])\n",
        "#valid_y = torch.stack([torch.LongTensor(i) for i in Y_valid])\n",
        "\n",
        "train_dataset = TensorDataset(train_x,train_y)\n",
        "test_dataset = TensorDataset(test_x,test_y)\n",
        "#valid_dataset = TensorDataset(valid_x,valid_y)\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(39209, 1, 32, 32)\n",
            "(39209, 43)\n",
            "(12630, 1, 32, 32)\n",
            "(12630, 43)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90W2JBoXVrZ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1428
        },
        "outputId": "74e6cc7e-6726-400a-eed7-5572434c8e28"
      },
      "source": [
        "#import torch\n",
        "#import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "#import torch.nn.functional as F\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    use_cuda = True\n",
        "    image_nc = 1 #number of channels\n",
        "    batch_size = 64\n",
        "\n",
        "    # Define what device we are using\n",
        "    print(\"CUDA Available: \", torch.cuda.is_available())\n",
        "    device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "    \n",
        "    #Training set\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, \n",
        "                                  batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "\n",
        "    # training the target model\n",
        "    target_model = target_net().to(device)\n",
        "    target_model.train()\n",
        "    opt_model = torch.optim.Adam(target_model.parameters(), lr=0.001)\n",
        "    epochs = 20\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        loss_epoch = 0\n",
        "        num_acc_correct = 0\n",
        "        if epoch == 20:\n",
        "            opt_model = torch.optim.Adam(target_model.parameters(), lr=0.0001)\n",
        "        for i, data in enumerate(train_dataloader, 0):\n",
        "            train_imgs, train_labels = data\n",
        "            train_imgs, train_labels = train_imgs.to(device), train_labels.to(device)\n",
        "            logits_model = target_model(train_imgs)\n",
        "            loss_model = F.cross_entropy(logits_model, torch.max(train_labels,1)[1])\n",
        "            loss_epoch += loss_model\n",
        "            \n",
        "            #stuff for accuracy\n",
        "            train_label = torch.argmax(train_labels,1)\n",
        "            pred_train = torch.argmax(logits_model,1)\n",
        "            num_acc_correct += torch.sum(pred_train==train_label,0)\n",
        "            \n",
        "            opt_model.zero_grad()\n",
        "            loss_model.backward()\n",
        "            opt_model.step()\n",
        "        \n",
        "        print('loss in epoch %d: %f\\n' % (epoch, loss_epoch.item()))\n",
        "        print('Train accuracy so far: %f\\n'%(100*num_acc_correct.item()/len(train_dataset)))\n",
        "\n",
        "    # save model\n",
        "    targeted_model_file_name = './target_model.pth'\n",
        "    torch.save(target_model.state_dict(), targeted_model_file_name)\n",
        "    target_model.eval()\n",
        "\n",
        "    # TESTING\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, \n",
        "                                 shuffle=False, num_workers=1)\n",
        "    \n",
        "    num_correct = 0\n",
        "    for i, data in enumerate(test_dataloader, 0):\n",
        "        test_img, test_label = data\n",
        "        test_img, test_label = test_img.to(device), test_label.to(device)\n",
        "        test_label = torch.argmax(test_label,1)\n",
        "        pred_lab = torch.argmax(target_model(test_img), 1)\n",
        "        num_correct += torch.sum(pred_lab==test_label,0)\n",
        "\n",
        "    print('accuracy in testing set: %f\\n'%(100*num_correct.item()/len(test_dataset)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available:  True\n",
            "loss in epoch 0: 1318.663208\n",
            "\n",
            "Train accuracy so far: 39.626106\n",
            "\n",
            "loss in epoch 1: 389.805115\n",
            "\n",
            "Train accuracy so far: 81.565457\n",
            "\n",
            "loss in epoch 2: 227.544159\n",
            "\n",
            "Train accuracy so far: 89.696243\n",
            "\n",
            "loss in epoch 3: 153.448517\n",
            "\n",
            "Train accuracy so far: 93.282155\n",
            "\n",
            "loss in epoch 4: 113.491829\n",
            "\n",
            "Train accuracy so far: 95.207733\n",
            "\n",
            "loss in epoch 5: 87.343185\n",
            "\n",
            "Train accuracy so far: 96.459996\n",
            "\n",
            "loss in epoch 6: 69.264786\n",
            "\n",
            "Train accuracy so far: 97.189421\n",
            "\n",
            "loss in epoch 7: 56.622807\n",
            "\n",
            "Train accuracy so far: 97.671453\n",
            "\n",
            "loss in epoch 8: 46.641766\n",
            "\n",
            "Train accuracy so far: 98.094825\n",
            "\n",
            "loss in epoch 9: 40.089954\n",
            "\n",
            "Train accuracy so far: 98.311612\n",
            "\n",
            "loss in epoch 10: 34.564842\n",
            "\n",
            "Train accuracy so far: 98.497794\n",
            "\n",
            "loss in epoch 11: 30.219105\n",
            "\n",
            "Train accuracy so far: 98.638068\n",
            "\n",
            "loss in epoch 12: 24.161337\n",
            "\n",
            "Train accuracy so far: 98.916065\n",
            "\n",
            "loss in epoch 13: 24.574877\n",
            "\n",
            "Train accuracy so far: 98.859956\n",
            "\n",
            "loss in epoch 14: 20.119884\n",
            "\n",
            "Train accuracy so far: 99.012982\n",
            "\n",
            "loss in epoch 15: 18.474098\n",
            "\n",
            "Train accuracy so far: 99.145604\n",
            "\n",
            "loss in epoch 16: 16.613565\n",
            "\n",
            "Train accuracy so far: 99.168558\n",
            "\n",
            "loss in epoch 17: 14.407311\n",
            "\n",
            "Train accuracy so far: 99.303731\n",
            "\n",
            "loss in epoch 18: 14.438875\n",
            "\n",
            "Train accuracy so far: 99.252723\n",
            "\n",
            "loss in epoch 19: 11.455852\n",
            "\n",
            "Train accuracy so far: 99.444005\n",
            "\n",
            "accuracy in testing set: 90.554236\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1FSu-q8WRLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torchvision\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "\n",
        "class AdvGAN_Attack:\n",
        "    def __init__(self,\n",
        "                 device,\n",
        "                 model,\n",
        "                 model_num_labels,\n",
        "                 image_nc,\n",
        "                 box_min,\n",
        "                 box_max):\n",
        "        output_nc = image_nc\n",
        "        self.device = device\n",
        "        self.model_num_labels = model_num_labels\n",
        "        self.model = model\n",
        "        self.input_nc = image_nc\n",
        "        self.output_nc = output_nc\n",
        "        self.box_min = box_min\n",
        "        self.box_max = box_max\n",
        "\n",
        "        self.gen_input_nc = image_nc\n",
        "        self.netG = Generator(self.gen_input_nc, image_nc).to(device)\n",
        "        self.netDisc = Discriminator(image_nc).to(device)\n",
        "\n",
        "        # initialize all weights\n",
        "        self.netG.apply(weights_init)\n",
        "        self.netDisc.apply(weights_init)\n",
        "\n",
        "        # initialize optimizers\n",
        "        self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
        "                                            lr=0.001)\n",
        "        self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
        "                                            lr=0.001)\n",
        "\n",
        "    def train_batch(self, x, labels):\n",
        "        # optimize D\n",
        "        for i in range(1):\n",
        "            perturbation = self.netG(x)\n",
        "            pert = 0.03\n",
        "            \n",
        "            # add a clipping trick\n",
        "            adv_images = torch.clamp(perturbation, -pert, pert) + x\n",
        "            adv_images = torch.clamp(adv_images, self.box_min, self.box_max)\n",
        "\n",
        "            self.optimizer_D.zero_grad()\n",
        "            pred_real = self.netDisc(x)\n",
        "            loss_D_real = F.mse_loss(pred_real, torch.ones_like(pred_real, device=self.device))\n",
        "            loss_D_real.backward()\n",
        "\n",
        "            pred_fake = self.netDisc(adv_images.detach())\n",
        "            loss_D_fake = F.mse_loss(pred_fake, torch.zeros_like(pred_fake, device=self.device))\n",
        "            loss_D_fake.backward()\n",
        "            loss_D_GAN = loss_D_fake + loss_D_real\n",
        "            self.optimizer_D.step()\n",
        "\n",
        "        # optimize G\n",
        "        for i in range(1):\n",
        "            self.optimizer_G.zero_grad()\n",
        "\n",
        "            # cal G's loss in GAN\n",
        "            pred_fake = self.netDisc(adv_images)\n",
        "            loss_G_fake = F.mse_loss(pred_fake, torch.ones_like(pred_fake, device=self.device))\n",
        "            loss_G_fake.backward(retain_graph=True)\n",
        "\n",
        "            # calculate perturbation norm\n",
        "            C = 0.1\n",
        "            loss_perturb = torch.mean(torch.norm(perturbation.view(perturbation.shape[0], -1), 2, dim=1))\n",
        "            # loss_perturb = torch.max(loss_perturb - C, torch.zeros(1, device=self.device))\n",
        "            \n",
        "            \n",
        "            # cal adv loss\n",
        "            logits_model = self.model(adv_images)\n",
        "            probs_model = F.softmax(logits_model, dim=1)\n",
        "            #print(\"logits model\")\n",
        "            #print(logits_model.shape)\n",
        "            onehot_labels = labels.type(torch.cuda.FloatTensor)   #torch.eye(self.model_num_labels, device=self.device)[labels]\n",
        "            #print(onehot_labels.shape)\n",
        "            #print(\"one hot labels\")\n",
        "            #print(onehot_labels)\n",
        "            #print(probs_model.shape)\n",
        "            #print('probs_model')\n",
        "            #print(probs_model)\n",
        "\n",
        "            # C&W loss function\n",
        "            \n",
        "            real = torch.sum(onehot_labels * probs_model, dim=1)\n",
        "            other, _ = torch.max((1 - onehot_labels) * probs_model - onehot_labels * 10000, dim=1)\n",
        "            zeros = torch.zeros_like(other)\n",
        "            loss_adv = torch.max(real - other, zeros)\n",
        "            loss_adv = torch.sum(loss_adv)\n",
        "\n",
        "            # maximize cross_entropy loss\n",
        "            # loss_adv = -F.mse_loss(logits_model, onehot_labels)\n",
        "            # loss_adv = - F.cross_entropy(logits_model, labels)\n",
        "\n",
        "            adv_lambda = 5\n",
        "            pert_lambda = 1\n",
        "            loss_G = adv_lambda * loss_adv + pert_lambda * loss_perturb #ADD WEIGHTS TO PERB\n",
        "            loss_G.backward(retain_graph=True)\n",
        "            self.optimizer_G.step()\n",
        "\n",
        "        return loss_D_GAN.item(), loss_G_fake.item(), loss_perturb.item(), loss_adv.item()\n",
        "\n",
        "    def train(self, train_dataloader, epochs):\n",
        "        t0 = time.time()\n",
        "        for epoch in range(1, epochs+1):\n",
        "\n",
        "            if epoch == 50:\n",
        "                self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
        "                                                    lr=0.0001)\n",
        "                self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
        "                                                    lr=0.0001)\n",
        "            if epoch == 80:\n",
        "                self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
        "                                                    lr=0.00001)\n",
        "                self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
        "                                                    lr=0.00001)\n",
        "            loss_D_sum = 0\n",
        "            loss_G_fake_sum = 0\n",
        "            loss_perturb_sum = 0\n",
        "            loss_adv_sum = 0\n",
        "            \n",
        "            \n",
        "            #For accuracy calculation\n",
        "            num_correct = 0\n",
        "             \n",
        "            \n",
        "            for i, data in enumerate(train_dataloader, start=0):\n",
        "                images, labels = data\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "                \n",
        "                perturbation = self.netG(images)\n",
        "                pert = 0.03\n",
        "                # add a clipping trick\n",
        "                adv_images = torch.clamp(perturbation, -pert, pert) + images\n",
        "                adv_images = torch.clamp(adv_images, self.box_min, self.box_max)\n",
        "                \n",
        "\n",
        "                #print('labels shape')\n",
        "                #print(labels.shape)\n",
        "                #print('images shape')\n",
        "                #print(images.shape)\n",
        "                \n",
        "                loss_D_batch, loss_G_fake_batch, loss_perturb_batch, loss_adv_batch = \\\n",
        "                    self.train_batch(images, labels)\n",
        "                loss_D_sum += loss_D_batch\n",
        "                loss_G_fake_sum += loss_G_fake_batch\n",
        "                loss_perturb_sum += loss_perturb_batch\n",
        "                loss_adv_sum += loss_adv_batch\n",
        "                \n",
        "                #calculate accuracy\n",
        "                train_label = torch.argmax(labels,1)\n",
        "                pred_label = torch.argmax(targeted_model(adv_images), 1)\n",
        "                num_correct += torch.sum(pred_label==train_label,0)\n",
        "                \n",
        "\n",
        "            # print statistics\n",
        "            num_batch = len(train_dataloader)\n",
        "            print(\"epoch %d:\\nloss_D: %.3f, loss_G_fake: %.3f,\\\n",
        "             \\nloss_perturb: %.3f, loss_adv: %.3f, \\n\" %\n",
        "                  (epoch, loss_D_sum/num_batch, loss_G_fake_sum/num_batch,\n",
        "                   loss_perturb_sum/num_batch, loss_adv_sum/num_batch))\n",
        "            print('accuracy in training set after perturbation: %f\\n'%(100*num_correct.item()/len(train_dataset)))\n",
        "            \n",
        "            print('{} seconds'.format(time.time() - t0))\n",
        "            #plt.imshow(images[10].cpu().permute(1, 2, 0))\n",
        "            #plt.imshow(adv_images[10].cpu().permute(1, 2, 0))\n",
        "           \n",
        "\n",
        "            # save generator\n",
        "            if epoch%20==0:\n",
        "                netG_file_name = models_path + 'netG_epoch_' + str(epoch) + '.pth'\n",
        "                torch.save(self.netG.state_dict(), netG_file_name)\n",
        "\n",
        "\n",
        "    def test(self, test_dataloader):\n",
        "        num_correct_test = 0\n",
        "        num_correct_all = 0\n",
        "        \n",
        "        miss = [] #defaultdict(list)\n",
        "        \n",
        "        for i, data in enumerate(test_dataloader, 0):\n",
        "            test_img, test_label = data\n",
        "            test_img, test_label = test_img.to(device), test_label.to(device)\n",
        "            #test_img.requires_grad=True\n",
        "            \n",
        "            perturbation = self.netG(test_img)\n",
        "            pert = 0.03\n",
        "            # add a clipping trick\n",
        "            adv_images_test = torch.clamp(perturbation, -pert, pert) + test_img\n",
        "            adv_images_test = torch.clamp(adv_images_test, self.box_min, self.box_max)\n",
        "            pred_label = targeted_model(adv_images_test)\n",
        "            pred_lab_test = torch.argmax(pred_label,1)\n",
        "            \n",
        "            #first see if the model will just get it wrong on the real image\n",
        "            output = targeted_model(test_img)\n",
        "            init_pred = torch.argmax(output,1)\n",
        "            test_label = torch.argmax(test_label,1)\n",
        "            \n",
        "            num_correct_all += torch.sum(pred_lab_test==test_label,0)\n",
        "            \n",
        "            \n",
        "            for j in range(len(init_pred)):\n",
        "            #if so, move on without adding perturbations\n",
        "                if (init_pred[j] != test_label[j]):\n",
        "                    continue\n",
        "                    \n",
        "                else:\n",
        "                    num_correct_test += torch.sum(pred_lab_test[j]==test_label[j],0)\n",
        "            \n",
        "                    if pred_lab_test[j] != test_label[j]:\n",
        "                        if len(miss)<10:\n",
        "                            adv_ex = adv_images_test[j].squeeze().detach().cpu().numpy()\n",
        "                            real_im = test_img[j].squeeze().detach().cpu().numpy()\n",
        "                            miss.append((sign_names[test_label[j]], real_im, sign_names[pred_lab_test[j]], adv_ex))\n",
        "        \n",
        "        print('accuracy in test set on all perturbed images: %f\\n'%(100*num_correct_all.item()/len(test_dataset)))\n",
        "        \n",
        "        print('misclassifications on previously correctly classified images: %f\\n'%(100-(100*num_correct_test.item()/len(test_dataset))))\n",
        "        \n",
        "        #print 10 misclassified images\n",
        "        #ctr=0\n",
        "        for i in range(len(miss)):\n",
        "            real,r_image,fake,f_image = miss[i]\n",
        "            f, axarr = plt.subplots(1,2)\n",
        "            axarr[0].imshow(r_image, cmap='gray')\n",
        "            axarr[0].set_title('Actual: %s'%(real))\n",
        "            axarr[1].imshow(f_image, cmap='gray')\n",
        "            axarr[1].set_title('Classified as: %s'%(fake))\n",
        "            f.tight_layout()\n",
        "            #plt.title(\"{} -> {}\".format(real,fake))\n",
        "            #plt.imshow(image,cmap='gray')\n",
        "\n",
        "            plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jw6eaS3WMlD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2568
        },
        "outputId": "953a6929-6511-40c9-c15a-887b05312164"
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "#import torch\n",
        "import torchvision\n",
        "import torchvision.datasets\n",
        "#import torchvision.transforms as transforms\n",
        "#from torch.utils.data import DataLoader\n",
        "\n",
        "use_cuda=True\n",
        "image_nc=1\n",
        "epochs = 100\n",
        "\n",
        "batch_size = 128\n",
        "BOX_MIN = 0\n",
        "BOX_MAX = 1\n",
        "\n",
        "# Define what device we are using\n",
        "print(\"CUDA Available: \",torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "pretrained_model = \"./target_model.pth\"\n",
        "targeted_model = target_net().to(device)\n",
        "targeted_model.load_state_dict(torch.load(pretrained_model))\n",
        "targeted_model.eval()\n",
        "\n",
        "#test to make sure restoring model worked\n",
        "num_correct2 = 0\n",
        "for i, data in enumerate(test_dataloader, 0):\n",
        "    test_img2, test_label2 = data\n",
        "    test_img2, test_label2 = test_img2.to(device), test_label2.to(device)\n",
        "    test_label2 = torch.argmax(test_label2,1)\n",
        "    pred_lab2 = torch.argmax(targeted_model(test_img2), 1)\n",
        "    num_correct2 += torch.sum(pred_lab2==test_label2,0)\n",
        "\n",
        "print('accuracy in testing set: %f\\n'%(100*num_correct2.item()/len(test_dataset)))\n",
        "\n",
        "\n",
        "model_num_labels = 43\n",
        "\n",
        "models_path = \"./models/\"\n",
        "if not os.path.exists(models_path):\n",
        "    os.makedirs(models_path)\n",
        "            \n",
        "# train dataset and dataloader declaration\n",
        "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "advGAN = AdvGAN_Attack(device,\n",
        "                          targeted_model,\n",
        "                          model_num_labels,\n",
        "                          image_nc,\n",
        "                          BOX_MIN,\n",
        "                          BOX_MAX)\n",
        "\n",
        "advGAN.train(dataloader, epochs)\n",
        "\n",
        "advGAN.test(test_dataloader)\n",
        "\n",
        "#ideas for optimizing:\n",
        "#improve target classifier - stop it from overfitting\n",
        "\n",
        "#Stephanie is doing:\n",
        "#color\n",
        "#p 0.005, 0.01, 0.03, 0.05, 0.1\n",
        "\n",
        "#Koosha is doing:\n",
        "#G:D ratio\n",
        "#filters in G\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available:  True\n",
            "accuracy in testing set: 89.540776\n",
            "\n",
            "epoch 1:\n",
            "loss_D: 0.436, loss_G_fake: 0.308,             \n",
            "loss_perturb: 12.172, loss_adv: 87.035, \n",
            "\n",
            "accuracy in training set after perturbation: 71.414726\n",
            "\n",
            "8.19588303565979 seconds\n",
            "epoch 2:\n",
            "loss_D: 0.315, loss_G_fake: 0.462,             \n",
            "loss_perturb: 13.774, loss_adv: 75.676, \n",
            "\n",
            "accuracy in training set after perturbation: 62.521360\n",
            "\n",
            "16.297422409057617 seconds\n",
            "epoch 3:\n",
            "loss_D: 0.225, loss_G_fake: 0.575,             \n",
            "loss_perturb: 12.708, loss_adv: 69.258, \n",
            "\n",
            "accuracy in training set after perturbation: 57.563315\n",
            "\n",
            "24.491129398345947 seconds\n",
            "epoch 4:\n",
            "loss_D: 0.183, loss_G_fake: 0.638,             \n",
            "loss_perturb: 11.558, loss_adv: 64.878, \n",
            "\n",
            "accuracy in training set after perturbation: 53.809074\n",
            "\n",
            "32.72665500640869 seconds\n",
            "epoch 5:\n",
            "loss_D: 0.152, loss_G_fake: 0.684,             \n",
            "loss_perturb: 10.364, loss_adv: 61.334, \n",
            "\n",
            "accuracy in training set after perturbation: 50.909230\n",
            "\n",
            "40.991092681884766 seconds\n",
            "epoch 6:\n",
            "loss_D: 0.130, loss_G_fake: 0.720,             \n",
            "loss_perturb: 9.106, loss_adv: 59.068, \n",
            "\n",
            "accuracy in training set after perturbation: 49.049963\n",
            "\n",
            "49.23685550689697 seconds\n",
            "epoch 7:\n",
            "loss_D: 0.129, loss_G_fake: 0.732,             \n",
            "loss_perturb: 8.686, loss_adv: 57.246, \n",
            "\n",
            "accuracy in training set after perturbation: 47.563060\n",
            "\n",
            "57.44741606712341 seconds\n",
            "epoch 8:\n",
            "loss_D: 0.108, loss_G_fake: 0.768,             \n",
            "loss_perturb: 7.993, loss_adv: 55.588, \n",
            "\n",
            "accuracy in training set after perturbation: 46.343952\n",
            "\n",
            "65.62628293037415 seconds\n",
            "epoch 9:\n",
            "loss_D: 0.102, loss_G_fake: 0.786,             \n",
            "loss_perturb: 7.565, loss_adv: 54.066, \n",
            "\n",
            "accuracy in training set after perturbation: 44.941212\n",
            "\n",
            "73.80723905563354 seconds\n",
            "epoch 10:\n",
            "loss_D: 0.092, loss_G_fake: 0.805,             \n",
            "loss_perturb: 7.540, loss_adv: 52.863, \n",
            "\n",
            "accuracy in training set after perturbation: 43.989900\n",
            "\n",
            "82.01133871078491 seconds\n",
            "epoch 11:\n",
            "loss_D: 0.073, loss_G_fake: 0.837,             \n",
            "loss_perturb: 7.019, loss_adv: 50.755, \n",
            "\n",
            "accuracy in training set after perturbation: 42.278558\n",
            "\n",
            "90.15529990196228 seconds\n",
            "epoch 12:\n",
            "loss_D: 0.076, loss_G_fake: 0.836,             \n",
            "loss_perturb: 6.476, loss_adv: 49.706, \n",
            "\n",
            "accuracy in training set after perturbation: 41.470071\n",
            "\n",
            "98.29901266098022 seconds\n",
            "epoch 13:\n",
            "loss_D: 0.069, loss_G_fake: 0.853,             \n",
            "loss_perturb: 6.161, loss_adv: 48.214, \n",
            "\n",
            "accuracy in training set after perturbation: 40.304522\n",
            "\n",
            "106.56994199752808 seconds\n",
            "epoch 14:\n",
            "loss_D: 0.061, loss_G_fake: 0.866,             \n",
            "loss_perturb: 6.043, loss_adv: 47.372, \n",
            "\n",
            "accuracy in training set after perturbation: 39.427172\n",
            "\n",
            "114.82077121734619 seconds\n",
            "epoch 15:\n",
            "loss_D: 0.065, loss_G_fake: 0.869,             \n",
            "loss_perturb: 6.075, loss_adv: 46.926, \n",
            "\n",
            "accuracy in training set after perturbation: 39.062460\n",
            "\n",
            "123.06872034072876 seconds\n",
            "epoch 16:\n",
            "loss_D: 0.062, loss_G_fake: 0.873,             \n",
            "loss_perturb: 6.158, loss_adv: 46.515, \n",
            "\n",
            "accuracy in training set after perturbation: 38.940039\n",
            "\n",
            "131.44789099693298 seconds\n",
            "epoch 17:\n",
            "loss_D: 0.049, loss_G_fake: 0.899,             \n",
            "loss_perturb: 5.749, loss_adv: 45.011, \n",
            "\n",
            "accuracy in training set after perturbation: 37.488842\n",
            "\n",
            "139.7477195262909 seconds\n",
            "epoch 18:\n",
            "loss_D: 0.049, loss_G_fake: 0.902,             \n",
            "loss_perturb: 5.934, loss_adv: 44.559, \n",
            "\n",
            "accuracy in training set after perturbation: 37.134331\n",
            "\n",
            "147.89770436286926 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2d214aa9516b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m                           BOX_MAX)\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0madvGAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0madvGAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-3c5de9275848>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataloader, epochs)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;31m#print(images.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0mloss_D_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_G_fake_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_perturb_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_adv_batch\u001b[0m \u001b[0;34m=\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0mloss_D_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_D_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mloss_G_fake_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_G_fake_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-3c5de9275848>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, x, labels)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# cal G's loss in GAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mpred_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetDisc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mloss_G_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mloss_G_fake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-966dbbae3a64>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 338\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
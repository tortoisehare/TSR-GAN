{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_torch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tortoisehare/TSR-GAN/blob/master/GAN_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DerxOridDA0O",
        "colab_type": "text"
      },
      "source": [
        "pert 0.03, 1:1 G:D, 1x filters, 100 epochs, 90% classifier, adv_lambda:pert_lambda 5:1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMMq81LtuW7R",
        "colab_type": "text"
      },
      "source": [
        "Foundation of PyTorch code provided by github: mathcbc\n",
        "\n",
        "https://github.com/mathcbc/advGAN_pytorch/blob/master/main.py\n",
        "\n",
        "Changes made: \n",
        "\n",
        "target model is LeNet 5\n",
        "\n",
        "generator given optimizations to allow it to train more than discriminator\n",
        "\n",
        "parameters like filters/strides changed in GAN\n",
        "\n",
        "traffic sign dataset loading and preprocessing added\n",
        "\n",
        "input_sizes for layers work with traffic sign images (32x32)\n",
        "\n",
        "training accuracy visualization added\n",
        "\n",
        "testing accuracy plus misclassification percentage\n",
        "\n",
        "misclassified images visualization\n",
        "\n",
        "restoration model testing added\n",
        "\n",
        "bugs fixed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTIpcLONWBq9",
        "colab_type": "code",
        "outputId": "94ce117a-5f82-47c4-bb70-7d5adc48eff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#torch.multiprocessing.set_start_method(\"spawn\") \n",
        "\n",
        "import torch.optim\n",
        "print(torch.__version__)\n",
        "#from torch import np\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')       \n",
        "get_ipython().magic('matplotlib inline')\n",
        "from matplotlib import pyplot    \n",
        "from matplotlib.pyplot import subplot     \n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Target Model definition - LeNet 5\n",
        "class target_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(target_net, self).__init__()\n",
        "\n",
        "        # LAYER 1: Convolution, Input 1x32x32, Output 6x28x28\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0, bias=True)\n",
        "        # Max-pooling, Input 6x28x28, Output 6x14x14\n",
        "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Dropout\n",
        "        self.drop1 = torch.nn.Dropout(0.5)\n",
        "       \n",
        "        # LAYER 2: Convolution, Input 6x14x14, Output 16x10x10\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)\n",
        "        # Max-pooling, Input 16x10x10, Output 16x5x5\n",
        "        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=2, stride=2) \n",
        "        # Dropout\n",
        "        self.drop2 = torch.nn.Dropout(0.5)\n",
        "        \n",
        "        # LAYER 3: FC, Input 400, Output 120\n",
        "        self.fc1 = torch.nn.Linear(16*5*5, 120)   # convert matrix with 16*5*5 (= 400) features to a matrix of 120 features (columns)\n",
        "        self.drop3 = torch.nn.Dropout(0.6)\n",
        "        \n",
        "        # LAYER 4: FC, Input 120, Output 84\n",
        "        self.fc2 = torch.nn.Linear(120, 84)       # convert matrix with 120 features to a matrix of 84 features (columns)\n",
        "        self.drop4 = torch.nn.Dropout(0.6)\n",
        "        \n",
        "        # LAYER 5: FC Input 84, Output 43\n",
        "        self.fc3 = torch.nn.Linear(84, 43)        # convert matrix with 84 features to a matrix of 43 features (columns)\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # convolve, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.conv1(x))  \n",
        "        # max-pooling with 2x2 grid \n",
        "        x = self.max_pool_1(x) \n",
        "        # convolve, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.conv2(x))\n",
        "        # max-pooling with 2x2 grid\n",
        "        x = self.max_pool_2(x)\n",
        "        # first flatten 'max_pool_2_out' to contain 16*5*5 columns\n",
        "        # read through https://stackoverflow.com/a/42482819/7551231\n",
        "        \n",
        "        x = x.view(-1, 16*5*5)\n",
        "        # FC-1, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        # FC-2, then perform ReLU non-linearity\n",
        "        x = torch.nn.functional.relu(self.fc2(x))\n",
        "        # FC-3\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, image_nc):\n",
        "        super(Discriminator, self).__init__()\n",
        "        # Traffic Sign Dataset: 1*32x32\n",
        "        model = [\n",
        "            nn.Conv2d(image_nc, 8, kernel_size=4, stride=2, padding=0, bias=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 8*16*16\n",
        "            nn.Conv2d(8, 16, kernel_size=4, stride=2, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 16*5*5\n",
        "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(32, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "            # 32*1*1\n",
        "        ]\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x).squeeze()\n",
        "        return output\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self,\n",
        "                 gen_input_nc,\n",
        "                 image_nc,\n",
        "                 ):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        encoder_lis = [\n",
        "            # Traffic Sign Dataset:1*32x32\n",
        "            nn.Conv2d(gen_input_nc, 8, kernel_size=3, stride=1, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            # 8*26*26\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            # 16*12*12\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=0, bias=True),\n",
        "            nn.InstanceNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            # 32*5*5\n",
        "        ]\n",
        "\n",
        "        bottle_neck_lis = [ResnetBlock(32),\n",
        "                       ResnetBlock(32),\n",
        "                       ResnetBlock(32),\n",
        "                       ResnetBlock(32),]\n",
        "\n",
        "        decoder_lis = [\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=0, bias=False),\n",
        "            nn.InstanceNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            # state size. 16 x 11 x 11\n",
        "            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=0, bias=False),\n",
        "            nn.InstanceNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            # state size. 8 x 23 x 23\n",
        "            nn.ConvTranspose2d(8, image_nc, kernel_size=6, stride=1, padding=0, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. image_nc x 32 x 32\n",
        "        ]\n",
        "\n",
        "        self.encoder = nn.Sequential(*encoder_lis)\n",
        "        self.bottle_neck = nn.Sequential(*bottle_neck_lis)\n",
        "        self.decoder = nn.Sequential(*decoder_lis)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.bottle_neck(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define a resnet block\n",
        "# modified from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, padding_type='reflect', norm_layer=nn.BatchNorm2d, use_dropout=False, use_bias=False):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
        "\n",
        "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
        "        conv_block = []\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim),\n",
        "                       nn.ReLU(True)]\n",
        "        if use_dropout:\n",
        "            conv_block += [nn.Dropout(0.5)]\n",
        "\n",
        "        p = 0\n",
        "        if padding_type == 'reflect':\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\n",
        "        elif padding_type == 'replicate':\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\n",
        "        elif padding_type == 'zero':\n",
        "            p = 1\n",
        "        else:\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
        "\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
        "                       norm_layer(dim)]\n",
        "\n",
        "        return nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "        return out\n",
        "    \n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hpfccYcHYRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#If you want to show the traffic sign name\n",
        "import csv\n",
        "\n",
        "sign_names = []\n",
        "with open('signnames.csv', 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "        sign_names.append(row[1])\n",
        "    sign_names.reverse()\n",
        "    sign_names.pop()\n",
        "    sign_names.reverse()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBB4luoDK1ZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2eba6d9d-946a-4a64-c907-ddfc57d7fcd5"
      },
      "source": [
        "#Data loading and preprocessing\n",
        "\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "import keras\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "training_file = 'train.p'\n",
        "testing_file = 'test.p'\n",
        "validation_file = 'valid.p'\n",
        "\n",
        "with open(training_file, mode='rb') as f:\n",
        "    tstrain = pickle.load(f)\n",
        "with open(testing_file, mode='rb') as f:\n",
        "    tstest = pickle.load(f)\n",
        "with open(validation_file, mode='rb') as f:\n",
        "    tsvalid = pickle.load(f)\n",
        "\n",
        "X_train, Y_train = tstrain['features'], tstrain['labels']\n",
        "X_valid, Y_valid = tsvalid['features'], tsvalid['labels']\n",
        "X_test, Y_test = tstest['features'], tstest['labels']\n",
        "\n",
        "#shuffle training set\n",
        "X_train, Y_train = shuffle(X_train, Y_train, random_state=33)\n",
        "X_test, Y_test = shuffle(X_test, Y_test, random_state=33)\n",
        "X_valid, Y_valid = shuffle(X_valid, Y_valid, random_state=33)\n",
        "\n",
        "#grayscale images\n",
        "grayscale = [0.299,0.587,0.144]\n",
        "\n",
        "X_test = np.dot(X_test, grayscale)\n",
        "X_train = np.dot(X_train, grayscale)\n",
        "X_valid = np.dot(X_valid, grayscale)\n",
        "\n",
        "\n",
        "#normalize\n",
        "X_train = np.array(X_train)/255\n",
        "X_test = np.array(X_test)/255\n",
        "X_valid = np.array(X_valid)/255\n",
        "\n",
        "X_train = np.concatenate((X_train,X_valid), axis=0)\n",
        "Y_train = np.concatenate((Y_train,Y_valid), axis=0)\n",
        "\n",
        "#expand dimensions to fit 4D input array\n",
        "X_train = np.expand_dims(X_train,-1)\n",
        "X_test = np.expand_dims(X_test,-1)\n",
        "#X_valid = np.expand_dims(X_valid,-1)\n",
        "\n",
        "X_train = np.transpose(X_train, (0,3,1,2))\n",
        "X_test = np.transpose(X_test, (0,3,1,2))\n",
        "#X_valid = np.transpose(X_valid, (0,3,1,2))\n",
        "\n",
        "assert(len(X_train)==len(Y_train))\n",
        "n_train = len(X_train)\n",
        "assert(len(X_test)==len(Y_test))\n",
        "n_test = len(X_test)\n",
        "\n",
        "#Y_train = Y_train.reshape(Y_train.shape[0],1)\n",
        "#Y_test = Y_test.reshape(Y_test.shape[0],1)\n",
        "#Y_valid = Y_valid.reshape(Y_valid.shape[0],1)\n",
        "Y_train = to_categorical(Y_train, num_classes=43)\n",
        "Y_test = to_categorical(Y_test, num_classes=43)\n",
        "#Y_valid = to_categorical(Y_valid, num_classes=43)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)\n",
        "#print(X_valid.shape)\n",
        "#print(Y_valid.shape)\n",
        "\n",
        "train_x = torch.stack([torch.Tensor(i) for i in X_train])\n",
        "train_y = torch.stack([torch.LongTensor(i) for i in Y_train])\n",
        "\n",
        "test_x = torch.stack([torch.Tensor(i) for i in X_test])\n",
        "test_y = torch.stack([torch.LongTensor(i) for i in Y_test])\n",
        "\n",
        "#valid_x = torch.stack([torch.Tensor(i) for i in X_valid])\n",
        "#valid_y = torch.stack([torch.LongTensor(i) for i in Y_valid])\n",
        "\n",
        "train_dataset = TensorDataset(train_x,train_y)\n",
        "test_dataset = TensorDataset(test_x,test_y)\n",
        "#valid_dataset = TensorDataset(valid_x,valid_y)\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(39209, 1, 32, 32)\n",
            "(39209, 43)\n",
            "(12630, 1, 32, 32)\n",
            "(12630, 43)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90W2JBoXVrZ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1428
        },
        "outputId": "74e6cc7e-6726-400a-eed7-5572434c8e28"
      },
      "source": [
        "#import torch\n",
        "#import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "#import torch.nn.functional as F\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    use_cuda = True\n",
        "    image_nc = 1 #number of channels\n",
        "    batch_size = 64\n",
        "\n",
        "    # Define what device we are using\n",
        "    print(\"CUDA Available: \", torch.cuda.is_available())\n",
        "    device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "    \n",
        "    #Training set\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, \n",
        "                                  batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "\n",
        "    # training the target model\n",
        "    target_model = target_net().to(device)\n",
        "    target_model.train()\n",
        "    opt_model = torch.optim.Adam(target_model.parameters(), lr=0.001)\n",
        "    epochs = 20\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        loss_epoch = 0\n",
        "        num_acc_correct = 0\n",
        "        if epoch == 20:\n",
        "            opt_model = torch.optim.Adam(target_model.parameters(), lr=0.0001)\n",
        "        for i, data in enumerate(train_dataloader, 0):\n",
        "            train_imgs, train_labels = data\n",
        "            train_imgs, train_labels = train_imgs.to(device), train_labels.to(device)\n",
        "            logits_model = target_model(train_imgs)\n",
        "            loss_model = F.cross_entropy(logits_model, torch.max(train_labels,1)[1])\n",
        "            loss_epoch += loss_model\n",
        "            \n",
        "            #stuff for accuracy\n",
        "            train_label = torch.argmax(train_labels,1)\n",
        "            pred_train = torch.argmax(logits_model,1)\n",
        "            num_acc_correct += torch.sum(pred_train==train_label,0)\n",
        "            \n",
        "            opt_model.zero_grad()\n",
        "            loss_model.backward()\n",
        "            opt_model.step()\n",
        "        \n",
        "        print('loss in epoch %d: %f\\n' % (epoch, loss_epoch.item()))\n",
        "        print('Train accuracy so far: %f\\n'%(100*num_acc_correct.item()/len(train_dataset)))\n",
        "\n",
        "    # save model\n",
        "    targeted_model_file_name = './target_model.pth'\n",
        "    torch.save(target_model.state_dict(), targeted_model_file_name)\n",
        "    target_model.eval()\n",
        "\n",
        "    # TESTING\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, \n",
        "                                 shuffle=False, num_workers=1)\n",
        "    \n",
        "    num_correct = 0\n",
        "    for i, data in enumerate(test_dataloader, 0):\n",
        "        test_img, test_label = data\n",
        "        test_img, test_label = test_img.to(device), test_label.to(device)\n",
        "        test_label = torch.argmax(test_label,1)\n",
        "        pred_lab = torch.argmax(target_model(test_img), 1)\n",
        "        num_correct += torch.sum(pred_lab==test_label,0)\n",
        "\n",
        "    print('accuracy in testing set: %f\\n'%(100*num_correct.item()/len(test_dataset)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available:  True\n",
            "loss in epoch 0: 1318.663208\n",
            "\n",
            "Train accuracy so far: 39.626106\n",
            "\n",
            "loss in epoch 1: 389.805115\n",
            "\n",
            "Train accuracy so far: 81.565457\n",
            "\n",
            "loss in epoch 2: 227.544159\n",
            "\n",
            "Train accuracy so far: 89.696243\n",
            "\n",
            "loss in epoch 3: 153.448517\n",
            "\n",
            "Train accuracy so far: 93.282155\n",
            "\n",
            "loss in epoch 4: 113.491829\n",
            "\n",
            "Train accuracy so far: 95.207733\n",
            "\n",
            "loss in epoch 5: 87.343185\n",
            "\n",
            "Train accuracy so far: 96.459996\n",
            "\n",
            "loss in epoch 6: 69.264786\n",
            "\n",
            "Train accuracy so far: 97.189421\n",
            "\n",
            "loss in epoch 7: 56.622807\n",
            "\n",
            "Train accuracy so far: 97.671453\n",
            "\n",
            "loss in epoch 8: 46.641766\n",
            "\n",
            "Train accuracy so far: 98.094825\n",
            "\n",
            "loss in epoch 9: 40.089954\n",
            "\n",
            "Train accuracy so far: 98.311612\n",
            "\n",
            "loss in epoch 10: 34.564842\n",
            "\n",
            "Train accuracy so far: 98.497794\n",
            "\n",
            "loss in epoch 11: 30.219105\n",
            "\n",
            "Train accuracy so far: 98.638068\n",
            "\n",
            "loss in epoch 12: 24.161337\n",
            "\n",
            "Train accuracy so far: 98.916065\n",
            "\n",
            "loss in epoch 13: 24.574877\n",
            "\n",
            "Train accuracy so far: 98.859956\n",
            "\n",
            "loss in epoch 14: 20.119884\n",
            "\n",
            "Train accuracy so far: 99.012982\n",
            "\n",
            "loss in epoch 15: 18.474098\n",
            "\n",
            "Train accuracy so far: 99.145604\n",
            "\n",
            "loss in epoch 16: 16.613565\n",
            "\n",
            "Train accuracy so far: 99.168558\n",
            "\n",
            "loss in epoch 17: 14.407311\n",
            "\n",
            "Train accuracy so far: 99.303731\n",
            "\n",
            "loss in epoch 18: 14.438875\n",
            "\n",
            "Train accuracy so far: 99.252723\n",
            "\n",
            "loss in epoch 19: 11.455852\n",
            "\n",
            "Train accuracy so far: 99.444005\n",
            "\n",
            "accuracy in testing set: 90.554236\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1FSu-q8WRLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torchvision\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "\n",
        "class AdvGAN_Attack:\n",
        "    def __init__(self,\n",
        "                 device,\n",
        "                 model,\n",
        "                 model_num_labels,\n",
        "                 image_nc,\n",
        "                 box_min,\n",
        "                 box_max):\n",
        "        output_nc = image_nc\n",
        "        self.device = device\n",
        "        self.model_num_labels = model_num_labels\n",
        "        self.model = model\n",
        "        self.input_nc = image_nc\n",
        "        self.output_nc = output_nc\n",
        "        self.box_min = box_min\n",
        "        self.box_max = box_max\n",
        "\n",
        "        self.gen_input_nc = image_nc\n",
        "        self.netG = Generator(self.gen_input_nc, image_nc).to(device)\n",
        "        self.netDisc = Discriminator(image_nc).to(device)\n",
        "\n",
        "        # initialize all weights\n",
        "        self.netG.apply(weights_init)\n",
        "        self.netDisc.apply(weights_init)\n",
        "\n",
        "        # initialize optimizers\n",
        "        self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
        "                                            lr=0.001)\n",
        "        self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
        "                                            lr=0.001)\n",
        "\n",
        "    def train_batch(self, x, labels):\n",
        "        # optimize D\n",
        "        for i in range(1):\n",
        "            perturbation = self.netG(x)\n",
        "            pert = 0.03\n",
        "            \n",
        "            # add a clipping trick\n",
        "            adv_images = torch.clamp(perturbation, -pert, pert) + x\n",
        "            adv_images = torch.clamp(adv_images, self.box_min, self.box_max)\n",
        "\n",
        "            self.optimizer_D.zero_grad()\n",
        "            pred_real = self.netDisc(x)\n",
        "            loss_D_real = F.mse_loss(pred_real, torch.ones_like(pred_real, device=self.device))\n",
        "            loss_D_real.backward()\n",
        "\n",
        "            pred_fake = self.netDisc(adv_images.detach())\n",
        "            loss_D_fake = F.mse_loss(pred_fake, torch.zeros_like(pred_fake, device=self.device))\n",
        "            loss_D_fake.backward()\n",
        "            loss_D_GAN = loss_D_fake + loss_D_real\n",
        "            self.optimizer_D.step()\n",
        "\n",
        "        # optimize G\n",
        "        for i in range(1):\n",
        "            self.optimizer_G.zero_grad()\n",
        "\n",
        "            # cal G's loss in GAN\n",
        "            pred_fake = self.netDisc(adv_images)\n",
        "            loss_G_fake = F.mse_loss(pred_fake, torch.ones_like(pred_fake, device=self.device))\n",
        "            loss_G_fake.backward(retain_graph=True)\n",
        "\n",
        "            # calculate perturbation norm\n",
        "            C = 0.1\n",
        "            loss_perturb = torch.mean(torch.norm(perturbation.view(perturbation.shape[0], -1), 2, dim=1))\n",
        "            # loss_perturb = torch.max(loss_perturb - C, torch.zeros(1, device=self.device))\n",
        "            \n",
        "            \n",
        "            # cal adv loss\n",
        "            logits_model = self.model(adv_images)\n",
        "            probs_model = F.softmax(logits_model, dim=1)\n",
        "            #print(\"logits model\")\n",
        "            #print(logits_model.shape)\n",
        "            onehot_labels = labels.type(torch.cuda.FloatTensor)   #torch.eye(self.model_num_labels, device=self.device)[labels]\n",
        "            #print(onehot_labels.shape)\n",
        "            #print(\"one hot labels\")\n",
        "            #print(onehot_labels)\n",
        "            #print(probs_model.shape)\n",
        "            #print('probs_model')\n",
        "            #print(probs_model)\n",
        "\n",
        "            # C&W loss function\n",
        "            \n",
        "            real = torch.sum(onehot_labels * probs_model, dim=1)\n",
        "            other, _ = torch.max((1 - onehot_labels) * probs_model - onehot_labels * 10000, dim=1)\n",
        "            zeros = torch.zeros_like(other)\n",
        "            loss_adv = torch.max(real - other, zeros)\n",
        "            loss_adv = torch.sum(loss_adv)\n",
        "\n",
        "            # maximize cross_entropy loss\n",
        "            # loss_adv = -F.mse_loss(logits_model, onehot_labels)\n",
        "            # loss_adv = - F.cross_entropy(logits_model, labels)\n",
        "\n",
        "            adv_lambda = 5\n",
        "            pert_lambda = 1\n",
        "            loss_G = adv_lambda * loss_adv + pert_lambda * loss_perturb #ADD WEIGHTS TO PERB\n",
        "            loss_G.backward(retain_graph=True)\n",
        "            self.optimizer_G.step()\n",
        "\n",
        "        return loss_D_GAN.item(), loss_G_fake.item(), loss_perturb.item(), loss_adv.item()\n",
        "\n",
        "    def train(self, train_dataloader, epochs):\n",
        "        t0 = time.time()\n",
        "        for epoch in range(1, epochs+1):\n",
        "\n",
        "            if epoch == 50:\n",
        "                self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
        "                                                    lr=0.0001)\n",
        "                self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
        "                                                    lr=0.0001)\n",
        "            if epoch == 80:\n",
        "                self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
        "                                                    lr=0.00001)\n",
        "                self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
        "                                                    lr=0.00001)\n",
        "            loss_D_sum = 0\n",
        "            loss_G_fake_sum = 0\n",
        "            loss_perturb_sum = 0\n",
        "            loss_adv_sum = 0\n",
        "            \n",
        "            \n",
        "            #For accuracy calculation\n",
        "            num_correct = 0\n",
        "             \n",
        "            \n",
        "            for i, data in enumerate(train_dataloader, start=0):\n",
        "                images, labels = data\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "                \n",
        "                perturbation = self.netG(images)\n",
        "                pert = 0.03\n",
        "                # add a clipping trick\n",
        "                adv_images = torch.clamp(perturbation, -pert, pert) + images\n",
        "                adv_images = torch.clamp(adv_images, self.box_min, self.box_max)\n",
        "                \n",
        "\n",
        "                #print('labels shape')\n",
        "                #print(labels.shape)\n",
        "                #print('images shape')\n",
        "                #print(images.shape)\n",
        "                \n",
        "                loss_D_batch, loss_G_fake_batch, loss_perturb_batch, loss_adv_batch = \\\n",
        "                    self.train_batch(images, labels)\n",
        "                loss_D_sum += loss_D_batch\n",
        "                loss_G_fake_sum += loss_G_fake_batch\n",
        "                loss_perturb_sum += loss_perturb_batch\n",
        "                loss_adv_sum += loss_adv_batch\n",
        "                \n",
        "                #calculate accuracy\n",
        "                train_label = torch.argmax(labels,1)\n",
        "                pred_label = torch.argmax(targeted_model(adv_images), 1)\n",
        "                num_correct += torch.sum(pred_label==train_label,0)\n",
        "                \n",
        "\n",
        "            # print statistics\n",
        "            num_batch = len(train_dataloader)\n",
        "            print(\"epoch %d:\\nloss_D: %.3f, loss_G_fake: %.3f,\\\n",
        "             \\nloss_perturb: %.3f, loss_adv: %.3f, \\n\" %\n",
        "                  (epoch, loss_D_sum/num_batch, loss_G_fake_sum/num_batch,\n",
        "                   loss_perturb_sum/num_batch, loss_adv_sum/num_batch))\n",
        "            print('accuracy in training set after perturbation: %f\\n'%(100*num_correct.item()/len(train_dataset)))\n",
        "            \n",
        "            print('{} seconds'.format(time.time() - t0))\n",
        "            #plt.imshow(images[10].cpu().permute(1, 2, 0))\n",
        "            #plt.imshow(adv_images[10].cpu().permute(1, 2, 0))\n",
        "           \n",
        "\n",
        "            # save generator\n",
        "            if epoch%20==0:\n",
        "                netG_file_name = models_path + 'netG_epoch_' + str(epoch) + '.pth'\n",
        "                torch.save(self.netG.state_dict(), netG_file_name)\n",
        "\n",
        "\n",
        "    def test(self, test_dataloader):\n",
        "        num_correct_test = 0\n",
        "        num_correct_all = 0\n",
        "        \n",
        "        miss = [] #defaultdict(list)\n",
        "        \n",
        "        for i, data in enumerate(test_dataloader, 0):\n",
        "            test_img, test_label = data\n",
        "            test_img, test_label = test_img.to(device), test_label.to(device)\n",
        "            #test_img.requires_grad=True\n",
        "            \n",
        "            perturbation = self.netG(test_img)\n",
        "            pert = 0.03\n",
        "            # add a clipping trick\n",
        "            adv_images_test = torch.clamp(perturbation, -pert, pert) + test_img\n",
        "            adv_images_test = torch.clamp(adv_images_test, self.box_min, self.box_max)\n",
        "            pred_label = targeted_model(adv_images_test)\n",
        "            pred_lab_test = torch.argmax(pred_label,1)\n",
        "            \n",
        "            #first see if the model will just get it wrong on the real image\n",
        "            output = targeted_model(test_img)\n",
        "            init_pred = torch.argmax(output,1)\n",
        "            test_label = torch.argmax(test_label,1)\n",
        "            \n",
        "            num_correct_all += torch.sum(pred_lab_test==test_label,0)\n",
        "            \n",
        "            \n",
        "            for j in range(len(init_pred)):\n",
        "            #if so, move on without adding perturbations\n",
        "                if (init_pred[j] != test_label[j]):\n",
        "                    continue\n",
        "                    \n",
        "                else:\n",
        "                    num_correct_test += torch.sum(pred_lab_test[j]==test_label[j],0)\n",
        "            \n",
        "                    if pred_lab_test[j] != test_label[j]:\n",
        "                        if len(miss)<10:\n",
        "                            adv_ex = adv_images_test[j].squeeze().detach().cpu().numpy()\n",
        "                            real_im = test_img[j].squeeze().detach().cpu().numpy()\n",
        "                            miss.append((sign_names[test_label[j]], real_im, sign_names[pred_lab_test[j]], adv_ex))\n",
        "        \n",
        "        print('accuracy in test set on all perturbed images: %f\\n'%(100*num_correct_all.item()/len(test_dataset)))\n",
        "        \n",
        "        print('misclassifications on previously correctly classified images: %f\\n'%(100-(100*num_correct_test.item()/len(test_dataset))))\n",
        "        \n",
        "        #print 10 misclassified images\n",
        "        #ctr=0\n",
        "        for i in range(len(miss)):\n",
        "            real,r_image,fake,f_image = miss[i]\n",
        "            f, axarr = plt.subplots(1,2)\n",
        "            axarr[0].imshow(r_image, cmap='gray')\n",
        "            axarr[0].set_title('Actual: %s'%(real))\n",
        "            axarr[1].imshow(f_image, cmap='gray')\n",
        "            axarr[1].set_title('Classified as: %s'%(fake))\n",
        "            f.tight_layout()\n",
        "            #plt.title(\"{} -> {}\".format(real,fake))\n",
        "            #plt.imshow(image,cmap='gray')\n",
        "\n",
        "            plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jw6eaS3WMlD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9231
        },
        "outputId": "d3d52ae3-b6f5-4aac-b8f8-1a93e497c62f"
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "#import torch\n",
        "import torchvision\n",
        "import torchvision.datasets\n",
        "#import torchvision.transforms as transforms\n",
        "#from torch.utils.data import DataLoader\n",
        "\n",
        "use_cuda=True\n",
        "image_nc=1\n",
        "epochs = 100\n",
        "\n",
        "batch_size = 128\n",
        "BOX_MIN = 0\n",
        "BOX_MAX = 1\n",
        "\n",
        "# Define what device we are using\n",
        "print(\"CUDA Available: \",torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "pretrained_model = \"./target_model.pth\"\n",
        "targeted_model = target_net().to(device)\n",
        "targeted_model.load_state_dict(torch.load(pretrained_model))\n",
        "targeted_model.eval()\n",
        "\n",
        "#test to make sure restoring model worked\n",
        "num_correct2 = 0\n",
        "for i, data in enumerate(test_dataloader, 0):\n",
        "    test_img2, test_label2 = data\n",
        "    test_img2, test_label2 = test_img2.to(device), test_label2.to(device)\n",
        "    test_label2 = torch.argmax(test_label2,1)\n",
        "    pred_lab2 = torch.argmax(targeted_model(test_img2), 1)\n",
        "    num_correct2 += torch.sum(pred_lab2==test_label2,0)\n",
        "\n",
        "print('accuracy in testing set: %f\\n'%(100*num_correct2.item()/len(test_dataset)))\n",
        "\n",
        "\n",
        "model_num_labels = 43\n",
        "\n",
        "models_path = \"./models/\"\n",
        "if not os.path.exists(models_path):\n",
        "    os.makedirs(models_path)\n",
        "            \n",
        "# train dataset and dataloader declaration\n",
        "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "advGAN = AdvGAN_Attack(device,\n",
        "                          targeted_model,\n",
        "                          model_num_labels,\n",
        "                          image_nc,\n",
        "                          BOX_MIN,\n",
        "                          BOX_MAX)\n",
        "\n",
        "advGAN.train(dataloader, epochs)\n",
        "\n",
        "advGAN.test(test_dataloader)\n",
        "\n",
        "#ideas for optimizing:\n",
        "#improve target classifier - stop it from overfitting\n",
        "\n",
        "#Stephanie is doing:\n",
        "#color\n",
        "#p 0.005, 0.01, 0.03, 0.05, 0.1\n",
        "\n",
        "#Koosha is doing:\n",
        "#G:D ratio\n",
        "#filters in G\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available:  True\n",
            "accuracy in testing set: 90.554236\n",
            "\n",
            "epoch 1:\n",
            "loss_D: 0.396, loss_G_fake: 0.323,             \n",
            "loss_perturb: 11.649, loss_adv: 81.360, \n",
            "\n",
            "accuracy in training set after perturbation: 67.094290\n",
            "\n",
            "8.227241039276123 seconds\n",
            "epoch 2:\n",
            "loss_D: 0.144, loss_G_fake: 0.616,             \n",
            "loss_perturb: 12.362, loss_adv: 71.181, \n",
            "\n",
            "accuracy in training set after perturbation: 59.004310\n",
            "\n",
            "16.45018458366394 seconds\n",
            "epoch 3:\n",
            "loss_D: 0.116, loss_G_fake: 0.702,             \n",
            "loss_perturb: 10.962, loss_adv: 66.485, \n",
            "\n",
            "accuracy in training set after perturbation: 55.288327\n",
            "\n",
            "24.70212173461914 seconds\n",
            "epoch 4:\n",
            "loss_D: 0.105, loss_G_fake: 0.733,             \n",
            "loss_perturb: 10.715, loss_adv: 63.494, \n",
            "\n",
            "accuracy in training set after perturbation: 53.026091\n",
            "\n",
            "32.97036147117615 seconds\n",
            "epoch 5:\n",
            "loss_D: 0.089, loss_G_fake: 0.777,             \n",
            "loss_perturb: 9.292, loss_adv: 59.855, \n",
            "\n",
            "accuracy in training set after perturbation: 49.906909\n",
            "\n",
            "41.20563268661499 seconds\n",
            "epoch 6:\n",
            "loss_D: 0.088, loss_G_fake: 0.794,             \n",
            "loss_perturb: 8.277, loss_adv: 57.647, \n",
            "\n",
            "accuracy in training set after perturbation: 48.149660\n",
            "\n",
            "49.47005295753479 seconds\n",
            "epoch 7:\n",
            "loss_D: 0.079, loss_G_fake: 0.813,             \n",
            "loss_perturb: 7.404, loss_adv: 55.232, \n",
            "\n",
            "accuracy in training set after perturbation: 46.145018\n",
            "\n",
            "57.83163595199585 seconds\n",
            "epoch 8:\n",
            "loss_D: 0.077, loss_G_fake: 0.826,             \n",
            "loss_perturb: 7.174, loss_adv: 53.893, \n",
            "\n",
            "accuracy in training set after perturbation: 45.183504\n",
            "\n",
            "66.11850118637085 seconds\n",
            "epoch 9:\n",
            "loss_D: 0.072, loss_G_fake: 0.843,             \n",
            "loss_perturb: 6.959, loss_adv: 51.695, \n",
            "\n",
            "accuracy in training set after perturbation: 43.321686\n",
            "\n",
            "74.4712438583374 seconds\n",
            "epoch 10:\n",
            "loss_D: 0.065, loss_G_fake: 0.854,             \n",
            "loss_perturb: 6.619, loss_adv: 50.210, \n",
            "\n",
            "accuracy in training set after perturbation: 42.179092\n",
            "\n",
            "82.67108106613159 seconds\n",
            "epoch 11:\n",
            "loss_D: 0.068, loss_G_fake: 0.857,             \n",
            "loss_perturb: 7.061, loss_adv: 49.038, \n",
            "\n",
            "accuracy in training set after perturbation: 41.171670\n",
            "\n",
            "90.87798285484314 seconds\n",
            "epoch 12:\n",
            "loss_D: 0.059, loss_G_fake: 0.869,             \n",
            "loss_perturb: 6.409, loss_adv: 47.067, \n",
            "\n",
            "accuracy in training set after perturbation: 39.518988\n",
            "\n",
            "99.03987574577332 seconds\n",
            "epoch 13:\n",
            "loss_D: 0.055, loss_G_fake: 0.882,             \n",
            "loss_perturb: 6.109, loss_adv: 45.872, \n",
            "\n",
            "accuracy in training set after perturbation: 38.534520\n",
            "\n",
            "107.22725224494934 seconds\n",
            "epoch 14:\n",
            "loss_D: 0.070, loss_G_fake: 0.855,             \n",
            "loss_perturb: 6.196, loss_adv: 45.149, \n",
            "\n",
            "accuracy in training set after perturbation: 37.950471\n",
            "\n",
            "115.38964748382568 seconds\n",
            "epoch 15:\n",
            "loss_D: 0.078, loss_G_fake: 0.847,             \n",
            "loss_perturb: 6.060, loss_adv: 44.155, \n",
            "\n",
            "accuracy in training set after perturbation: 37.098625\n",
            "\n",
            "123.57175970077515 seconds\n",
            "epoch 16:\n",
            "loss_D: 0.060, loss_G_fake: 0.884,             \n",
            "loss_perturb: 6.669, loss_adv: 43.500, \n",
            "\n",
            "accuracy in training set after perturbation: 36.621694\n",
            "\n",
            "131.7495939731598 seconds\n",
            "epoch 17:\n",
            "loss_D: 0.071, loss_G_fake: 0.860,             \n",
            "loss_perturb: 6.029, loss_adv: 42.848, \n",
            "\n",
            "accuracy in training set after perturbation: 36.022342\n",
            "\n",
            "139.98572731018066 seconds\n",
            "epoch 18:\n",
            "loss_D: 0.048, loss_G_fake: 0.906,             \n",
            "loss_perturb: 5.928, loss_adv: 42.325, \n",
            "\n",
            "accuracy in training set after perturbation: 35.693336\n",
            "\n",
            "148.2668058872223 seconds\n",
            "epoch 19:\n",
            "loss_D: 0.064, loss_G_fake: 0.874,             \n",
            "loss_perturb: 5.911, loss_adv: 41.500, \n",
            "\n",
            "accuracy in training set after perturbation: 34.923104\n",
            "\n",
            "156.68914699554443 seconds\n",
            "epoch 20:\n",
            "loss_D: 0.042, loss_G_fake: 0.913,             \n",
            "loss_perturb: 5.581, loss_adv: 41.037, \n",
            "\n",
            "accuracy in training set after perturbation: 34.522686\n",
            "\n",
            "165.12256932258606 seconds\n",
            "epoch 21:\n",
            "loss_D: 0.053, loss_G_fake: 0.899,             \n",
            "loss_perturb: 5.559, loss_adv: 40.220, \n",
            "\n",
            "accuracy in training set after perturbation: 33.951389\n",
            "\n",
            "173.35019087791443 seconds\n",
            "epoch 22:\n",
            "loss_D: 0.049, loss_G_fake: 0.904,             \n",
            "loss_perturb: 5.696, loss_adv: 39.606, \n",
            "\n",
            "accuracy in training set after perturbation: 33.390293\n",
            "\n",
            "181.54802441596985 seconds\n",
            "epoch 23:\n",
            "loss_D: 0.049, loss_G_fake: 0.908,             \n",
            "loss_perturb: 5.257, loss_adv: 38.895, \n",
            "\n",
            "accuracy in training set after perturbation: 32.834298\n",
            "\n",
            "189.6898694038391 seconds\n",
            "epoch 24:\n",
            "loss_D: 0.047, loss_G_fake: 0.910,             \n",
            "loss_perturb: 5.632, loss_adv: 38.610, \n",
            "\n",
            "accuracy in training set after perturbation: 32.576704\n",
            "\n",
            "197.93361139297485 seconds\n",
            "epoch 25:\n",
            "loss_D: 0.045, loss_G_fake: 0.914,             \n",
            "loss_perturb: 5.374, loss_adv: 37.955, \n",
            "\n",
            "accuracy in training set after perturbation: 31.977352\n",
            "\n",
            "206.21756601333618 seconds\n",
            "epoch 26:\n",
            "loss_D: 0.046, loss_G_fake: 0.913,             \n",
            "loss_perturb: 5.509, loss_adv: 37.373, \n",
            "\n",
            "accuracy in training set after perturbation: 31.561631\n",
            "\n",
            "214.50551652908325 seconds\n",
            "epoch 27:\n",
            "loss_D: 0.050, loss_G_fake: 0.912,             \n",
            "loss_perturb: 5.157, loss_adv: 37.013, \n",
            "\n",
            "accuracy in training set after perturbation: 31.135709\n",
            "\n",
            "222.72405624389648 seconds\n",
            "epoch 28:\n",
            "loss_D: 0.036, loss_G_fake: 0.935,             \n",
            "loss_perturb: 5.387, loss_adv: 36.563, \n",
            "\n",
            "accuracy in training set after perturbation: 30.903619\n",
            "\n",
            "231.00558257102966 seconds\n",
            "epoch 29:\n",
            "loss_D: 0.033, loss_G_fake: 0.940,             \n",
            "loss_perturb: 4.879, loss_adv: 36.123, \n",
            "\n",
            "accuracy in training set after perturbation: 30.441990\n",
            "\n",
            "239.27886128425598 seconds\n",
            "epoch 30:\n",
            "loss_D: 0.047, loss_G_fake: 0.922,             \n",
            "loss_perturb: 5.175, loss_adv: 35.706, \n",
            "\n",
            "accuracy in training set after perturbation: 30.064526\n",
            "\n",
            "247.51525688171387 seconds\n",
            "epoch 31:\n",
            "loss_D: 0.044, loss_G_fake: 0.922,             \n",
            "loss_perturb: 5.147, loss_adv: 35.661, \n",
            "\n",
            "accuracy in training set after perturbation: 30.123186\n",
            "\n",
            "255.7327959537506 seconds\n",
            "epoch 32:\n",
            "loss_D: 0.038, loss_G_fake: 0.929,             \n",
            "loss_perturb: 5.344, loss_adv: 35.306, \n",
            "\n",
            "accuracy in training set after perturbation: 29.832436\n",
            "\n",
            "263.91943311691284 seconds\n",
            "epoch 33:\n",
            "loss_D: 0.033, loss_G_fake: 0.943,             \n",
            "loss_perturb: 5.270, loss_adv: 34.909, \n",
            "\n",
            "accuracy in training set after perturbation: 29.574843\n",
            "\n",
            "272.1688368320465 seconds\n",
            "epoch 34:\n",
            "loss_D: 0.040, loss_G_fake: 0.930,             \n",
            "loss_perturb: 5.104, loss_adv: 34.604, \n",
            "\n",
            "accuracy in training set after perturbation: 29.184626\n",
            "\n",
            "280.34215235710144 seconds\n",
            "epoch 35:\n",
            "loss_D: 0.043, loss_G_fake: 0.916,             \n",
            "loss_perturb: 4.863, loss_adv: 34.208, \n",
            "\n",
            "accuracy in training set after perturbation: 28.832666\n",
            "\n",
            "288.4893515110016 seconds\n",
            "epoch 36:\n",
            "loss_D: 0.037, loss_G_fake: 0.937,             \n",
            "loss_perturb: 4.669, loss_adv: 33.951, \n",
            "\n",
            "accuracy in training set after perturbation: 28.608228\n",
            "\n",
            "296.6715397834778 seconds\n",
            "epoch 37:\n",
            "loss_D: 0.034, loss_G_fake: 0.947,             \n",
            "loss_perturb: 4.854, loss_adv: 33.742, \n",
            "\n",
            "accuracy in training set after perturbation: 28.501109\n",
            "\n",
            "305.0485634803772 seconds\n",
            "epoch 38:\n",
            "loss_D: 0.035, loss_G_fake: 0.939,             \n",
            "loss_perturb: 4.725, loss_adv: 33.502, \n",
            "\n",
            "accuracy in training set after perturbation: 28.256268\n",
            "\n",
            "313.2404143810272 seconds\n",
            "epoch 39:\n",
            "loss_D: 0.027, loss_G_fake: 0.952,             \n",
            "loss_perturb: 4.833, loss_adv: 33.413, \n",
            "\n",
            "accuracy in training set after perturbation: 28.251167\n",
            "\n",
            "321.3672652244568 seconds\n",
            "epoch 40:\n",
            "loss_D: 0.033, loss_G_fake: 0.948,             \n",
            "loss_perturb: 4.824, loss_adv: 33.235, \n",
            "\n",
            "accuracy in training set after perturbation: 28.039481\n",
            "\n",
            "329.6338438987732 seconds\n",
            "epoch 41:\n",
            "loss_D: 0.042, loss_G_fake: 0.928,             \n",
            "loss_perturb: 4.696, loss_adv: 32.851, \n",
            "\n",
            "accuracy in training set after perturbation: 27.705374\n",
            "\n",
            "337.7232611179352 seconds\n",
            "epoch 42:\n",
            "loss_D: 0.035, loss_G_fake: 0.940,             \n",
            "loss_perturb: 4.796, loss_adv: 32.671, \n",
            "\n",
            "accuracy in training set after perturbation: 27.593155\n",
            "\n",
            "345.8138656616211 seconds\n",
            "epoch 43:\n",
            "loss_D: 0.027, loss_G_fake: 0.951,             \n",
            "loss_perturb: 4.345, loss_adv: 32.286, \n",
            "\n",
            "accuracy in training set after perturbation: 27.111122\n",
            "\n",
            "353.89932441711426 seconds\n",
            "epoch 44:\n",
            "loss_D: 0.031, loss_G_fake: 0.945,             \n",
            "loss_perturb: 4.463, loss_adv: 32.270, \n",
            "\n",
            "accuracy in training set after perturbation: 27.307506\n",
            "\n",
            "362.03830075263977 seconds\n",
            "epoch 45:\n",
            "loss_D: 0.039, loss_G_fake: 0.931,             \n",
            "loss_perturb: 4.756, loss_adv: 31.899, \n",
            "\n",
            "accuracy in training set after perturbation: 26.935142\n",
            "\n",
            "370.171594619751 seconds\n",
            "epoch 46:\n",
            "loss_D: 0.030, loss_G_fake: 0.945,             \n",
            "loss_perturb: 4.496, loss_adv: 31.933, \n",
            "\n",
            "accuracy in training set after perturbation: 26.996353\n",
            "\n",
            "378.2615487575531 seconds\n",
            "epoch 47:\n",
            "loss_D: 0.035, loss_G_fake: 0.939,             \n",
            "loss_perturb: 4.610, loss_adv: 31.744, \n",
            "\n",
            "accuracy in training set after perturbation: 26.858629\n",
            "\n",
            "386.3806507587433 seconds\n",
            "epoch 48:\n",
            "loss_D: 0.032, loss_G_fake: 0.950,             \n",
            "loss_perturb: 4.935, loss_adv: 31.713, \n",
            "\n",
            "accuracy in training set after perturbation: 26.861180\n",
            "\n",
            "394.6350212097168 seconds\n",
            "epoch 49:\n",
            "loss_D: 0.028, loss_G_fake: 0.955,             \n",
            "loss_perturb: 4.677, loss_adv: 31.281, \n",
            "\n",
            "accuracy in training set after perturbation: 26.486266\n",
            "\n",
            "402.7395272254944 seconds\n",
            "epoch 50:\n",
            "loss_D: 0.023, loss_G_fake: 0.961,             \n",
            "loss_perturb: 4.922, loss_adv: 30.482, \n",
            "\n",
            "accuracy in training set after perturbation: 25.489046\n",
            "\n",
            "410.87857818603516 seconds\n",
            "epoch 51:\n",
            "loss_D: 0.022, loss_G_fake: 0.962,             \n",
            "loss_perturb: 4.480, loss_adv: 29.884, \n",
            "\n",
            "accuracy in training set after perturbation: 24.688209\n",
            "\n",
            "418.9835112094879 seconds\n",
            "epoch 52:\n",
            "loss_D: 0.020, loss_G_fake: 0.965,             \n",
            "loss_perturb: 4.119, loss_adv: 29.589, \n",
            "\n",
            "accuracy in training set after perturbation: 24.275039\n",
            "\n",
            "427.07182240486145 seconds\n",
            "epoch 53:\n",
            "loss_D: 0.020, loss_G_fake: 0.965,             \n",
            "loss_perturb: 3.852, loss_adv: 29.407, \n",
            "\n",
            "accuracy in training set after perturbation: 24.139866\n",
            "\n",
            "435.17400789260864 seconds\n",
            "epoch 54:\n",
            "loss_D: 0.020, loss_G_fake: 0.966,             \n",
            "loss_perturb: 3.656, loss_adv: 29.290, \n",
            "\n",
            "accuracy in training set after perturbation: 24.017445\n",
            "\n",
            "443.24895906448364 seconds\n",
            "epoch 55:\n",
            "loss_D: 0.019, loss_G_fake: 0.967,             \n",
            "loss_perturb: 3.489, loss_adv: 29.159, \n",
            "\n",
            "accuracy in training set after perturbation: 23.917978\n",
            "\n",
            "451.356317281723 seconds\n",
            "epoch 56:\n",
            "loss_D: 0.018, loss_G_fake: 0.969,             \n",
            "loss_perturb: 3.369, loss_adv: 29.049, \n",
            "\n",
            "accuracy in training set after perturbation: 23.800658\n",
            "\n",
            "459.48305439949036 seconds\n",
            "epoch 57:\n",
            "loss_D: 0.018, loss_G_fake: 0.970,             \n",
            "loss_perturb: 3.252, loss_adv: 28.922, \n",
            "\n",
            "accuracy in training set after perturbation: 23.716494\n",
            "\n",
            "467.69360637664795 seconds\n",
            "epoch 58:\n",
            "loss_D: 0.018, loss_G_fake: 0.971,             \n",
            "loss_perturb: 3.174, loss_adv: 28.852, \n",
            "\n",
            "accuracy in training set after perturbation: 23.698641\n",
            "\n",
            "476.0208902359009 seconds\n",
            "epoch 59:\n",
            "loss_D: 0.018, loss_G_fake: 0.972,             \n",
            "loss_perturb: 3.114, loss_adv: 28.720, \n",
            "\n",
            "accuracy in training set after perturbation: 23.532862\n",
            "\n",
            "484.2018826007843 seconds\n",
            "epoch 60:\n",
            "loss_D: 0.018, loss_G_fake: 0.971,             \n",
            "loss_perturb: 3.076, loss_adv: 28.623, \n",
            "\n",
            "accuracy in training set after perturbation: 23.502257\n",
            "\n",
            "492.37103486061096 seconds\n",
            "epoch 61:\n",
            "loss_D: 0.018, loss_G_fake: 0.971,             \n",
            "loss_perturb: 3.052, loss_adv: 28.521, \n",
            "\n",
            "accuracy in training set after perturbation: 23.392588\n",
            "\n",
            "500.5475220680237 seconds\n",
            "epoch 62:\n",
            "loss_D: 0.018, loss_G_fake: 0.972,             \n",
            "loss_perturb: 3.010, loss_adv: 28.453, \n",
            "\n",
            "accuracy in training set after perturbation: 23.313525\n",
            "\n",
            "508.7130148410797 seconds\n",
            "epoch 63:\n",
            "loss_D: 0.017, loss_G_fake: 0.973,             \n",
            "loss_perturb: 2.982, loss_adv: 28.370, \n",
            "\n",
            "accuracy in training set after perturbation: 23.257415\n",
            "\n",
            "516.883820772171 seconds\n",
            "epoch 64:\n",
            "loss_D: 0.017, loss_G_fake: 0.972,             \n",
            "loss_perturb: 2.974, loss_adv: 28.287, \n",
            "\n",
            "accuracy in training set after perturbation: 23.201306\n",
            "\n",
            "525.0407772064209 seconds\n",
            "epoch 65:\n",
            "loss_D: 0.017, loss_G_fake: 0.973,             \n",
            "loss_perturb: 2.951, loss_adv: 28.231, \n",
            "\n",
            "accuracy in training set after perturbation: 23.198755\n",
            "\n",
            "533.1829853057861 seconds\n",
            "epoch 66:\n",
            "loss_D: 0.017, loss_G_fake: 0.973,             \n",
            "loss_perturb: 2.935, loss_adv: 28.138, \n",
            "\n",
            "accuracy in training set after perturbation: 23.045729\n",
            "\n",
            "541.3037633895874 seconds\n",
            "epoch 67:\n",
            "loss_D: 0.017, loss_G_fake: 0.973,             \n",
            "loss_perturb: 2.911, loss_adv: 28.058, \n",
            "\n",
            "accuracy in training set after perturbation: 23.015124\n",
            "\n",
            "549.4864177703857 seconds\n",
            "epoch 68:\n",
            "loss_D: 0.016, loss_G_fake: 0.974,             \n",
            "loss_perturb: 2.873, loss_adv: 27.991, \n",
            "\n",
            "accuracy in training set after perturbation: 22.941161\n",
            "\n",
            "557.7759466171265 seconds\n",
            "epoch 69:\n",
            "loss_D: 0.016, loss_G_fake: 0.972,             \n",
            "loss_perturb: 2.849, loss_adv: 27.924, \n",
            "\n",
            "accuracy in training set after perturbation: 22.882501\n",
            "\n",
            "566.0812644958496 seconds\n",
            "epoch 70:\n",
            "loss_D: 0.016, loss_G_fake: 0.973,             \n",
            "loss_perturb: 2.845, loss_adv: 27.870, \n",
            "\n",
            "accuracy in training set after perturbation: 22.890153\n",
            "\n",
            "574.2772579193115 seconds\n",
            "epoch 71:\n",
            "loss_D: 0.016, loss_G_fake: 0.973,             \n",
            "loss_perturb: 2.842, loss_adv: 27.816, \n",
            "\n",
            "accuracy in training set after perturbation: 22.826392\n",
            "\n",
            "582.4483897686005 seconds\n",
            "epoch 72:\n",
            "loss_D: 0.016, loss_G_fake: 0.974,             \n",
            "loss_perturb: 2.817, loss_adv: 27.750, \n",
            "\n",
            "accuracy in training set after perturbation: 22.772833\n",
            "\n",
            "590.6320564746857 seconds\n",
            "epoch 73:\n",
            "loss_D: 0.016, loss_G_fake: 0.974,             \n",
            "loss_perturb: 2.793, loss_adv: 27.691, \n",
            "\n",
            "accuracy in training set after perturbation: 22.663164\n",
            "\n",
            "598.8190894126892 seconds\n",
            "epoch 74:\n",
            "loss_D: 0.015, loss_G_fake: 0.974,             \n",
            "loss_perturb: 2.783, loss_adv: 27.655, \n",
            "\n",
            "accuracy in training set after perturbation: 22.652962\n",
            "\n",
            "607.0591249465942 seconds\n",
            "epoch 75:\n",
            "loss_D: 0.016, loss_G_fake: 0.974,             \n",
            "loss_perturb: 2.767, loss_adv: 27.599, \n",
            "\n",
            "accuracy in training set after perturbation: 22.568798\n",
            "\n",
            "615.5585842132568 seconds\n",
            "epoch 76:\n",
            "loss_D: 0.016, loss_G_fake: 0.974,             \n",
            "loss_perturb: 2.749, loss_adv: 27.576, \n",
            "\n",
            "accuracy in training set after perturbation: 22.558596\n",
            "\n",
            "623.7971677780151 seconds\n",
            "epoch 77:\n",
            "loss_D: 0.015, loss_G_fake: 0.974,             \n",
            "loss_perturb: 2.735, loss_adv: 27.493, \n",
            "\n",
            "accuracy in training set after perturbation: 22.431074\n",
            "\n",
            "632.066211938858 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TSFClassifier_TF.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tortoisehare/TSR-GAN/blob/master/TSFClassifier_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isD9mtH373R4",
        "colab_type": "text"
      },
      "source": [
        "Modified by: Stephanie Tietz\n",
        "\n",
        "Original Model from: muddassir235 on full German Traffic Sign dataset\n",
        "\n",
        "https://github.com/muddassir235/German-Traffic-Sign-Classifier\n",
        "\n",
        "Modified to work with German TS Dataset plus \"not a sign\" images, optimized for 3 classes first then expanding to 44\n",
        "\n",
        "Multiclass Image Classification in TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc_cvQN_70HP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5l_XxOH8Lgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#upload data\n",
        "#from google.colab import files\n",
        "\n",
        "#uploaded = files.upload()\n",
        "\n",
        "#traindata = np.load('smalltrain.npy') #only 3 classes, both circle signs\n",
        "#traindata = np.load('alltrain.npy') #all classes\n",
        "\n",
        "#testdata = np.load('smalltest.npy')\n",
        "#testdata = np.load('alltest.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bDJvjGb8geR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "ca84c0f9-9879-4347-f4a2-c97424a2de7d"
      },
      "source": [
        "traindata = np.load('smalltrain2.npy') #smalltrain2 has triangle sign\n",
        "testdata = np.load('smalltest2.npy')\n",
        "\n",
        "print(traindata.shape) #expect (5620, 3073) for triangle dataset, (41109,3073) for all\n",
        "print(testdata.shape) #expect (1900, 3073) for triangle dataset, (13330,3073) for all\n",
        "print(testdata)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5620, 3073)\n",
            "(1900, 3073)\n",
            "[[214 217 218 ... 134 136   2]\n",
            " [123 123 126 ...  51  47   2]\n",
            " [ 98  98  93 ...  99  94   2]\n",
            " ...\n",
            " [ 21  38  69 ...  37  27   1]\n",
            " [ 28  85  10 ...  50  21   0]\n",
            " [ 18  35  57 ...  19  29   1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WZHWYqi8lMi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "3e61d2b2-6339-4424-c063-0cb8ea264e4f"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "#Randomize (shuffle) the data\n",
        "traindata = shuffle(traindata, random_state=33)\n",
        "testdata = shuffle(testdata, random_state=37)\n",
        "print(testdata)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 75  86 137 ...  50  54   2]\n",
            " [132  87  85 ...  25 197   1]\n",
            " [255 255 255 ... 255 255   2]\n",
            " ...\n",
            " [164 129  89 ... 116 125   2]\n",
            " [ 16 131 189 ... 117 153   1]\n",
            " [188 182 187 ... 129 130   2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMi6286r8tVV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9fa2465b-66ec-4341-b40b-927d4dd875dc"
      },
      "source": [
        "#split into x and y\n",
        "X_train = traindata[:,:-1]\n",
        "Y_train = traindata[:,-1]\n",
        "Y_train = Y_train.reshape(len(Y_train),1)\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "\n",
        "X_test = testdata[:,:-1]\n",
        "Y_test = testdata[:,-1]\n",
        "Y_test = Y_test.reshape(len(Y_test),1)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)\n",
        "\n",
        "#reshape into (32,32,3) for each example for this model input\n",
        "X_train = (X_train.reshape(traindata.shape[0], 3, 32, 32)).transpose([0,2,3,1])\n",
        "print(X_train.shape)\n",
        "X_test = (X_test.reshape(testdata.shape[0], 3, 32, 32)).transpose([0,2,3,1])\n",
        "print(X_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5620, 3072)\n",
            "(5620, 1)\n",
            "(1900, 3072)\n",
            "(1900, 1)\n",
            "(5620, 32, 32, 3)\n",
            "(1900, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLk-mamjn4LL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Normalize data\n",
        "X_train = np.array(X_train)/255\n",
        "X_test = np.array(X_test)/255\n",
        "\n",
        "#compute mean image of training set and subtract from training images\n",
        "chann_index_swap = np.swapaxes(X_train,0,3)\n",
        "mean_image = [[[sum(pixel)/len(pixel) for pixel in col] for col in row] for row in chann_index_swap]\n",
        "mean_image = np.swapaxes(mean_image,0,2)\n",
        "mean_image = np.swapaxes(mean_image,0,1)\n",
        "\n",
        "X_train = X_train - mean_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rptTydNb8otG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 3 #change to 44 for alldata\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "num_epochs = 15\n",
        "\n",
        "assert(len(X_train)==len(Y_train))\n",
        "n_train = len(X_train)\n",
        "assert(len(X_test)==len(Y_test))\n",
        "n_test = len(X_test)\n",
        "\n",
        "image_size = 32\n",
        "image_shape = (32, 32, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAxWJ3R-kR9g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "eee18285-af62-4a15-800d-8dcfcf474d37"
      },
      "source": [
        "#Histogram visualization of classes\n",
        "hist_data = np.histogram(Y_train, bins=range(num_classes+1))\n",
        "hist_map = {}\n",
        "for occr,i in zip(hist_data[0], hist_data[1]):\n",
        "  hist_map[occr] = i\n",
        "\n",
        "plt.hist(Y_train, bins=range(num_classes+1), color = '#00ffCC')\n",
        "plt.axis([0,num_classes,0,2500])\n",
        "plt.show()\n",
        "print(\"Class with most examples:\")\n",
        "print(hist_map[np.amax(hist_data[0])],np.amax(hist_data[0]))\n",
        "print(\"Class with least examples:\")\n",
        "print(hist_map[np.amin(hist_data[0])],np.amin(hist_data[0]))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD8CAYAAACYebj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndYaCJKkzFCRJnaEgSer+D8o/TpwMm3ZdAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Class with most examples:\n",
            "0 2220\n",
            "Class with least examples:\n",
            "1 1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zmowz0_mDdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Convenience Function'''\n",
        "def conv2d(x,W,b,output_depth,train=1,strides=1):\n",
        "\n",
        "    '''Tensor flow conv2d with same vertical and horizontal strides'''\n",
        "    x = tf.nn.conv2d(x,W,strides=[1,strides,strides,1],padding='VALID')\n",
        "    \n",
        "    '''Batch Normaliztion'''\n",
        "    x = batch_norm_conv(x,output_depth,train)\n",
        "    \n",
        "    '''Adding the bias'''\n",
        "    x = tf.nn.bias_add(x,b)\n",
        "    \n",
        "    '''Relu Activation'''\n",
        "    return tf.nn.relu(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYEV0rtlmL3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Convenience Function'''\n",
        "def max_pool(x,k=2):\n",
        "    return tf.nn.max_pool(x,ksize=[1,k,k,1],strides=[1,k,k,1],padding='VALID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZMiB4pvmSfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#7 layers, 2 conv, 4 fully connected, plus output\n",
        "depth = {\n",
        "    '1_c': 30, #5x5\n",
        "    '2_c': 200, #5x5\n",
        "    '3_f': 2200,\n",
        "    '4_f': 1000,\n",
        "    '5_f': 500,\n",
        "    '6_f': 120,\n",
        "    '7_o': num_classes\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbhxm3FLmfQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "decce88b-13f5-4051-f83f-d4cd9d7daa99"
      },
      "source": [
        "#Weights Initialization using Xavier\n",
        "\n",
        "weights = {\n",
        "    '1_conv': tf.get_variable(\"W_1_conv\", shape=[5, 5, 3, depth['1_c']],initializer=tf.contrib.layers.xavier_initializer()),\n",
        "    \n",
        "    '2_conv': tf.get_variable(\"W_2_conv\", shape=[5, 5, depth['1_c'], depth['2_c']],initializer=tf.contrib.layers.xavier_initializer()),\n",
        "    \n",
        "    '3_fullc': tf.get_variable(\"W_3_fullc\", shape=[5000, depth['3_f']],initializer=tf.contrib.layers.xavier_initializer()),\n",
        "    \n",
        "    '4_fullc': tf.get_variable(\"W_4_fullc\", shape=[depth['3_f'], depth['4_f']],initializer=tf.contrib.layers.xavier_initializer()),\n",
        "    \n",
        "    '5_fullc': tf.get_variable(\"W_5_fullc\", shape=[depth['4_f'], depth['5_f']],initializer=tf.contrib.layers.xavier_initializer()),\n",
        "    \n",
        "    '6_fullc': tf.get_variable(\"W_6_fullc\", shape=[depth['5_f'], depth['6_f']],initializer=tf.contrib.layers.xavier_initializer()),\n",
        "    \n",
        "    '7_out': tf.get_variable(\"W_7_out\", shape=[depth['6_f'], depth['7_o']],initializer=tf.contrib.layers.xavier_initializer())\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_NIzrEvmny1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Biases'''\n",
        "biases = {\n",
        "    '1_conv': tf.Variable(tf.zeros(depth['1_c'])),\n",
        "    '2_conv': tf.Variable(tf.zeros(depth['2_c'])),\n",
        "    '3_fullc': tf.Variable(tf.zeros(depth['3_f'])),\n",
        "    '4_fullc': tf.Variable(tf.zeros(depth['4_f'])),\n",
        "    '5_fullc': tf.Variable(tf.zeros(depth['5_f'])),\n",
        "    '6_fullc': tf.Variable(tf.zeros(depth['6_f'])),\n",
        "    '7_out': tf.Variable(tf.zeros(depth['7_o']))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efDEmuHAmoj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_norm_conv(x, n_out, train):\n",
        "    \"\"\"\n",
        "    Batch normalization on convolutional maps.\n",
        "    Ref.: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n",
        "    Args:\n",
        "        x:           Tensor, 4D BHWD input maps\n",
        "        n_out:       integer, depth of input maps\n",
        "        phase_train: boolean tf.Varialbe, true indicates training phase\n",
        "        scope:       string, variable scope\n",
        "    Return:\n",
        "        normed:      batch-normalized maps\n",
        "    \"\"\"\n",
        "    with tf.variable_scope('bn'):\n",
        "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
        "                                     name='beta', trainable=True)\n",
        "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
        "                                      name='gamma', trainable=True)\n",
        "        batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n",
        "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
        "\n",
        "        def mean_var_with_update():\n",
        "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
        "            with tf.control_dependencies([ema_apply_op]):\n",
        "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
        "\n",
        "        mean, var = tf.cond(train>0,mean_var_with_update,lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
        "        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
        "    return normed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a2EIoP1mxqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_norm_fullc(x, n_out, train):\n",
        "    \n",
        "    '''Batch normalization for fully connected layers'''\n",
        "    \n",
        "    with tf.variable_scope('bn'):\n",
        "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
        "                                     name='beta', trainable=True)\n",
        "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
        "                                      name='gamma', trainable=True)\n",
        "        batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n",
        "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
        "\n",
        "        def mean_var_with_update():\n",
        "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
        "            with tf.control_dependencies([ema_apply_op]):\n",
        "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
        "\n",
        "        mean, var = tf.cond(train>0,\n",
        "                            mean_var_with_update,\n",
        "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
        "        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
        "    return normed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__m7bsHwm5QQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "def TS_Classifier(x,train=1):  \n",
        "    \n",
        "    ''' Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x30. '''\n",
        "    conv1 = conv2d(x,weights['1_conv'],biases['1_conv'],depth['1_c'],train)\n",
        "\n",
        "    # Max Pooling. Input = 28x28x30. Output = 14x14x30.\n",
        "    conv1 = max_pool(conv1)\n",
        "    \n",
        "    \n",
        "    ''' Layer 2: Convolutional. Input = 14x14x30 Output = 10x10x200. '''\n",
        "    \n",
        "    conv2 = conv2d(conv1,weights['2_conv'],biases['2_conv'],depth['2_c'],train)\n",
        "\n",
        "    # Max Pooling. Input = 10x10x200. Output = 10x10x200.\n",
        "    conv2 = max_pool(conv2)\n",
        "    \n",
        "    \n",
        "    ''' Flatten: Input = 10x10x200. Output = 5000 '''\n",
        "    \n",
        "    fc0 = keras.layers.Flatten()(conv2)\n",
        "    \n",
        "    \n",
        "    ''' Layer 3: Fully Connected. Input = 5000. Output = 2200. '''\n",
        "    \n",
        "    fullc1 = tf.add(tf.matmul(fc0,weights['3_fullc']),biases['3_fullc'])\n",
        "    \n",
        "    # Batch Normalization. (Only During Training)\n",
        "    fullc1 = batch_norm_fullc(fullc1,depth['3_f'],train)\n",
        "    \n",
        "    # Relu Activation.\n",
        "    fullc1 = tf.nn.relu(fullc1)\n",
        "    \n",
        "    # Dropout: 0.5 (Only During Training)\n",
        "    fullc1 = tf.cond(train>0, lambda: tf.nn.dropout(fullc1,0.50), lambda: fullc1)\n",
        "    \n",
        "    \n",
        "    ''' Layer 4: Fully Connected. Input = 2200. Output = 1000'''\n",
        "    \n",
        "    fullc2 = tf.add(tf.matmul(fullc1,weights['4_fullc']),biases['4_fullc'])\n",
        "    \n",
        "    # Batch Normalization (Only During Training)\n",
        "    fullc2 = batch_norm_fullc(fullc2,depth['4_f'],train)\n",
        "    \n",
        "    # Relu Activation\n",
        "    fullc2 = tf.nn.relu(fullc2)\n",
        "    \n",
        "    # Dropout: 0.5 (Only During Training)\n",
        "    fullc2 = tf.cond(train>0, lambda: tf.nn.dropout(fullc2,0.50), lambda: fullc2)\n",
        "    \n",
        "    \n",
        "    ''' Layer 5: Fully Connected. Input = 1000. Output = 500'''\n",
        "    \n",
        "    fullc3 = tf.add(tf.matmul(fullc2,weights['5_fullc']),biases['5_fullc'])\n",
        "    \n",
        "    # Batch normalization. (Only During Training)\n",
        "    fullc3 = batch_norm_fullc(fullc3,depth['5_f'],train)\n",
        "    \n",
        "        # Relu Activation\n",
        "    fullc3 = tf.nn.relu(fullc3)\n",
        "    \n",
        "    # Dropout: 0.5 (Only During Training)\n",
        "    fullc3 = tf.cond(train>0, lambda: tf.nn.dropout(fullc3,0.50), lambda: fullc3)\n",
        "    \n",
        "    \n",
        "    ''' Layer 6: Fully Connected. Input = 500. Output = 120'''\n",
        "    \n",
        "    fullc4 = tf.add(tf.matmul(fullc3,weights['6_fullc']),biases['6_fullc'])\n",
        "    \n",
        "    # Batch normalization. (Only During Training)\n",
        "    fullc4 = batch_norm_fullc(fullc4,depth['6_f'],train)\n",
        "    \n",
        "    # Relu Activation\n",
        "    fullc4 = tf.nn.relu(fullc4)\n",
        "    \n",
        "    # Dropout 0.5 (Only During Training)\n",
        "    fullc4 = tf.cond(train>0, lambda: tf.nn.dropout(fullc4,0.50), lambda: fullc4)\n",
        "    \n",
        "    \n",
        "    ''' Layer 7: Output, Fully Connected. Input = 120. Output = num_classes'''\n",
        "    \n",
        "    logits = tf.add(tf.matmul(fullc4,weights['7_out']),biases['7_out'])\n",
        "    #print(logits.get_shape())\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFTnLCNOqAGu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc521c81-5b42-480b-e33b-9de646ef62f2"
      },
      "source": [
        "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
        "y = tf.placeholder(tf.int32, (None))\n",
        "\n",
        "one_hot_y = tf.one_hot(y,3)\n",
        "#print(one_hot_y.get_shape())\n",
        "\n",
        "\n",
        "\n",
        "'''Learning rate made into a placeholder for exponential decay'''\n",
        "rate = tf.placeholder(tf.float32)\n",
        "\n",
        "'''L2 Regularization Hyper-parameter'''\n",
        "beta = tf.placeholder(tf.float32)\n",
        "\n",
        "'''Parameter for switching between training and testing modes'''\n",
        "train = tf.placeholder(tf.int32)\n",
        "\n",
        "'''Initial Learning Rate'''\n",
        "lr = 0.0001\n",
        "\n",
        "'''L2 regularization (Beta)'''\n",
        "b = 1e-6\n",
        "\n",
        "'''Rate of Learning rate decay'''\n",
        "k = 1e-7\n",
        "\n",
        "\n",
        "'''MODEL'''\n",
        "logits = TS_Classifier(x,train)\n",
        "\n",
        "'''LOSS'''\n",
        "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y)\n",
        "loss = tf.reduce_mean(cross_entropy)\n",
        "loss = tf.reduce_mean(loss+beta*tf.nn.l2_loss(weights['1_conv']))\n",
        "loss = tf.reduce_mean(loss+beta*tf.nn.l2_loss(weights['2_conv']))\n",
        "loss = tf.reduce_mean(loss+beta*tf.nn.l2_loss(weights['3_fullc']))\n",
        "loss = tf.reduce_mean(loss+beta*tf.nn.l2_loss(weights['4_fullc']))\n",
        "loss = tf.reduce_mean(loss+beta*tf.nn.l2_loss(weights['5_fullc']))\n",
        "loss = tf.reduce_mean(loss+beta*tf.nn.l2_loss(weights['6_fullc']))\n",
        "loss = tf.reduce_mean(loss+beta*tf.nn.l2_loss(weights['7_out']))\n",
        "loss = tf.reduce_mean(loss+beta*tf.nn.l2_loss(biases['1_conv']))\n",
        "loss = tf.reduce_mean(loss+beta*tf.nn.l2_loss(biases['2_conv']))\n",
        "loss = tf.reduce_mean(loss+beta*tf.nn.l2_loss(biases['3_fullc']))\n",
        "loss = tf.reduce_mean(loss+beta*tf.nn.l2_loss(biases['4_fullc']))\n",
        "loss = tf.reduce_mean(loss+beta*tf.nn.l2_loss(biases['5_fullc']))\n",
        "loss = tf.reduce_mean(loss+beta*tf.nn.l2_loss(biases['6_fullc']))\n",
        "loss_operation = tf.reduce_mean(loss+beta*tf.nn.l2_loss(biases['7_out']))\n",
        "\n",
        "'''OPTIMIZER'''\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
        "training_operation = optimizer.minimize(loss_operation)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Add_156:0\", shape=(?, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLcpfARWpiAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct_prediction = tf.equal(tf.argmax(logits, axis=0), tf.argmax(one_hot_y, axis=0))\n",
        "\n",
        "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "def evaluate(X_data, y_data):\n",
        "    num_examples = len(X_data)\n",
        "    total_accuracy = 0\n",
        "    sess = tf.get_default_session()\n",
        "    #print(\"in evaluate\")\n",
        "    for offset in range(0, num_examples, BATCH_SIZE):\n",
        "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
        "\n",
        "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y,train:-1})\n",
        "        #print(\"after accuracy\")\n",
        "        total_accuracy += (accuracy * len(batch_x))\n",
        "    return total_accuracy / num_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ejafF-DqQTM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1768
        },
        "outputId": "a92c5c68-d2c7-4a2d-c7a0-9a5ea65031f4"
      },
      "source": [
        "import math\n",
        "\n",
        "''' Small Training Subset of 20 images '''\n",
        "small_sample_x = X_train[0:20]\n",
        "small_sample_y = Y_train[0:20]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer(),feed_dict={train:1})\n",
        "    num_examples = len(small_sample_x)\n",
        "    \n",
        "    batch_size = 10\n",
        "    print(\"Can we overfit?.......\")\n",
        "    print()\n",
        "    \n",
        "    print(\"Decay: \"+str(k))\n",
        "    for i in range(100):\n",
        "        \n",
        "        lr = float(lr*np.exp(-k*i))\n",
        "        \n",
        "        for offset in range(0,num_examples,batch_size):\n",
        "            end = offset + batch_size\n",
        "            batch_x, batch_y = small_sample_x[offset:end], Y_train[offset:end]\n",
        "            sess.run(training_operation, feed_dict={x:small_sample_x, y:small_sample_y, train:-1, rate:lr, beta:b})\n",
        "            \n",
        "        c = sess.run(loss_operation, feed_dict={x: small_sample_x, y: small_sample_y,train:-1, rate:lr, beta:b})\n",
        "        print(\"Epoch: \"+str(i)+\" loss: \"+str(c)+\" learn rate:\"+str(lr)+\" beta: \"+str(b)+\" decay: \"+str(k))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Can we overfit?.......\n",
            "\n",
            "Decay: 1e-07\n",
            "Epoch: 0 loss: 51065440.0 learn rate:0.0001 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 1 loss: 0.0026751799 learn rate:9.99999900000005e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 2 loss: 847262.4 learn rate:9.99999700000045e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 3 loss: 0.0026724054 learn rate:9.9999940000018e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 4 loss: 0.0026707884 learn rate:9.999990000005e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 5 loss: 148232.8 learn rate:9.999985000011251e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 6 loss: 0.0026671884 learn rate:9.99997900002205e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 7 loss: 47243.6 learn rate:9.999972000039199e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 8 loss: 0.0026633039 learn rate:9.9999640000648e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 9 loss: 0.0026612824 learn rate:9.99995500010125e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 10 loss: 0.002659195 learn rate:9.99994500015125e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 11 loss: 0.0026570498 learn rate:9.9999340002178e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 12 loss: 0.0026548535 learn rate:9.999922000304199e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 13 loss: 0.0026526002 learn rate:9.999909000414048e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 14 loss: 0.0026503094 learn rate:9.999895000551248e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 15 loss: 0.0026479885 learn rate:9.999880000719997e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 16 loss: 0.0026456355 learn rate:9.999864000924796e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 17 loss: 0.0026432537 learn rate:9.999847001170444e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 18 loss: 0.0026408357 learn rate:9.999829001462042e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 19 loss: 94661.7 learn rate:9.999810001804989e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 20 loss: 0.002635937 learn rate:9.999790002204985e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 21 loss: 0.0026334634 learn rate:9.99976900266803e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 22 loss: 0.0026309632 learn rate:9.999747003200423e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 23 loss: 0.0026284384 learn rate:9.999724003808764e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 24 loss: 0.002625892 learn rate:9.999700004499954e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 25 loss: 0.002623325 learn rate:9.999675005281192e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 26 loss: 0.0026207392 learn rate:9.999649006159976e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 27 loss: 0.0026181366 learn rate:9.999622007144107e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 28 loss: 0.002615518 learn rate:9.999594008241685e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 29 loss: 0.0026128858 learn rate:9.999565009461109e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 30 loss: 0.0026102406 learn rate:9.999535010811078e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 31 loss: 0.002607584 learn rate:9.999504012300592e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 32 loss: 0.0026049165 learn rate:9.99947201393895e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 33 loss: 0.0026022405 learn rate:9.999439015735752e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 34 loss: 0.0025995562 learn rate:9.999405017700894e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 35 loss: 0.0025968645 learn rate:9.999370019844578e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 36 loss: 0.0025941667 learn rate:9.999334022177303e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 37 loss: 0.0025914633 learn rate:9.999297024709866e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 38 loss: 0.0025887557 learn rate:9.999259027453367e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 39 loss: 0.002586044 learn rate:9.999220030419203e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 40 loss: 0.0025833293 learn rate:9.999180033619075e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 41 loss: 0.0025806127 learn rate:9.99913903706498e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 42 loss: 0.002577894 learn rate:9.999097040769216e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 43 loss: 0.0025751742 learn rate:9.999054044744382e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 44 loss: 0.002572454 learn rate:9.999010049003376e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 45 loss: 0.0025697341 learn rate:9.998965053559395e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 46 loss: 0.0025670154 learn rate:9.998919058425937e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 47 loss: 0.0025642973 learn rate:9.9988720636168e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 48 loss: 0.0025615813 learn rate:9.998824069146081e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 49 loss: 0.0025588677 learn rate:9.998775075028178e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 50 loss: 0.0025561561 learn rate:9.998725081277787e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 51 loss: 0.0025534483 learn rate:9.998674087909905e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 52 loss: 0.0025507442 learn rate:9.998622094939829e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 53 loss: 0.0025480438 learn rate:9.998569102383157e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 54 loss: 0.002545348 learn rate:9.998515110255782e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 55 loss: 0.0025426573 learn rate:9.998460118573903e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 56 loss: 0.0025399711 learn rate:9.998404127354015e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 57 loss: 0.002537291 learn rate:9.998347136612913e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 58 loss: 0.0025346167 learn rate:9.998289146367693e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 59 loss: 0.0025319483 learn rate:9.998230156635749e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 60 loss: 0.0025292865 learn rate:9.998170167434777e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 61 loss: 0.0025266316 learn rate:9.998109178782771e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 62 loss: 0.0025239838 learn rate:9.998047190698026e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 63 loss: 0.0025213433 learn rate:9.997984203199136e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 64 loss: 0.0025187107 learn rate:9.997920216304994e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 65 loss: 0.002516086 learn rate:9.997855230034794e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 66 loss: 0.0025134687 learn rate:9.997789244408029e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 67 loss: 0.0025108603 learn rate:9.99772225944449e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 68 loss: 0.0025082603 learn rate:9.997654275164273e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 69 loss: 0.0025056696 learn rate:9.997585291587769e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 70 loss: 0.0025030877 learn rate:9.997515308735668e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 71 loss: 0.0025005152 learn rate:9.997444326628963e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 72 loss: 0.002497952 learn rate:9.997372345288945e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 73 loss: 0.0024953983 learn rate:9.997299364737204e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 74 loss: 0.0024928546 learn rate:9.99722538499563e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 75 loss: 0.0024903205 learn rate:9.997150406086413e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 76 loss: 0.0024877968 learn rate:9.997074428032043e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 77 loss: 0.0024852834 learn rate:9.99699745085531e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 78 loss: 0.002482781 learn rate:9.996919474579301e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 79 loss: 0.0024802885 learn rate:9.996840499227404e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 80 loss: 0.0024778065 learn rate:9.996760524823308e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 81 loss: 0.002475336 learn rate:9.996679551391e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 82 loss: 0.0024728766 learn rate:9.996597578954766e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 83 loss: 0.0024704277 learn rate:9.996514607539192e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 84 loss: 0.0024679902 learn rate:9.996430637169164e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 85 loss: 0.002465564 learn rate:9.996345667869869e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 86 loss: 0.0024631498 learn rate:9.99625969966679e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 87 loss: 0.0024607468 learn rate:9.99617273258571e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 88 loss: 0.0024583552 learn rate:9.996084766652714e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 89 loss: 0.0024559756 learn rate:9.995995801894184e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 90 loss: 0.002453608 learn rate:9.995905838336804e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 91 loss: 0.0024512517 learn rate:9.995814876007554e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 92 loss: 0.002448908 learn rate:9.995722914933717e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 93 loss: 0.002446576 learn rate:9.995629955142871e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 94 loss: 0.0024442566 learn rate:9.995535996662897e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 95 loss: 0.002441949 learn rate:9.995441039521977e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 96 loss: 0.0024396537 learn rate:9.995345083748587e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 97 loss: 0.0024373708 learn rate:9.995248129371504e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 98 loss: 0.0024351 learn rate:9.995150176419807e-05 beta: 1e-06 decay: 1e-07\n",
            "Epoch: 99 loss: 0.002432842 learn rate:9.99505122492287e-05 beta: 1e-06 decay: 1e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp9-zu3Cvvey",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "46aeb30b-5859-4a28-9e88-fa1a84e4bfd9"
      },
      "source": [
        "EPOCHS = 100\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer(), feed_dict={train:1})\n",
        "    num_examples = len(X_train)\n",
        "    \n",
        "    print(\"Training...\")\n",
        "    print()\n",
        "    for i in range(EPOCHS):\n",
        "        lr = float(lr*np.exp(-k*i))\n",
        "        \n",
        "        #X_train, Y_train = shuffle(X_train, Y_train)\n",
        "        \n",
        "        for offset in range(0, num_examples, BATCH_SIZE):\n",
        "            end = offset + BATCH_SIZE\n",
        "            batch_x, batch_y = X_train[offset:end], Y_train[offset:end]\n",
        "            #print(batch_x.shape)\n",
        "            #print(batch_y.shape)\n",
        "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, train:1, rate:lr, beta:b})\n",
        "        \n",
        "        #print(X_test.shape)\n",
        "        #print(Y_test.shape)\n",
        "        \n",
        "        test_accuracy = evaluate(X_test, Y_test)\n",
        "        print(\"EPOCH {} ...\".format(i+1))\n",
        "        print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
        "        print()\n",
        "        \n",
        "    saver.save(sess, 'ts_classifier')\n",
        "    print(\"Model saved\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "\n",
            "in evaluate\n",
            "EPOCH 1 ...\n",
            "Test Accuracy = 0.195\n",
            "\n",
            "in evaluate\n",
            "EPOCH 2 ...\n",
            "Test Accuracy = 0.209\n",
            "\n",
            "in evaluate\n",
            "EPOCH 3 ...\n",
            "Test Accuracy = 0.214\n",
            "\n",
            "in evaluate\n",
            "EPOCH 4 ...\n",
            "Test Accuracy = 0.219\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
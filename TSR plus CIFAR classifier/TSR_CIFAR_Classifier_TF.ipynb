{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mark2_44_TSFClassifier_TF.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tortoisehare/TSR-GAN/blob/master/TSR_CIFAR_Classifier_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isD9mtH373R4",
        "colab_type": "text"
      },
      "source": [
        "Author: Stephanie Tietz\n",
        "\n",
        "LeNet5 model design and grayscale preprocessing idea from: mohamedameen93 on full German Traffic Sign dataset\n",
        "\n",
        "https://github.com/mohamedameen93/German-Traffic-Sign-Classification-Using-TensorFlow\n",
        "\n",
        "Regularization options provided, Adadelta or Adam optimizer, trained on German TS Dataset plus \"not a sign\" images, optimized for 3 classes first then expanding to 44\n",
        "\n",
        "Multiclass Image Classification in TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc_cvQN_70HP",
        "colab_type": "code",
        "outputId": "0450440e-e706-40ec-ca17-316e8ceeb1fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(1187) #to help reproduce\n",
        "\n",
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import csv\n",
        "#import random\n",
        "#import cv2\n",
        "#from skimage.filters import rank\n",
        "#from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras import optimizers\n",
        "#from keras.utils import np_utils\n",
        "#from keras import backend as K\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5l_XxOH8Lgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#upload data\n",
        "#from google.colab import files\n",
        "\n",
        "#uploaded = files.upload()\n",
        "\n",
        "#traindata = np.load('smalltrain.npy') #only 3 classes, both circle signs\n",
        "traindata = np.load('alltrain.npy') #all classes\n",
        "\n",
        "#testdata = np.load('smalltest.npy')\n",
        "testdata = np.load('alltest.npy')\n",
        "\n",
        "validdata = np.load('allvalid.npy')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TskHKwV0zO21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "X_train, Y_train = tstrain['features'], tstrain['labels']\n",
        "X_valid, Y_valid = tsvalid['features'], tsvalid['labels']\n",
        "X_test, Y_test = tstest['features'], tstest['labels']\n",
        "\n",
        "num_train = X_train.shape[0]\n",
        "num_test = X_test.shape[0]\n",
        "num_valid = X_valid.shape[0]\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bDJvjGb8geR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#traindata = np.load('smalltrain2.npy') #smalltrain2 has triangle sign\n",
        "#testdata = np.load('smalltest2.npy')\n",
        "\n",
        "#traindata = np.load('tsrtrain.npy')\n",
        "#testdata = np.load('tsrtest.npy')\n",
        "\n",
        "print(traindata.shape) #(34799,3073) for signs\n",
        "print(testdata.shape) #(12630,3073) for signs\n",
        "print(validdata.shape) #(4410,3073) for signs\n",
        "print(testdata)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LppQcdFyMxKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#signs = []\n",
        "#with open('signnames.csv', 'r') as csvfile:\n",
        "    #signnames = csv.reader(csvfile, delimiter=',')\n",
        "    #next(signnames,None)\n",
        "    #for row in signnames:\n",
        "     #   signs.append(row[1])\n",
        "    #csvfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMi6286r8tVV",
        "colab_type": "code",
        "outputId": "56370d0c-f08b-4826-abc8-0cfe9467ce1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "\n",
        "#split into x and y\n",
        "X_train = traindata[:,:-1]\n",
        "Y_train = traindata[:,-1]\n",
        "#Y_train = Y_train.reshape(len(Y_train),1)\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "\n",
        "X_test = testdata[:,:-1]\n",
        "Y_test = testdata[:,-1]\n",
        "#Y_test = Y_test.reshape(len(Y_test),1)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)\n",
        "\n",
        "X_valid = validdata[:,:-1]\n",
        "Y_valid = validdata[:,-1]\n",
        "#Y_valid = Y_valid.reshape(len(Y_valid),1)\n",
        "print(X_valid.shape)\n",
        "print(Y_valid.shape)\n",
        "\n",
        "#reshape into (32,32,3) for each example for this model input\n",
        "X_train = (X_train.reshape(traindata.shape[0], 3, 32, 32)).transpose([0,2,3,1])\n",
        "print(X_train.shape)\n",
        "X_test = (X_test.reshape(testdata.shape[0], 3, 32, 32)).transpose([0,2,3,1])\n",
        "print(X_test.shape)\n",
        "X_valid = (X_valid.reshape(validdata.shape[0], 3, 32, 32)).transpose([0,2,3,1])\n",
        "print(X_valid.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(34799, 3072)\n",
            "(34799,)\n",
            "(12630, 3072)\n",
            "(12630,)\n",
            "(4410, 3072)\n",
            "(4410,)\n",
            "(34799, 32, 32, 3)\n",
            "(12630, 32, 32, 3)\n",
            "(4410, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLk-mamjn4LL",
        "colab_type": "code",
        "outputId": "401d9f34-6475-4f5f-cf22-b026de3ead91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#START OF DATA PREPROCESSING\n",
        "#Shuffle\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "#Randomize (shuffle) the data\n",
        "print(Y_train)\n",
        "X_train, Y_train = shuffle(X_train, Y_train)\n",
        "X_valid, Y_valid = shuffle(X_valid, Y_valid)\n",
        "X_test, Y_test = shuffle(X_test, Y_test)\n",
        "print(Y_train)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[41 41 41 ... 25 25 25]\n",
            "[25  3  2 ... 13 18 34]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8LWatNlVPog",
        "colab_type": "code",
        "outputId": "2c47daf0-24fb-4423-adc8-e051a9b8ad46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#Make grayscale\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_valid.shape)\n",
        "grayscale = [0.299,0.587,0.144]\n",
        "\n",
        "X_test = np.dot(X_test, grayscale)\n",
        "X_train = np.dot(X_train, grayscale)\n",
        "X_valid = np.dot(X_valid, grayscale)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_valid.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(34799, 32, 32, 3)\n",
            "(12630, 32, 32, 3)\n",
            "(4410, 32, 32, 3)\n",
            "(34799, 32, 32)\n",
            "(12630, 32, 32)\n",
            "(4410, 32, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR_P1x9lhSPC",
        "colab_type": "code",
        "outputId": "83e235d5-2d8d-49c2-eb85-226459e56ffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Normalize data\n",
        "X_train = np.array(X_train)/255\n",
        "X_test = np.array(X_test)/255\n",
        "X_valid = np.array(X_valid)/255\n",
        "print(X_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(34799, 32, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjrwooR2fwdW",
        "colab_type": "code",
        "outputId": "d771ddcb-4caa-46a2-961e-17134b906cad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Expand dimensions to fit 4D input array\n",
        "\n",
        "X_train = np.expand_dims(X_train,-1)\n",
        "print(X_train.shape)\n",
        "\n",
        "X_test = np.expand_dims(X_test,-1)\n",
        "print(X_test.shape)\n",
        "\n",
        "X_valid = np.expand_dims(X_valid,-1)\n",
        "print(X_valid.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(34799, 32, 32, 1)\n",
            "(12630, 32, 32, 1)\n",
            "(4410, 32, 32, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX_1efCDVRHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compute mean image of training set and subtract from training images (not test images)\n",
        "#chann_index_swap = np.swapaxes(X_train,0,3)\n",
        "#mean_image = [[[sum(pixel)/len(pixel) for pixel in col] for col in row] for row in chann_index_swap]\n",
        "#mean_image = np.swapaxes(mean_image,0,2)\n",
        "#mean_image = np.swapaxes(mean_image,0,1)\n",
        "\n",
        "#X_train = X_train - mean_image\n",
        "#print(X_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rptTydNb8otG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 44 #change to 44 if including cifar\n",
        "\n",
        "assert(len(X_train)==len(Y_train))\n",
        "n_train = len(X_train)\n",
        "assert(len(X_test)==len(Y_test))\n",
        "n_test = len(X_test)\n",
        "assert(len(X_valid)==len(Y_valid))\n",
        "n_valid = len(X_valid)\n",
        "\n",
        "#Useful image variables\n",
        "im_rows = 32\n",
        "im_cols = 32\n",
        "image_shape = (32, 32, 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAxWJ3R-kR9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Histogram visualization of classes\n",
        "hist_data = np.histogram(Y_train, bins=range(num_classes+1))\n",
        "hist_map = {}\n",
        "for occr,i in zip(hist_data[0], hist_data[1]):\n",
        "  hist_map[occr] = i\n",
        "\n",
        "plt.hist(Y_train, bins=range(num_classes+1), color = '#00ffCC')\n",
        "plt.axis([0,num_classes,0,2500])\n",
        "plt.show()\n",
        "print(\"Train Class with most examples:\")\n",
        "print(hist_map[np.amax(hist_data[0])],np.amax(hist_data[0]))\n",
        "print(\"Class with least examples:\")\n",
        "print(hist_map[np.amin(hist_data[0])],np.amin(hist_data[0]))\n",
        "\n",
        "hist_data = np.histogram(Y_valid, bins=range(num_classes+1))\n",
        "hist_map = {}\n",
        "for occr,i in zip(hist_data[0], hist_data[1]):\n",
        "  hist_map[occr] = i\n",
        "\n",
        "plt.hist(Y_valid, bins=range(num_classes+1), color = '#00ffCC')\n",
        "plt.axis([0,num_classes,0,250])\n",
        "plt.show()\n",
        "print(\"Valid Class with most examples:\")\n",
        "print(hist_map[np.amax(hist_data[0])],np.amax(hist_data[0]))\n",
        "print(\"Class with least examples:\")\n",
        "print(hist_map[np.amin(hist_data[0])],np.amin(hist_data[0]))\n",
        "\n",
        "hist_data = np.histogram(Y_test, bins=range(num_classes+1))\n",
        "hist_map = {}\n",
        "for occr,i in zip(hist_data[0], hist_data[1]):\n",
        "  hist_map[occr] = i\n",
        "\n",
        "plt.hist(Y_test, bins=range(num_classes+1), color = '#00ffCC')\n",
        "plt.axis([0,num_classes,0,1000])\n",
        "plt.show()\n",
        "print(\"Test Class with most examples:\")\n",
        "print(hist_map[np.amax(hist_data[0])],np.amax(hist_data[0]))\n",
        "print(\"Class with least examples:\")\n",
        "print(hist_map[np.amin(hist_data[0])],np.amin(hist_data[0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gQozorl2wrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LeNet:  \n",
        "\n",
        "    def __init__(self, n_out=44, mu=0, sigma=0.1, learning_rate=0.001):\n",
        "        # Hyperparameters\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "\n",
        "        # Layer 1 (Convolutional): Input = 32x32x1. Output = 28x28x6.\n",
        "        self.filter1_width = 5\n",
        "        self.filter1_height = 5\n",
        "        self.input1_channels = 1\n",
        "        self.conv1_output = 6\n",
        "        # Weight and bias\n",
        "        self.conv1_weight = tf.Variable(tf.truncated_normal(\n",
        "            shape=(self.filter1_width, self.filter1_height, self.input1_channels, self.conv1_output),\n",
        "            mean = self.mu, stddev = self.sigma))\n",
        "        self.conv1_bias = tf.Variable(tf.zeros(self.conv1_output))\n",
        "        # Apply Convolution\n",
        "        self.conv1 = tf.nn.conv2d(x, self.conv1_weight, strides=[1, 1, 1, 1], padding='VALID') + self.conv1_bias\n",
        "        \n",
        "        # Activation:\n",
        "        self.conv1 = tf.nn.relu(self.conv1)\n",
        "        \n",
        "        # Pooling: Input = 28x28x6. Output = 14x14x6.\n",
        "        self.conv1 = tf.nn.max_pool(self.conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "        \n",
        "        # Layer 2 (Convolutional): Output = 10x10x16.\n",
        "        self.filter2_width = 5\n",
        "        self.filter2_height = 5\n",
        "        self.input2_channels = 6\n",
        "        self.conv2_output = 16\n",
        "        # Weight and bias\n",
        "        self.conv2_weight = tf.Variable(tf.truncated_normal(\n",
        "            shape=(self.filter2_width, self.filter2_height, self.input2_channels, self.conv2_output),\n",
        "            mean = self.mu, stddev = self.sigma))\n",
        "        self.conv2_bias = tf.Variable(tf.zeros(self.conv2_output))\n",
        "        # Apply Convolution\n",
        "        self.conv2 = tf.nn.conv2d(self.conv1, self.conv2_weight, strides=[1, 1, 1, 1], padding='VALID') + self.conv2_bias\n",
        "        \n",
        "        # Activation:\n",
        "        self.conv2 = tf.nn.relu(self.conv2)\n",
        "        \n",
        "        # Pooling: Input = 10x10x16. Output = 5x5x16.\n",
        "        self.conv2 = tf.nn.max_pool(self.conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "        \n",
        "        # Flattening: Input = 5x5x16. Output = 400.\n",
        "        self.fully_connected0 = Flatten()(self.conv2)\n",
        "        \n",
        "        # Layer 3 (Fully Connected): Input = 400. Output = 120.\n",
        "        self.connected1_weights = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = self.mu, stddev = self.sigma))\n",
        "        self.connected1_bias = tf.Variable(tf.zeros(120))\n",
        "        self.fully_connected1 = tf.add((tf.matmul(self.fully_connected0, self.connected1_weights)), self.connected1_bias)\n",
        "        \n",
        "        # Activation:\n",
        "        self.fully_connected1 = tf.nn.relu(self.fully_connected1)\n",
        "    \n",
        "        # Layer 4 (Fully Connected): Input = 120. Output = 84.\n",
        "        self.connected2_weights = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = self.mu, stddev = self.sigma))\n",
        "        self.connected2_bias = tf.Variable(tf.zeros(84))\n",
        "        self.fully_connected2 = tf.add((tf.matmul(self.fully_connected1, self.connected2_weights)), self.connected2_bias)\n",
        "        \n",
        "        # Activation.\n",
        "        self.fully_connected2 = tf.nn.relu(self.fully_connected2)\n",
        "    \n",
        "        # Layer 5 (Fully Connected): Input = 84. Output = 44.\n",
        "        self.output_weights = tf.Variable(tf.truncated_normal(shape=(84, 44), mean = self.mu, stddev = self.sigma))\n",
        "        self.output_bias = tf.Variable(tf.zeros(44))\n",
        "        self.logits =  tf.add((tf.matmul(self.fully_connected2, self.output_weights)), self.output_bias)\n",
        "\n",
        "        \n",
        "        # Training operation\n",
        "        self.one_hot_y = tf.one_hot(y, n_out)\n",
        "        self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.one_hot_y)\n",
        "        self.loss_operation = tf.reduce_mean(self.cross_entropy)\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "        self.training_operation = self.optimizer.minimize(self.loss_operation)\n",
        "\n",
        "        # Accuracy operation\n",
        "\n",
        "        self.correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.one_hot_y,1))\n",
        "        self.accuracy_operation = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
        "\n",
        "\n",
        "        # Saving all variables\n",
        "        self.saver = tf.train.Saver()\n",
        "    \n",
        "    def y_predict(self, X_data, BATCH_SIZE=64):\n",
        "        num_examples = len(X_data)\n",
        "        y_pred = np.zeros(num_examples, dtype=np.int32)\n",
        "        sess = tf.get_default_session()\n",
        "        for offset in range(0, num_examples, BATCH_SIZE):\n",
        "            #print(\"in y_pred offset\")\n",
        "            batch_x = X_data[offset:offset+BATCH_SIZE]\n",
        "            y_pred[offset:offset+BATCH_SIZE] = sess.run(tf.argmax(self.logits, 1), feed_dict={x:batch_x, keep_prob:1, keep_prob_conv:1})\n",
        "            print(y_pred)\n",
        "        return y_pred\n",
        "    \n",
        "    def evaluate(self, X_data, y_data, BATCH_SIZE=64):\n",
        "        num_examples = len(X_data)\n",
        "        total_accuracy = 0\n",
        "        sess = tf.get_default_session()\n",
        "        for offset in range(0, num_examples, BATCH_SIZE):\n",
        "            batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
        "            #print(\"in eval offset\")\n",
        "            accuracy = sess.run(self.accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0, keep_prob_conv: 1.0 })\n",
        "            #print(\"after acc in offset\")\n",
        "            total_accuracy += (accuracy * len(batch_x))\n",
        "        return total_accuracy / num_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH6vbnVR28cI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
        "y = tf.placeholder(tf.int32, (None))\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)       # For fully-connected layers\n",
        "keep_prob_conv = tf.placeholder(tf.float32)  # For convolutional layers\n",
        "\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 64\n",
        "DIR = 'Saved_Models'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnQDHOKZ3Luu",
        "colab_type": "code",
        "outputId": "20d9ca0d-a505-4454-db8e-51ec5bb1938c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "LeNet_Model = LeNet(n_out = num_classes)\n",
        "model_name = \"LeNet\"\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    num_examples = len(Y_train)\n",
        "    print(\"Training ...\")\n",
        "    print()\n",
        "    for i in range(EPOCHS):\n",
        "        #X_train, Y_train = shuffle(X_train, Y_train)\n",
        "        for offset in range(0, num_examples, BATCH_SIZE):\n",
        "            end = offset + BATCH_SIZE\n",
        "            batch_x, batch_y = X_train[offset:end], Y_train[offset:end]\n",
        "            sess.run(LeNet_Model.training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob : 0.5, keep_prob_conv: 0.7})\n",
        "            \n",
        "        validation_accuracy = LeNet_Model.evaluate(X_valid, Y_valid)\n",
        "        print(\"EPOCH {} : Validation Accuracy = {:.3f}%\".format(i+1, (validation_accuracy*100)))\n",
        "    LeNet_Model.saver.save(sess, os.path.join(DIR, model_name))\n",
        "    print(\"Model saved\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Training ...\n",
            "\n",
            "EPOCH 1 : Validation Accuracy = 6.984%\n",
            "EPOCH 2 : Validation Accuracy = 9.932%\n",
            "EPOCH 3 : Validation Accuracy = 10.340%\n",
            "EPOCH 4 : Validation Accuracy = 9.909%\n",
            "EPOCH 5 : Validation Accuracy = 10.204%\n",
            "EPOCH 6 : Validation Accuracy = 10.658%\n",
            "EPOCH 7 : Validation Accuracy = 10.181%\n",
            "EPOCH 8 : Validation Accuracy = 9.909%\n",
            "EPOCH 9 : Validation Accuracy = 9.660%\n",
            "EPOCH 10 : Validation Accuracy = 9.637%\n",
            "EPOCH 11 : Validation Accuracy = 9.320%\n",
            "EPOCH 12 : Validation Accuracy = 9.637%\n",
            "EPOCH 13 : Validation Accuracy = 9.524%\n",
            "EPOCH 14 : Validation Accuracy = 9.184%\n",
            "EPOCH 15 : Validation Accuracy = 9.002%\n",
            "EPOCH 16 : Validation Accuracy = 8.866%\n",
            "EPOCH 17 : Validation Accuracy = 8.322%\n",
            "EPOCH 18 : Validation Accuracy = 8.231%\n",
            "EPOCH 19 : Validation Accuracy = 7.982%\n",
            "EPOCH 20 : Validation Accuracy = 7.937%\n",
            "EPOCH 21 : Validation Accuracy = 7.823%\n",
            "EPOCH 22 : Validation Accuracy = 7.937%\n",
            "EPOCH 23 : Validation Accuracy = 7.732%\n",
            "EPOCH 24 : Validation Accuracy = 7.846%\n",
            "EPOCH 25 : Validation Accuracy = 7.982%\n",
            "EPOCH 26 : Validation Accuracy = 8.186%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM8dlbLQgXeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "with tf.Session() as sess:\n",
        "    LeNet_Model.saver.restore(sess, os.path.join(DIR, \"LeNet\"))\n",
        "    y_pred = LeNet_Model.y_predict(X_test)\n",
        "    test_accuracy = sum(Y_test == y_pred)/len(Y_test)\n",
        "    print(\"Test Accuracy = {:.1f}%\".format(test_accuracy*100))\n",
        "    \n",
        "\n",
        "cm = confusion_matrix(Y_test, y_pred)\n",
        "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "cm = np.log(.0001 + cm)\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Log of normalized Confusion Matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwKI1Hg0Q9NI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
